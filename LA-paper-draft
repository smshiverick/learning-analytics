\documentclass[sigconf]{acmart}

\input{format/final}

\begin{document}
  \title{Predicting Student Success with Learning Analytics}
  \author{Sean M. Shiverick}
  \affiliation{\institution{smshiverick@gmail.com}
  }
\renewcommand{\shortauthors}{S.M. Shiverick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

This paper compares predictive models of student performance and identifies 
features that affect student achievement. 


Modeling student performance can help educators to identify students who are 
struggling and the factors that contribute to student success. 

Learning analytics approaches can be used by administrators Identify students interested in pursuing higher education but failing to perform and students 
with no plans to pursue higher education, but who are passing, which may be a 
source for additional recruitment efforts. . 




Predictive modeling provides a useful approach for analyzing student achievement 
and identifying features that contribute to MUPO. 

Contribute to better understanding of factors that contribute to student success. 

Comparing predictive models in order to determine the best approach for 
modeling attributes of student achievement. 


This study compared ten classification models 
using four performance metrics: accuracy, sensitivity (i.e. recall), 
precision, and $f_1$-score. Data on student performance datset from the UCI Machine
Learning Repository consisted of N=1044 observations. 

Twenty-six percent of respondents reported using prescription 
opioid medications in the past year, 11\% reported misusing or abusing pain 
relievers, and 2\% reported heroin use. 

The classifier models were fit to 
a training set using sixteen features that included demographic variables, 
medications, and illicit drugs. The binary target variable was pain reliever
misuse and abuse. 

Model performance was evaluated on the testing set. The
$f_1$-score was used as the performance metric due to unbalanced classes in
the data. 

Logistic regression, random forests, and decision trees had the 
highest $f_1$-scores compared to other models. 

All three models identified cocaine use as the most informative feature for predicting pain reliever 
misuse and abuse. Amphetamine use was selected as the second most important 
variable by logistic regression and random forests.

The models differed in 
the importance they assigned to heroin use, tranquilizer use, age category, 
and mental health as predictors of pain reliever misuse and abuse. 
Advantages and limitations of the classifier models are considered and the 
tradeoff between model complexity and interpretability is discussed. 
\footnote{Address correspondence to \textit{smshiverick@gmail.com}.}

\end{abstract}
\keywords{Predictive Modeling, Supervised Learning, Classification Models}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The exponential increase in computing speed and processing power in recent 
decades has led to the generation an immense volume of data in higher education 
\cite{Daniel17}. The development and use of analytic approaches for 
statistical 'analysis and predictive modeling allows researchers to discover 
patterns in data and provide insights about learning for effective decision 
making. Learning Analytics (LA) is an emerging, multidisciplinary field at 
the intersection of learning science, social science, statistics, and computer 
science that leverages big data to understand learning and the environments 
in which it occurs \cite{siemens13}. The development of LA as a field of 
inquiry have been primarily driven by big data in education and the shift 
toward online learning. The primary stakeholders for LA are learners, educators, 
researchers and administrators. The ability to mine large scale data from online 
learning platforms has applications for student success, course design, and 
institutional programs \cite{Lester19}. This paper reviews LA methodologies, 
applications, and considers challenges and opportunities for implementing 
LA in education.

Predictive modeling provides useful methods for analyzing the student 
performance and identifying features that contribute to successful learning. 
Data mining can predict individuals who may be susceptible to failing or 
dropping out and provide insights for improving student retention. This study 
compares the performance of several classification models to determine the 
best approaches for modeling student performance and success. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning Analytics (LA) and Educational Data Mining (EDM)}

LA is a form of education data mining (EDM) that uses research methods and 
predictive analysis to implement platforms for improving student performance and 
instructional design \cite{Baker09, Lester19}. LA and EDM share similar goals 
at the intersection of learning science and data-driven analytics, but these 
approaches differ in origins, emphasis, techniques, and knowledge discovery 
\cite{siemens12}. For example, EDM takes a reductionist approach to analyzing 
individual components in the learning process that focuses on automated discovery
and adaptive learning (e.g., intelligent tutoring). By contrast, LA takes a 
holistic viewpoint to understand the complexity of learning, by leveraging human 
judgment to inform and empower instructors and learners \cite{Papamitsiou14}. 
The development of LA as a separate field of inquiry from EDM has shown a gradual 
shift from a technological focus toward an educational focus or learner-focused 
analytics (Ferguson, 2012). Despite the differences in origin and goals of LA 
and EDM, they can be seen as complementary approaches using similar methodologies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Historically, education institutions have examined factors such as persistence 
and social integration as influential for learning and used analytic methods to 
track student drop-out rates \cite{ferguson12}. The development of the web opened 
up new avenues for collecting information from online sources along with methods 
for processing and sharing content. As extensive educational datasets became 
available for analysis, EDM researchers use a variety of data mining techniques 
(e.g., classification, clustering, association rule mining) to discover 
potentially useful patterns in data for understanding students and the settings 
in which they learn \cite{bakerYucef09}. To a large extent, EDM emerged from 
a computational approach to analyzing logs of student-computer interactions with
data mined from LMS, ITS (intelligent tutoring system), and technology enhanced 
learning (TEL) systems to evaluate learning processes, enhance web-based learning 
and help learners \cite{penaAyala14, romero10}. Thus, EDM research has focused 
more on the technical challenge of extracting value about learning from big data 
in education. Practitioners distinguish LA as a separate field of research by a 
more holistic, education-focused approach to learning, which seeks to ground 
analytics in learning theory, to benefit learners and teachers, and optimize 
learning in online environments \cite{lang17, papamitsiou14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Together, LA and EDM represent an ecosystem of methodologies and techniques 
for gathering, processing, and acting on data to promote learning. These 
procedures facilitate the preparation, measurement, and collection of data 
about learning activities for subsequent analysis, interpretation, and r
eporting. Much LA/EDM research data is collected within a virtual learning 
environment (VLE), learning management system (LMS), or massive open online 
course (MOOC), which can automatically track student participation by login 
frequency, number of resources accessed, time to solve tasks, response times 
to answer questions, number of discussion posts or chat messages between 
participants, questions submitted to instructors. In addition to quantitative 
measures, meaningful analytics systems will include qualitative measures about 
the content or type of contribution (on-topic, evaluative, question). 
Researchers have also studied how affective states (e.g., boredom, 
frustration, confusion, happiness) influence engagement and learning outcomes 
\cite{pardos14}. Merging LMS data with institutional data about student 
performance (i.e., past grades) can provide valuable insights about the 
conditions for successful learning and decision making (Hora, 2019). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LA/EDM consists of a diverse set of methodologies and approaches for 
addressing challenges in higher education such as underprepared students, 
decreased retention, and foster student success. The most common approaches in 
LA research are modeling students and student behavior, predicting performance, 
dropout, and retention, increasing (self-)reflection and (self-)awareness, 
analyzing connections among learners, improving feedback and assessment, and 
recommending educational resources \cite{lang17, lester19, papamitsiou14}. 
Descriptive, correlational, and predictive methodologies are used to study 
learning processes and context, including predictive modeling, relationship 
mining, similarity grouping, content analysis, and social network analysis. 
Classification is the most commonly used data analytic method, followed by 
clustering, regression (logistic, multiple), and predictive modeling (discovery
with models). Several performance metrics are used to evaluate different 
methods: precision, accuracy, sensitivity, coherence, fitness measures, 
similarity weights \cite{bakerYucef09}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubheading{Student / Student Behavior Modeling}.
Modeling student characteristics takes a descriptive approach to reveal factors 
for learning such as domain knowledge, 
motivation, metacognition, attitudes, affect, metacognitive states, learning 
strategies, and behavior \cite{Papamitsiou14}. Student characteristics can be 
combined with data from the LMS system about support, teacher feedback, 
curriculum, and course sequencing. Modeling student characteristics is most 
often based on frequency, but can involve correlation analysis, similarity 
grouping or clustering (k-means), content analysis (i.e., text mining, NLP). 
Quantitative data about activities and behavior (e.g., response time, or 
time reflecting on hints) are helpful for modeling engagement or disengagement 
in learning activities, and provide information that allows software to respond 
to individual differences. Discovery and modeling of behaviors during learner 
interactions in a MOOC.  Clustering can be used to grouping similar individuals 
together based on learner profiles and to aid in the development of more 
personalized learning environment (PLE)\cite{Vellido10}. Analysis of qualitative 
data, may rely on human judgment, can reveal deeper concepts about learning, 
such as inferring reasoning strategies from interaction with a cognitive tutor 
\cite{Fournier11}. Predictive approaches are used also for modeling student 
behavior and assessing functionality in intelligent tutoring systems (ITS)
\cote{penaayala14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubheading{Predicting Performance} 

Predictive methodologies can lead to a better 
understanding of student engagement in online education, behavior patterns 
that predict successful performance, and factors that influence student failure 
and non-retention. The Course Signals program developed at Purdue University 
is one of the most well known platforms for predicting students at-risk of 
falling behind using statistical analysis of LMS data \cite{arnoldPistilli12}. 
In addition to grades, demographic information, academic history, and student 
interaction on the Blackboard LMS were used to track student performance. A 
predictive algorithm was used to calculate likelihood of student success based 
on performance, effort, history, and student characteristics. The course signals 
provided students with real-time feedback about their status via the LMS as 
traffic signal indicators (i.e., red=high risk, yellow=moderate risk, green=low 
risk). Instructors enacted interventions for high risk students providing them 
with actionable information about their performance in emails, texts, referrals
to academic advising, or academic resources center, and face to face meetings.
Courses that implemented the Signals program and provided feedback for students
showed an increase in satisfactory grades, decrease in withdrawals, and 
improved retention. Course signals helped to integrate students into the 
university academically in several ways: first, by facilitating contact 
between faculty and students, second, by providing high risk students with 
resources for student success, and third, by using analytics with real time 
data about student performance. in terms of academic success and retention, 
underprepared students in difficult courses using the signals program fared 
better than more well-prepared students in courses that were not using the 
signals program. Furthermore, LA provided faculty with a useful tool for 
identifying struggling students and encouraging them to take corrective actions.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most common approach used to predict student performance, dropout, and 
retention is classification, which operationalizes performance as a binary 
outcome. Classification methods include logistic regression, naive Bayes 
models, decision trees, machine learning algorithms, and neural networks 
\cite{Lykourentzou09}. Past empirical findings indicate that the number of 
quizzes successfully passed is the most influential predictor of final grades, 
in addition to engagement, participation in course activities, number of posts, 
frequency of events, and time spent in the course LMS 
\cite{Papamitsiou14, romerozaldivar12}. Affect and motivation are also 
influential for predicting performance; notably, concentration and 
frustration are significantly correlated with final grades, but boredom and 
confusion were negatively related to performance (Pardos et al., 2013). 
Using data-driven machine learning algorithms on student profiles and data 
about activity within the LMS can facilitate early detection of at-risk 
students \cite{Dekkar09}. Furthermore, studentsâ€™ sense of belonging in a 
course is essential for engagement and improved satisfaction which can 
reduces student dropout.measures of student satisfaction can vary according 
to their perceived usefulness or efficiency of training courses. The most 
common reasons for  student disengagement in MOOCs were personal commitments, 
work conflict and course overload (Kizilcec, 2013). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predictive Modeling}

Predictive modeling, statistical learning, or machine learning describe a 
set of procedures and automated processes for extracting knowledge from 
data \cite{james13, kuhn13, muller17, raschka17}. The two main branches 
of predictive modeling are supervised learning and unsupervised learning. 
Supervised learning problems involve prediction about a specific target 
or outcome variable. If a dataset has no target outcome, unsupervised 
learning methods can help to reveal underlying structure in the data 
(e.g. clustering). Supervised learning is used to predict an outcome based 
on input provided to a model, when examples of input/output pairs are 
available in the data \cite{muller17}. A statistical learning model is 
constructed using a set of observations to train the model and then make
predictions with new observations. Two main approaches for supervised learning 
problems are regression and classification. When the target variable
is continuous or there is continuity in the outcome (e.g. home prices), a 
regression model tests how a set of features predicts the target variable. 
If the target is a class label, binary variable, or set of categories 
(e.g., spam or ham emails, benign or malignant cells), a classification model 
will predict which class or category label new instances are assigned to. 
This study used a supervised learning approach to classify prescription pain
reliever misuse and abuse as a binary outcome. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the era of `big data', large amounts of health information are being 
generated from electronic medical records (EMRs), clinical research data, and 
population-level health data \cite{herland14}. Although it can be difficult 
to obtain reliable information about opioid use based on self-reports, surveys 
provide data on a range of issues that people may be reluctant to disclose 
such as illicit drug use and mental health problems. The data for the present 
study was obtained from the National Survey on Drug Use and Health (NSDUH) 
which is a major source of information for the use of illicit drugs and mental 
health issues among the U.S. population aged 12 or older \cite{samhsa18}. 
The NSDUH is a comprehensive public survey that includes a diverse array 
of questions and variables related to the use, misuse, and abuse of substances 
including alcohol, tobacco, prescription medications, and illicit drugs. In 
addition to typical demographic information, the survey includes self-reported
measures on items related to physical health, mental health (e.g., depression, 
anxiety, suicide), as well as counseling, and drug or alcohol treatment. Data 
from the NSDUH has been used for identifying groups at high risk for substance 
use and the co-occurrence of substance use and mental health disorders. The 
target variable for the study was any previous misuse or abuse of prescription 
opioid pain relievers (e.g., oxycodone, hydrocodone). The predictor variables 
were demographic features (e.g., age, sex, education, etc.), use of medications 
(e.g., tranquilizers, sedatives), and illicit drugs (e.g., cocaine, 
amphetamines, heroin). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}

\subsection{The Data}

The student performance dataset was downloaded from the UC-Irvine Machine 
Learning Repository (UCI-MLR) and saved as data frame objects in a python 
interactive notebook. The data was collected from two secondary schools in 
the Alentejo region of Portugal during the 2005-2006 school year and 
contained information from a questionnaire and school reports of student 
grades \cite{cortez09}. The dataset includes thirty predictor variables 
consisting of demographic information (e.g. family size, parental education 
and employment status), social/ emotional attributes (e.g. extracurricular 
activities, romantic relationship, alcohol consumption), school related 
variables (e.g., weekly time studying, past class failures, absences) related 
to student  (see Table 1). Student performance was evaluated on a 20 point 
scale-as in other European countries (e.g., France)-at three points during 
the school year (i.e., G1, G2, G3) for two courses, mathematics (n=395) and 
Portugese (n=649). The target outcome was the final grade. A binary dummy 
variable was created based on the final exam grades, where scores greater than 
10 indicated a passing grades and scores below 10 indicated failing grades. 
The student performance data was modeled using binary classification.

The sample consisted of 1044 students (56.6\% female, Mage=16.71, SD=1.19). 
Descriptive statistics are provided in Table 2 according to the binary 
measure of student performance. The majority of students (73.7\%) were 
from urban areas, from families larger than three members (70.7\%), and 
with parents living together in the sam household (88%).

Ten individuals between 20 and 22 were added to the category of 19+ years. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Model Construction and Evaluation}

The dataset was divided using a 70 to 30 percent split to create the training
set ($n_1$=731) and testing set ($n_2$=313). The same general procedure 
was used for constructing and evaluating each classification model: (i) The 
model was fit to the training set; (ii) New values were predicted on the 
holdout scores in the testing set; and (iii) Model performance on the test 
set was evaluated in a confusion matrix. The performance metrics of accuracy, 
sensitivity or recall, precision, and $f_1$-score were obtained or derived 
from the confusion matrix. The logistic regression classifier, LDA, QDA, 
decision trees classifier, random forests classifier, gradient boosted trees 
were constructed using the caret package. The KNN classifier, support 
vector classifier (SVC), naive Bayes classifier, and neural network 
(multilayer perceptron) were built using scikit-learn. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure3.pdf}
%  \caption{Proportion of Pain Reliever Misuse and Abuse
%  as a function of Heroin Use.}
%  \label{f:Figure3}
%\end{figure}


Main Parameters
K-Nearest Neighbors  Number of Neighbors = 4 
Support Vector Machines (RBF)  C = 10, gamma = 0.1
Naive Bayes  Cost = 0.01 
Neural Network  Hidden Layers = 1
Decision Trees  Maximum Depth = 4
Random Forests  Number of Trees = 1000 
Boosted Trees  Learning Rate = 0.01 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classification Models}

\subsubsection{Linear Models}

As stated by the statistician George E. P. Box, "All models are wrong, but 
some models are useful" \cite{box05}. There are advantages and limitations 
to selecting any model. Logistic regression is one of the most reliable 
and interpretable models for classification. Logistic regression models 
the conditional distribution of probabilities for a binary response 
(e.g., $Pr(Y=k|X=x)$) as a combination of a set of predictor variables 
\cite{james13, raschka17}. The decision boundary for the logistic regression 
classifier is a linear function of the input; a binary classifier separates 
two classes using a line, plane, or hyperplane \cite{muller17}. Given that 
the probability values for the outcome range between 0 and 1, predictions 
can be made based on a default value. For example, a default value of `Yes' 
could be predicted for any individual for whom the probability of pain reliever
misuse and abuse is greater than fifty-percent: $Pr(PRLMISAB) > 0.5$. 
Logistic regression uses a maximum likelihood method to predict the coefficient
estimates that correspond as closely as possible to the default state. 
In other words, the model will predict a number close to one for individuals 
who have misused or abused pain relievers and a number close to  zero for 
individuals who have not. The distribution of conditional probabilities in 
the logit model has an S-shaped curve. By taking the natural log of the left 
side of the regression equation, we obtain the logit function which is linear 
in the parameters, where X = ($X_1$...$X_p$) predictor variables (equation 1). 
The coefficient estimates are selected to maximize the likelihood function, 
and are interpreted as an indication of the log-odds change in the outcome 
variable that is associated with a one-unit increase in the predictor variable, 
holding the effects of other predictor variables constant. The intercept, 
$\beta_0$, is the log of the odds ratio when X is 0 \cite{gujarati09}.

\begin{equation}
  \ ln\frac{P_i}{1-P_i} = \beta_0 + \beta_1X_1 + \beta_1X_2 +... \beta_pX_p \
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Linear discriminant analysis} (LDA) is an alternative approach to 
estimating probabilities that models the distribution of predictors 
separately in each of the response classes and then uses Bayes' theorem 
to `flip' these into estimates for, $Pr(Y=k | X=x)$ \cite{james13}. The term 
linear in LDA refers to the the discriminant functions being linear functions 
of the predictors. For distributions assumed to be normal (i.e., multivariate 
Gaussian), LDA provides a model that is similar in form to logistic regression, 
but more stable. LDA is also preferred for outcomes with more than two response 
classes. The important assumptions for LDA are, first, a common covariance 
matrix for all classes, and second, the class boundaries are linear functions 
of the predictors. \emph{Quadratic Discriminant Analysis} (QDA) is an approach 
that assumes each class has its own covariance matrix and the decision 
boundaries are quadratically curvilinear in the predictor space \cite{kuhn13}. 
LDA is less flexible as a classifier than QDA, but can perform better with 
relatively few training observations or when the majority of predictors in the 
data represent discrete categories. QDA is recommended over LDA with a very 
large training set or when the decision boundary between two classes is 
non-linear. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Non-linear Models}

The performance of linear classifiers suffers when there is a non-linear 
relationship between the predictors and target outcome. With training
observations that can be separated by hyperplane, the maximal marginal 
classifier provides the maximum distance (i.e., margin) from each observation
to the hyperplane \cite{james13}. The test observations are classified based 
on which side of the hyperplane they fall; however, in many cases no separating 
hyperplane exists. The \emph{support vector classifier} (SVC) extends the 
maximal margin classifier by using a soft margin that allows a small number 
of observations to be misclassified on the wrong side of the hyperplane 
\cite{kuhn13, cortes95}. The observations that fall directly on the margin or 
on the wrong side of the hyperplane are called `support vectors'. The parameter 
`C' indicates the number of observations that can violate the margin; if $C>0$, 
no more than C observations can be on the wrong side of the hyperplane. 
SVC addresses the problem of non-linear boundaries between classes by 
enlarging the feature space with higher order (e.g., quadratic, cubic, 
polynomial) functions of the predictors. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure1.pdf}
  \caption{Decision Boundary and Support Vectors for Support Vector Machine 
  (SVM) with Nonlinear Kernel \cite{muller17}}
  \label{f:Figure1}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Support vector machines} (SVM) are an extension of SVC that use a 
kernel trick to reduce computational load. The radial basis function (RBF) 
kernel (i.e., Gaussian kernel) is one of the most commonly used approaches. 
In training the model, only a subset of data points is used to construct the 
decision boundary, namely the support vectors that lie on the border that 
separates the two classes. In predicting classes for new observations, the 
algorithm calculates the distance to each of the support vectors measured 
by the Guassian kernel \cite{muller17}. Figure 1 shows an example of the 
non-linear decision boundary obtained with SVM using the RBF kernel; the 
decision boundary is a smooth curve and the support vectors are the large 
points in bold outline. Even with the default settings, the RBF kernel 
provides a decision boundary that is decidedly non-linear. The parameters 
for SVM are `C', which regulates the importance of each data point, and 
`gamma' which controls the width of the Guassian kernel. A small value of 
C indicates a restricted model in which the influence of each data point is 
limited and the algorithm adjusts to the majority of data points. With larger 
values of C, more importance is given to each data point and the model tries 
to correctly classify as many training observations as possible, which results 
in more curvature in the decision boundary. Large values of gamma mean that 
only close values are relevant for classification, resulting in a smooth 
decision boundary. Small values of gamma mean that far points are similar. 
If the values of both C and gamma are large, each point can have a large 
influence in a small region, which produces a choppy decision boundary. 
If the values of C and gamma are both small, the decision boundary becomes 
close to linear.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Bayes' rule was mentioned above in relation to LDA; in this section, the 
\emph{Naive Bayes Classifier} is considered as a non-linear model.
The Bayes theorem (equation 2) is represented by a set of probabilities 
that answer the question, ``Based on a given set of predictors, 
what is the probability than an outcome belongs to a particular class?''

\begin{equation}
  \ P(Y=cl|X) = \frac{P(Y)*P(X|Y=cl)}{P(X)}\
\end{equation}

The prior probability, P(Y), is the expected probability of a given class 
based on what is known (e.g. rate of disease in the population). P(X) 
is the probability of the predictor variables. The conditional probability,
$P(X=cl|Y)$, is the probability of observing the predictor variables for data 
associated with a given class. The naive Bayes model assumes that all of the 
predictor variables are independent, which is not always realistic. The 
conditional probabilities are calculated based on the probability densities 
for each individual predictor \cite{kuhn13}. For categorical predictors, the 
observed frequencies in the training set data can be used to determine the 
probability distributions. The prior probability allows us to tilt the final 
probability toward a particular class. Class probabilities are created and 
the predicted class is the one that is associated with the largest class
probability. Despite the somewhat unrealistic assumption of independence among
predictors, the naive Bayes model is computationally quick, even with large 
training sets, and performs competitively compared to other models. The naive 
Bayes model encounters issues when dealing with frequencies or probabilities 
equal to zero, especially for small sample sizes. In addition, as the number 
of predictors increases relative to the size of the sample, the posterior 
probabilities will become more extreme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Neural Networks} are powerful models for classification and regression 
that are based on theories of connectivity in the brain \cite{kuhn13}. 
The present study considers a simple method called a multilayer perceptron 
(MLP) which is a feed-forward neural network \cite{muller17, raschka17}. 
The outcome is modeled by an intermediary set of unobserved variables called 
hidden units, which are linear combinations of the original predictors
(see Appendix). Each hidden unit is a combination of some or all of the 
predictors which are then transformed by a nonlinear function (e.g. 
sigmoidal). A neural network usually has multiple hidden units used 
to model the outcome. The MLP classifier computes weights between the inputs 
and hidden units, and weights between the layers of hidden units and the 
output. After computing each hidden unit, the output is modeled by a nonlinear 
combination of the hidden units. The nonlinear function allows the neural 
network to fit more complicated functions than a linear model; however, 
neural networks are sensitive to the scaling of the features and can require
extensive data preprocessing. There are several ways to modify the complexity 
of a neural network: by selecting the number of hidden layers, the number of 
units within each layer, and the regularization parameter (L2) which shrinks 
the weights towards zero. The feature weights provide an estimate of feature 
importance. Although neural networks can capture information in large amounts 
of data with very complex models, they tend to overfit data used to train the 
model and can be difficult to interpret. Neural networks may work best with 
homogenous datasets where the predictor variables all have similar meanings 
\cite{muhuri13}. For datasets with many different kinds of features, 
tree-based methods offer a better approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Tree-based Models} Decision trees are based on a hierarchy of 
\emph{`if-else'} questions starting from a root node and proceeding through a 
series of binary decisions or choices. Each node in the tree represents either 
a question or a terminal node (i.e., leaf) that contains the outcome. Applied to 
a binary classification task, the decision tree algorithm learns the sequence
of if-else questions that arrives at the outcome most quickly. For continuous 
features, questions are expressed in the form: ``Is feature x larger than 
value y?'' In constructing the tree, the algorithm searches through all 
possible tests and finds a solution that is most informative about the target 
outcome \cite{muller17}. The recursive branching process yields a binary tree 
of decisions, with each node representing a test for a single feature. This 
process of partitioning is repeated until each leaf in the decision tree 
contains only a single target. Prediction for a new data point proceeds by 
checking which region of the partition the point falls in, and predicting the 
majority in that feature space. The main advantage of tree models is that they 
require little adjustment and are easy to interpret. A drawback is that they 
can lead to complex models which are highly overfit to the training data. 
`Prepruning' can help reduce overfitting by limiting the maximum depth of the 
tree, or the maximum number of leaves. Another approach is to set the minimum 
number of points in a node required for splitting. Decision trees work well 
with features measured on different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Random Forests} is an ensemble approach that combines many simple trees 
that each overfit the data in different ways. By building many trees and 
averaging their results, random forests help to reduce overfitting. In 
constructing the forests, the user selects the number of trees to build 
(e.g., 1000). Randomness is introduced using a bootstrapping method that 
repeatedly draws random samples of size \textit{n} from the data set, with 
replacement. The decision trees are built on these random samples 
of the same size, with some points missing and some data points repeated 
\cite{muller17,raschka17}. The algorithm makes a random selection of 
\textit{p}-features, and uses a different set of features at each node branch. 
These processes ensure that all of the decision trees in the random forest are 
different. Random forests is one of the most widely used supervised learning 
algorithms and works well without very much parameter tuning or scaling of data. 
A limitation is that random forests do not perform well with high-dimensional 
data, or data that is sparse (e.g., text).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Gradient Boosting} is another ensemble method that combines multiple 
decision trees in a serial fashion, where each tree tries to correct for 
mistakes of the previous one \cite{muller17}. Gradient boosted regression 
trees use strong prepruning, with shallow trees of a depth of one to five. 
Each tree only provides a good estimate of part of the data; combining many 
shallow trees (i.e., ``weak learners'') iteratively improves performance. 
In addition to pre-pruning and the number of trees, an important parameter 
for gradient boosting is the \emph{learning rate} which determines how 
strongly each tree tries to correct for mistakes of previous trees. A high 
learning rate produces stronger corrections, allowing for more complex models. 
Adding more trees to the ensemble also increases model complexity. Gradient 
boosting and random forests perform well on similar tasks and data. A common 
approach is to first perform random forests and then include gradient boosting 
to improve model accuracy. 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Confusion Matrix for Binary Classification.}
  \label{tab:freq}
  \begin{tabular}{cccc}
    \toprule
     & &  \textbf{Actual Outcome} & \\
    \midrule
    \textbf{Predicted} & \textbf{Outcome} & Pass & Fail \\
    \midrule
    & No & \textit{True Negative} & False Negative \\
    & & (TN) & (FN) \\
    \midrule
    & PRL Misuse & False Positive & \textit{True Positive} \\
    & & (FP) & (TP)  \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluating Model Performance}

To identify which model is best for a given problem, with the data available, 
it is necessary to evaluate the performance of different learning algorithms. 
Binary classification is assessed in terms of the successful assignment of 
observations to one of two classes: positive or negative. No classification 
model can make perfect predictions, as errors are always to be found. Medical 
testing is often used as an example to illustrate classification decisions 
and errors. For example, in the actual state of the world, a patient either 
has an illness or not, and the person is either diagnosed as having the 
illness or not. In the present case, the positive class represents self-
reported pain reliever misuse and abuse (PRL Misuse), and predictions based on 
the classifier models will be either correct or incorrect in relation to the 
observed outcomes. For example, a person who has never misused or abused pain 
relievers may be misclassified as having done so (i.e., 'false positive'), or 
conversely, a person who actually has misused and abused pain relievers may 
be mislabeled as never having done so (i.e., 'false negative').

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Correct decisions and classification errors are represented in a 
\emph{confusion matrix} that indicates the correspondence between predicted 
and actual outcomes (Table 1). The confusion matrix is a two-by-two array in 
which the columns correspond to the actual observed classes and the rows 
correspond to the predicted classes. The main diagonal indicates the number 
of correctly classified samples (i.e., true negative, true positive), while 
the other entries represent the number of samples in one class that were 
mistakenly classified as another class. Several performance metrics can be 
obtained from the confusion matrix, including accuracy, sensitivity (i.e., 
recall), specificity, precision, and the $f_1$-score, presented in Table 2 
\cite{kuhn13, wiki18}. Model performance is most commonly evaluated using 
\emph{Accuracy} which is assessed by the number of correct predictions 
divided by the total observations. Sensitivity or \emph{Recall} provides the 
``True Positive Rate'' (TPR), measured as the number of positive samples that 
are correctly identified by the prediction (number of sick patients correctly
diagnosed). Recall is used when the goal is to avoid false negatives. The 
True Negative Rate (TNR) is described as Specificity, which indicates the 
proportion of negative cases correctly identified (healthy people not 
misdiagnosed). \emph{Precision} provides the ``Positive Predictive Value'' 
(PPV), which measures how many of the samples predicted as positive are 
actually positive. Precision is used as a metric when the goal is to limit
the number of false positives. The \emph{$f_1$-score} represents a harmonic 
mean between recall and precision (\(\frac{2}{ \frac{1}{R} + \frac{1}{P} }\)). 
The $f_1$-score can be a better measure of performance than accuracy in 
datasets with imbalanced classes, where one class is much more frequent 
than the other class, as it takes into account both recall and precision 
\cite{muller17, yun09}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Performance Metrics for Classifier Models.}
  \label{tab:freq}
  \begin{tabular}{lc}
    \toprule
    Accuracy & \(\frac{TP+TN}{TP+FP+TN+FN}\) \\
    & \\
    Sensitivity, Recall (TPR) &  \(\frac{TP}{TP+FN}\) \\
    & \\
    Specificity (TNR) &  \(\frac{TN}{TN+FP}\) \\
    & \\
    Precision (PPV) & \(\frac{TP}{TP+FP}\)  \\
    & \\
    f_1 score & 2*\(\frac{Precision*Recall}{Precision+Recall}\) \\
    & \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Variables Included in the Sample Data for Model Construction.}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    \textit{Target Variable} & Label \\
    \midrule
    Course final grade (0=Fail, 1=Pass) & PASS \\
    \midrule    
    \textit{Predictor Variables} &    \\
    \midrule
    Sex (0=Male,1=Female) & SEX  \\  
    Age (15, 16, 17, 18, 19+ years) & AGE  \\
    Home address type (0=Rural, 1=Urban) & AREA  \\
    Family size (0=Three or less, 1=More than three) & FAMSIZE  \\
    Parents' cohabitation status (0=Separate, 1=Together) & PARENTS  \\ 
    Mother's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & MEDU  \\
    Father's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & FEDU  \\
    Mother's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & MJOB  \\
    Father's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & Fjob  \\
    Student's guardian (0=Other, 1=Father, 2=Mother) & Guardian  \\
    Time from home to school (1=<15 min, 2=15-30 min, 3=30-60 min, 4=>60 min) & TRAVEL  \\
    Weekly study time (1=<2 hours, 2=2-5 hours, 3=5-10 hours, 4=>10 hours) & STUDY  \\
    Number of past class failures (numeric: n if 1<=n<3, else 4) & FAILURES  \\
    Extra educational support (0=No, 1=Tes) & SCHOOLSUP  \\
    Family educational support (0=No, 1=Tes) & FAMSUP  \\
    Paid classes within course subject (Math or Portuguese) (0=No, 1=Tes) & TUTOR  \\
    Extra-curricular activities (0=No, 1=Tes) & ACTIVITIES  \\
    Wants to take higher education (0=No, 1=Yes) & HIGHER  \\
    Internet access at home (0=No, 1=Tes) & INTERNET  \\
    In a romantic relationship (0=No, 1=Tes) & ROMANTIC  \\
    Quality of family relationships (1=Very Bad, 5=Excellent) & FAMREL  \\
    Free time after school (1=Very Low, 5=Very High) & FREETIME  \\
    Going out with friends (1=Very Low, 5=Very High) & GOOUT  \\
    Workday alcohol consumption (1=Very Low, 5=Very High) & DALC  \\
    Weekend alcohol consumption (1=Very Low, 5=Very High) & WALC  \\
    Current health status (1=Very Bad, 5=Very Good) & HEALTH  \\ 
    Number of school absences (Count range: 0 to 93) & ABSENCES  \\
    Course in mathematics or Portugese (0=Portugese, 1=Math) & COURSES  \\
    First period grade (0=Lowest, 20=Highest) & GR1  \\
    Second period grade (0=Lowest, 20=Highest) & GR2  \\
    \bottomrule
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training Accuracy and Test Set Accuracy}

In constructing and evaluating predictive models it is important to select 
a model that performs well not only with the data used to train the model, 
but also with new observations. A standard practice is to divide the sample 
data into a \emph{training set} and a \emph{testing set} of observations 
that is set aside and used to evaluate model performance. By convention, 
approximately 70 to 80 percent of observations are used in the training set 
and the remaining portion is held in the test set. Two main problems in 
evaluating model performance are \emph{overfitting} and \emph{underfitting}. 
In the case of overfitting, a model can have high accuracy on the training 
set but perform poorly with new data in the test set because the model is 
overly fit to the training data. By contrast, in the case of underfitting, 
a simple model may not generalize well to new observations as it does not 
include all of the features relevant for predicting the target outcome. 
One of the simplest classification models, K-Nearest Neighbors (KNN) 
provides an example of the tradeoff between training accuracy and test 
set accuracy. KNN classifies observations by assigning the label that 
is most frequent among the \textit{'k'}-number of nearest training samples 
(k is a parameter selected by the user). The accuracy of the KNN classifier 
for the training set and testing set is plotted in Figure 2 as a function
of the parameter k-neighbors. The plot shows that increased accuracy on 
the training set is associated with lower testing set accuracy; conversely, 
increased accuracy on the testing set is related to a decrease in training 
set accuracy. The ideal model is one that optimizes test set accuracy while 
striking a balance between the problems of overfitting and underfitting. 
In the case of KNN, testing set increases slightly between 2 and 4 neighbors, 
but does not improve much beyond 5 neighbors. Therefore, a model with k=4 
neighbors provides a reasonable solution for the data. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Imbalanced Classes}

Previous studies have analyzed the misuse and abuse of prescription opioids
(MUPO) using logistic regression and identified factors that influence MUPO 
such as gender and mental illness \cite{rice12, unick13, jones15, mccabe12}. 
The present study extends previous work by comparing the performance of ten 
classifier models of pain reliever misuse and abuse and evaluating each model 
using four performance metrics (accuracy, sensitivity, precision, $f_1$-score). 
The sample data set has imbalanced classes in the target variable as the 
number of instances of the negative class greatly outnumber instances of 
the positive class. A difficulty of using traditional classification 
algorithms with imbalanced data is that they tend to classify observations
as belonging to the majority class when the class of interest (positive) is 
represented by the minority of observations \cite{brown12, yun09}. This can 
produce a result that underestimates the occurrence of positive cases which 
are misclassified as false negatives. Efforts to address the issue of 
imbalanced data have included sampling methods such as `boosting'. 
As described above, the $f_1$-score is considered a better measure of 
performance than accuracy with data that have imbalanced classes as it 
takes into account both recall and precision. The study also identified 
features important for predicting MUPO. Tradeoffs between model complexity,
performance, and interpretability are discussed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\section{Results}

\subsection{Exploratory Data Analysis}

Table X provides the frequency and percent of respondents who passed or
failed the final evaluation by demographic characteristics. Sixty-three percent 
of the student sample passed their courses (61\% male, 65\% female) and 
approximately 37\% failed their courses (39\% male, 35\% female). The 
proportion of pass/ fail between males and females was not significant. 


Pain reliever misuse was most frequent among individuals who described 
themselves as single, divorced, or with some college education. The 
proportion of pain reliever misuse and abuse was highest among individuals 
between 18 to 25 years of age and decreased among older age groups. Two 
percent of respondents (n=3433) disclosed ever using heroin (2019 males; 
1414 females). Among respondents who reported using pain relievers in the 
past year, the rate of misuse and abuse of pain relievers was twice as 
large for individuals who reported using heroin than those who had not 
used heroin (shown in Figure 3), which is consistent with past findings 
that indicate a connection between MUPO and heroin use 
\cite{muhuri13, unick13}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Frequency of Pain Reliever Misuse and Abuse
  by Demographics Features for NSDUH 2015-2017 Sample.}
  \label{tab:freq}
  \begin{tabular}{llllll}
    \toprule
              & Pass & & & Fail & \\
              & N & \% &  & N & \% \\
    \midrule
    \textbf{Total} & 661 & 63.3\% & & 383 & 36.7\% \\
    \midrule
    Male      & 277 & 61.1\% & & 176 & 38.9\%  \\
    Female    & 384 & 65.0\% & & 207 & 35.0\%  \\
    \midrule
    \textbf{Age Group} &  &  &  &  & \\
    15     & 139 & 71.6\% & &  55 & 28.4\% \\
    16     & 194 & 69.0\% & &  87 & 31.0\% \\
    17     & 171 & 61.7\% & & 106 & 38.3\% \\
    18     & 128 & 56.8\% & &  96 & 43.2\% \\
    19+    & 31 &  44.7\% & &  39 & 55.7\% \\
    \midrule
    \textbf{Area} &  &  &  &  & \\
    Rural    & 155 & 54.4\% & & 130 & 45.6\%  \\
    Urban    & 506 & 66.7\% & & 253 & 33.3\%  \\ 
    \midrule
    \textbf{Parents Status} &  &  &  &  & \\
    Apart     &  81 & 66.9\% & & 107 & 33.2\%  \\
    Together  & 580 & 62.6\% & & 276 & 37.2\%  \\
    \midrule
    \textbf{Family Size} &  &  &  &  & \\
    Less than 3  & 199 & 65.0\% & & 107 & 35.0\%  \\
    3 or More    & 461 & 62.6\% & & 276 & 37.4\%  \\
    \midrule
    \textbf{Activities} &  &  &  &  & \\
    No Activities & 326 & 61.7\% & & 202 & 38.0\%  \\
    Activities    & 335 & 64.9\% & & 181 & 35.1\%  \\
    \midrule
    \textbf{Higher Education Plans} &  &  &  &  & \\
    No Plans  &  21 & 23.6\% & &  68 & 45.6\%  \\
    Planned   & 640 & 67.0\% & & 315 & 33.0\%  \\
    \midrule
    \textbf{Internet} &  &  &  &  & \\
    No        & 118 & 54.4\% & &  99 & 45.6\%  \\
    Ye        & 543 & 65.7\% & & 284 & 34.3\%  \\
    \midrule
    \textbf{Romantic Relationship} &  &  &  &  & \\
    No Relationship & 440 & 65.4\% & & 233 & 34.6\%  \\
    Relationship    & 221 & 59.6\% & & 150 & 40.4\%  \\    
    \midrule
    \textbf{Course} &  &  &  &  & \\
    Portugese  & 452 & 69.6\% & & 197 & 30.4\%  \\
    Math       & 209 & 52.9\% & & 253 & 47.1\%  \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classifier Model Performance}

Performance of the classification models was evaluated in the confusion 
matrices reported in Table 5, which includes the metrics of accuracy, 
sensitivity (recall), precision, and $f_1$-score. The model accuracy 
scores ranged form 86.5\% to 90.7\%. Because of the unbalanced classes in 
the sample data|the proportion of respondents who had not misused or abused 
pain relievers (89\%) was much greater than the proportion who had|the 
$f_1$-score was used as the preferred performance metric rather than 
accuracy. The $f_1$-scores ranged from 0.87 to 0.95. Logistic regression 
and the random forest model were tied for the highest $f_1$-score (0.95) 
followed by the decision tree model (0.949). The random forests model 
identified more true positives (1069) and fewer false negatives (3397) 
than the single decision tree or logistic regression. QDA identified 
the highest number of true positives, but detected far fewer true negatives
than logistic regression or LDA. In general, default parameter setting 
were used for most models; the main parameter settings for the 
classification models are presented in Table 6. The results for the 
three top performing models are described below.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{table*}[ht]
  \caption{Confusion Matrices and Performance Metrics for Classification 
  Models of Pain Reliever Misuse and Abuse}
  \label{tab:freq}
  \begin{tabular}{llllllll}
    \toprule
    Model& & Confusion Matrix & & Accuracy & Sensitivity & Precision & F1-Score \\
    \midrule
    K-Nearest Neighbors & & Fail & Pass &  &  &  & \\
     & Fail & 37567 & 453 & 89.8\% & 0.900 & 0.870 & 0.870 \\
     & Pass & 3905 & 654 &  &  &  & \\
    
    \midrule
    Logistic Regression & & Fail & Pass &  &  &  & \\
     & Fail & 56 &  20 & 74.1\% & 0.48 & 0.727 & 0.598 \\
     & Pass & 58 & 177 &  &  &  & \\
    \midrule
    Linear Discriminant Analysis (LDA) & & Fail & Pass &  &  &  & \\
     & Fail & 55 &  24 & 72.8\% & 0.474 & 0.696 & 0.564 \\
     & Pass & 61 & 173 &  &  &  & \\
    \midrule
    Quadratic Discriminant Analysis (QDA) & & Fail & Pass &  &  &  & \\
     & Fail & 66 &  21 & 77.3\% & 0.569 & 0.759 & 0.650 \\
     & Pass & 50 & 176 &  &  &  & \\
    
    \midrule
    Support Vector Machines (SVM) & & Fail & Pass &  &  &  & \\
     & Fail & 37660 & 360 & 90.3\% & 0.900 & 0.880 & 0.880 \\
     & Pass & 3776 & 783 &  &  &  & \\
    \midrule
    Naive Bayes & & Fail & Pass &  &  &  & \\
     & Fail & 67 &  24 & 76.7\% & 0.578 & 0.736 & 0.648 \\
     & Pass & 49 & 173 &  &  &  & \\
    \midrule
    Neural Network (MLP) & & Fail & Pass &  &  &  & \\
     & Fail & 378 & 534 & 90.6\% & 0.90 & 0.89 & 0.880 \\
     & Pass & 342 & 116 &  &  &  & \\
    
    \midrule
    Decision Trees & & Fail & Pass &  &  &  & \\
     & Fail &  58 &  25 & 73.5\% & 0.50 & 0.699 & 0.583 \\
     & Pass &  58 & 172 &  &  &  & \\
    \midrule
    Random Forests & & Fail & Pass &  &  &  & \\
     & Fail &  57 &  22 & 74.1\% & 0.491 & 0.722 & 0.585 \\
     & Pass &  59 & 175 &  &  &  & \\
    \midrule
    Gradient Boosted Trees & & Fail & Pass &  &  &  & \\
     & Fail & 37955 &  65 & 89.9\% & 0.90 & 0.89 & 0.895 \\
     & Pass & 4261 & 298 &  &  &  & \\
    \bottomrule
  \end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Logistic Regression}

An initial logistic regression on student performance was run including all 
the predictor variables; the model was rerun leaving out non-significant 
features. The parameter estimates, error, z-values, and odds ratios for the 
second model are presented in Table 7. The coefficient estimates are 
interpreted as the multiplicative increase (or decrease) in the odds of the 
dependent variable event happening given a one unit change in the value of 
the predictor variable (for interval scale), or if the event represented 
by a dichotomous predictor variable occurs (e.g., parents together), holding
constant the effects of other independent variables. The parameters relate to 
the log of the odds ratio rather than to the dependent variable directly. 
The odds are calculated as the probability of the event occurring divided 
by the probability of the event not occurring ( \(\frac{P}{P-1}\) ). The 
odds ratio is obtained by taking the antilog of the estimated coefficient, 
which is the exponentiated parameter estimate ($e^x$). In general, taking the 
antilog of the coefficient estimate, subtracting 1 from it, and multiplying 
the result by 100, provides the percent change in the odds for a unit 
increase in the independent variable  \cite{gujarati09}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Parameter Estimates for Logistic Regression Model and Log-odds 
  Ratios (Training Set).}
  \label{tab:freq}
  \begin{tabular}{lllll}
    \toprule
    Predictor&  Estimate& Std. Err.& Z-Value & Odds Ratio \\    
    \midrule
    (Intercept)         &   0.91 &  0.52 &   1.77     &   \\
    Failures            &  -1.567 &  0.224 &  -6.98 ***  &  4.79  \\
    Course              &  -1.121 &  0.187 &  -6.00 ***  &  3.07  \\
    Higher Ed Plans     &   1.182 &  0.351 &   3.37 ***  &  3.26  \\
    Father's Education  &   0.256 &  0.085 &   2.99 **   &  1.29  \\ 
    School Support      &  -0.792 &  0.269 &  -2.94 **   &  2.21  \\    
    Weekly Alcohol      &  -0.184 &  0.076 &  -2.41 *    &  1.20  \\
    Guardian            &  -0.328 &  0.156 &  -2.11 *    &  1.39  \\
    Internet            &   0.445 &  0.218 &   2.04 *    &  1.56  \\
    Going Out           &  -0.169 &  0.086 &  -1.97 *    &  1.18  \\
    \bottomrule 
    Note: p-value& *$<$ 0.05  & **$<$ 0.01 & ***$<$ 0.001 &   
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As past failures increased by one unit, the odds of successfully passing the 
course decreased by 4.79 times (379\% increase), holding the effects of other 
variables constant. In terms of course, the odds of successfully passing the 
course in mathematics decreased 3.07 times compared to course in Portugese. 
in other words, the odds of students passing the math course was 207\% lower 
than students in the Portugese course. For students with plans to pursue 
higher education, the odds of successfully passing increased by 3.26 times 
(226\%) compared to students with no plans for higher education. A one unit 
increase in father's education level was associated with a 1.29 or 29\% 
increase in a passing final grade. In addition, the odds of passing increased
by 1.56 for internet access; the odds of passing for students with internet 
access at home was 56\% higher than for students without internet access.  
Several factors had a negative impact on student success. For example, the 
odds of passing decreased 2.21 for students who received school support
compared to students who did not receive school support. A one-unit increase
in weekly alcohol consumption decreased the odds of successfully passing by 1.20 
(20\%). The odds of successfully passing increased by 1.39 (39\%) given a unit 
change in custodian guardian ('other', 'father', 'mother'). Finally, a one-unit 
increase in going out with friends decreased the odds of passing by 1.18 (18\%). 

The odds ratio indicates the ratio of the positive event to the negative event, 
but does not reveal the probability of the positive outcome. The terms in the 
logistic regression equation can be rearranged to obtain the function of the 
event probability, as presented in equation 3: 

\begin{equation}
  \ \textit{P} = \frac{1}{1+exp[-(\beta_0 + \beta_1X_1 + \beta_1X_2 +... \beta_p*X_p)]} \
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Decision Trees} 

The decision tree model was prepruned to a maximum depth of 4, which means 
the algorithm split on four consecutive features (see Figure 4). The  
algorithm selected cocaine as the root node which includes the n=127738 
observations in the training set. With a classification tree, we predict 
that each observation will belong to the class of training observations in 
the region to which it belongs, and want to determine not only the class
prediction belonging to a terminal node region, but also the class 
proportions among the training observations in that region \cite{james13}. 
One way to interpret a decision tree is by following the number of samples 
represented at the split for each node. Another way to interpret the decision 
tree in Figure 4 is by the examining the proportion of observations of 
class A captured by that leaf over the entire number of observations captured 
by the leaf during model training. Starting from the root node, the branch 
to the left represents n=112730 observations with no or low cocaine use; 
of those, 0.07 or 7\% reported misusing or abusing opioid pain relievers. 
Following the right branch are n=15008 observations positive for cocaine use, 
of which 0.39 or 39\% reported pain reliever misuse or abuse. This means that 
the proportion of pain reliever misuse and abuse was more than five times as 
large for individuals who reported using cocaine than those who had not. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure4.pdf}
  \caption{Decision Tree Model of Pain Reliever Misuse and Abuse
  fit to the Training Set.}
  \label{f:Figure4}
\end{figure}

The second split was based on heroin use; the left branch included n=12876 
respondents who had not used heroin, of which 0.34 or 34\% reported pain
reliever misuse or abuse. Following the right branch to the terminal node 
(i.e., `leaf'), n=2132 individuals reported heroin use, of which 0.70 or 
70\% had misused or abused pain relievers. Thus, the proportion of PRLMISAB 
was twice as large for respondents who had used heroin than those who had 
not used heroin (as seen in Figure 3). The third split in the decision tree
was based on tranquilizers. The left branch represented n=9757 individuals 
who reported no or low tranquilizer use; of these, the proportion of 
PRLMISAB was 0.28 or 28\%. The branch to the right represented n=3119 
observations with moderate to high tranquilizer use, of which 0.52 or 52\% 
reported PRLMISAB. The rate of pain reliever misuse and abuse for individuals 
with moderate to high tranquilizer use was almost twice as large as for 
those reporting no to low tranquilizer use. The fourth split was based on age 
category; the branch to the left represented n=1394 individuals age 36 or older, 
of which 0.38 or 38\% reported PRLMISAB. The branch to the right represented 
n=1725 individuals age 35 and younger, of whom 0.63 or 63\% reported PRLMISAB. 
This findings shows that pain reliever misuse and abuse was much more likely
among respondents age 35 or younger than among individuals older than 35 years. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Feature Importance for the Random Forests Model}

The random forest model was fit using 1000 trees, with all of the features 
considered at each node to determine the randomness of each tree. After a 
large number of trees is generated, each tree represents a vote for the
most popular class. Although random forests perform well with data that has
imbalanced classes, it can be difficult to interpret the result of the 
averaged trees. Feature importance is a model summary for random forests 
that rates how important each feature is for classification decisions made 
in the algorithm. The Gini index provides a measure of node purity which is 
used to evaluate the quality of a particular split; a small value indicates 
that a node predominantly contains observations from a single class. 
Feature importance was measured by the mean decrease in Gini coefficient 
(Table 8), which indicates how each variable contributes to the homogeneity 
of the nodes and leaves in the resulting random forest. For classification 
trees, the splits are chosen so as to minimize entropy or Gini impurity in 
the resulting subsets. For random forests, variables that result in nodes 
with higher purity have a higher decrease in the Gini coefficient. Feature 
importance is computed by aggregating the feature importance over trees in 
the random forest, and gives non-zero importance to more features than a 
single tree. A feature may have a low importance value because another feature 
encodes the same information. Table 8 provides the feature importance for 
the random forests model sorted by the mean decrease in the Gini coefficient. 
The algorithm selected cocaine as the most informative feature for predicting 
pain reliever misuse and abuse. In contrast to the single decision tree, 
amphetamines, mental health, and health were selected among the top four most 
important features in the random forests model, followed by tranquilizers, 
age category, and heroin use, which were ranked as more influential in the 
single tree (Figure 4). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Feature Importance for the Random Forest Model.}
  \label{tab:freq}
  \begin{tabular}{lll}
    \toprule
    Predictor&  & Mean Decrease  \\    
             &  & in Gini Score  \\
    \midrule
    Failures            &  &  35.99 \\
    Absences            &  &  25.51 \\
    Courses             &  &  17.00 \\ 
    Going Out           &  &  16.31 \\
    Age                 &  &  15.89 \\
    Mother's Education  &  &  15.84 \\
    Free Time           &  &  15.47 \\
    Mother's Job        &  &  14.96 \\
    Weekly Alcohol      &  &  14.56 \\
    Father's Education  &  &  13.79 \\
    Health              &  &  13.68 \\
    Study Time          &  &  13.21 \\ 
    Family Relations    &  &  12.31 \\
    Father's Job        &  &  11.59 \\
    Travel Time         &  &  10.24 \\
    \bottomrule
  \end{tabular}
\end{table}

%Dalc               8.86
%guardian           8.02
%sex                7.46
%higher             7.00
%paid               6.90
%romantic           6.35
%famsup             6.33
%activities         6.30
%famsize            6.24
%internet           6.16
%area               5.87
%schoolsup          5.73
%parents            3.54





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISCUSSION}

The results show that there were imbalanced classes in the sample data given 
that the proportion of respondents who reported no MUPO (i.e., negative class)
was much larger than the 11 percent of respondents who reported pain reliever 
misuse and abuse. Logistic regression, random forests, and decision trees
performed better than other classifier models using the $f_1$-score as the 
appropriate performance metric. Traditional algorithms tend to classify new 
observations based on the majority class, as seen with the naive Bayes model, 
which had a relatively high $f_1$-score despite identifying only 35 true 
positive instances and having a high number of false negatives. The random 
forests model performed slightly better than the other models as it correctly 
predicted more true positive instances than the logistic regression and had 
fewer false negative errors than the single decision tree. The QDA and LDA 
models both identified the largest number of true positives, but each correctly 
labeled fewer true negatives compared to the logistic regression model, which 
had higher $f_1$-score and accuracy overall. This finding indicates that the 
decision boundary for classifying pain reliever misuse and abuse can be modeled 
as a linear function of the predictor variables \cite{james13, raschka17}. 
The decision tree and random forests models would be preferred if pain reliever 
misuse and abuse were modeled as a non-linear function of the predictors,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Cocaine use was selected by all three models as the most informative predictor 
of pain reliever misuse and abuse (PRLMISAB). The odds of PRLMISAB increased 
99 percent given a one unit increase in cocaine use, holding other independent
variables constant. In terms of frequency, the proportion of PRLMISAB among 
individuals who reported moderate to high cocaine use was more than five times 
as large as for individuals who reported no or low cocaine use. Logistic 
regression and random forests both identified amphetamine use as the second 
most important feature for predicting PRLMISAB. The odds of PRLMISAB
increased by 84 percent for a unit increase in amphetamine use. In contrast, 
the single decision tree identified heroin use as the second most important 
predictor. Of those respondents who disclosed heroin use, more than two-thirds 
reported misusing or abusing pain relievers. In other words, the proportion of 
PRLMISAB was twice as large for respondents who had used heroin as for those 
who had not, Similarly, logistic regression model revealed that the odds of 
PRLMISAB increased by 112 percent given previous heroin use, holding other 
variables constant. These findings are consistent with past studies that
indicate a connection between pain reliever misuse and abuse and heroin use
\cite{jones13, jones15, muhuri13, unick13}. Tranquilizers were identified as 
the third most important variable by both the decision tree and logistic 
regression models. The odds of PRLMISAB increased 1.46 times for a one unit 
increase in tranquilizer use. Similar to the pattern observed for heroin, the 
proportion of PRLMISAB was nearly twice as large for respondents with moderate 
to high tranquilizer use as for individuals with no to low tranquilizer use. 

Age category, mental health, health issues, education level, and previous 
drug or alcohol treatment were also identified as important features for 
predicting PRLMISAB; however, the models differed in the importance they 
assigned to these variables. A general finding related to age is that pain 
reliever misuse and abuse was more prevalent among respondents between
18 to 25 years of age and decreased among older individuals. Although more 
females reported using pain relievers than males, more males reported 
misusing or abusing pain relievers than females, and the odds of pain reliever 
misuse or abuse was 17 percent lower for females as for males. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Model Complexity and Interpretability}

Every classification model has advantages and limitations. There is a 
tradeoff between model complexity, performance, and interpretability.
Simple models provide interpretable solutions but have lower accuracy, 
whereas complex models yield improved performance but are more difficult 
to interpret. The advantage of logistic regression is that it provides 
coefficient estimates for individual features that can be interpreted in 
terms of odds ratios. The predicted outcome (Y-hat) is represented by the
weighted combination of all the independent variables. The logit function 
is linear in the parameters, but the probabilities of the expected outcome 
are non-linear. The drawbacks of logistic regression are that it does not 
perform well for modeling non-linear relationships between the target
outcome and predictor variables, or with high dimensional data (e.g., $p>n$). 

Decision trees are useful for modeling nonlinear data and can be 
interpreted in terms of the frequency or proportion of observations selected 
at each branch. A single tree visually represents the decision process in a 
manner that is effective for communicating results; however, a disadvantage 
is that decision can become very complex without pruning, especially as the 
number of predictors increases. Limiting the decision tree to a depth of four 
nodes improves the interpretability, but reduces the generalizability of the 
model to new observations. Random forests reduce overfitting by averaging
multiple different trees to identify class membership, which improves 
generalizability to new data. A limitation of random forests is that feature 
importance is determined by the mean decrease in Gini score, a measure of node
purity that is difficult to interpret in terms of the outcome. Gradient 
boosting typically improves accuracy using many simple models iteratively and 
correcting for individual trees; however, in the present study, the boosting 
model did not perform better than random forests. 

As the complexity of a model increases, interpretation becomes more difficult, 
as seen with neural networks. The multilayer perceptron (MLP) is one of the 
most widely used neural network models for classification. With a limited 
number of predictor variables and single hidden layer, it is possible to 
interpret the relations among weights and nodes in the hidden layer 
(see Appendix). As the number of predictors and hidden layers increases, 
complex neural network become opaque to interpretation and represent a 
``black box'' model that is not comprehensible at a human level. In 
applying sophisticated algorithms in predictive modeling, interpretability 
is often sacrificed for greater model accuracy \cite{elgin18}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Limitations}

A surprising finding is that several models which typically perform well for 
binary classification tasks (gradient boosting, SVM, neural networks) did not 
perform better than was observed in this study. This may be due to imbalanced 
classes in the sample data. A limitation of the study is that the default 
settings were used in constructing the classifier models. Many complex models 
are sensitive to parameter settings and scaling of the data. Additional 
parameter tuning could have improved performance of complex models. Cross-
validation can be used to select hyperparameters that optimize model 
performance. Another limitation is that the aggregated features represent a 
subset of the entire range of features in the NSDUH datasets. In a future 
study, it may be useful to include a more comprehensive set of features to 
identify additional variables for predicting opioid dependence and addiction. 
Given the large proportion of the sample that had not previously used any
prescription opioid pain relievers, it may be useful to classify pain reliever 
misuse and abuse using a subset of individuals who have previously used pain 
medications, although this would result in a reduced sample. It is a widely
accepted truism in predictive analatics that, ``more data is always better''. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Collaboration}

Learning is a complex phenomenon that is not always directly observable and 
often inferred from student behavior in online learning platforms. Theories of 
learning reveal the importance of collaboration; from the social constructivist 
perspective, knowledge is constructed through interaction with more knowledgeable
partners, parents, teachers, or peers (Vygotsky, 1978). Sociological research
has investigated the characteristic structure of social networks, showing the 
strength of weak social connections (Granovetter, 1973). Social network analysis 
(SNA) provides a valuable tool for exploring interactions among learners in 
various contexts. Visualization tools help to reveal connections in large 
datasets and analyze collaboration in online learning platforms (Dawson, 2009). 
Content analysis and automated recommender systems have also been used to guide 
learners toward more personalized learning environments (PLE) (Siemens, 2012, 2013). 
Individual differences in metacognitive (e.g., self-reflection, self-awareness), 
disposition, experience and motivation are influential for developing learning 
relationships (Dawson, et al., 2014; GaÅ¡eviÄ‡, et al., 2015). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

Predictive modeling offers several useful approaches for analyzing the misuse
and abuse of prescription opioids and identifying factors that contribute to 
MUPO. This study compared ten classification models of pain reliever misuse 
and abuse using the $f_1$-score to evaluate model performance with unbalanced 
classes in the data. Logistic regression, random forests, and decision trees 
had the best performance compared to other models. Cocaine use was selected 
as the most informative variable by all three models, followed by amphetamine 
use which was selected as the second most important feature by the logistic
regression and random forests models. The models differed in  the importance 
they assigned to heroin use, tranquilizers, and demographic features such as 
age group, mental health, and health problems. A general conclusion is that 
there are tradeoffs between model complexity, performance, and 
interpretability. A simple decision tree provides an interpretable model 
that is prone to overfitting. Random forests reduce overfitting and improve 
generalizability, but are difficult to interpret. Logistic regression strikes 
a balance between complexity and interpretability, but does not perform well
with non-linear relationships or high-dimensional data. Additional research 
is needed to understand the relationships among the variables identified by 
predictive models. The findings may inform decision making and policy efforts 
to address the opioid crisis and reduce the risk of overdose death. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acks}

Portions of this paper were completed as part of a course project for
`Big Data Applications and Analytics' taught by Professor Gregor von Laszewski 
at Indiana University in Fall 2017. Thanks to the teaching assistants Juliette 
Zurick and Miao Jiang. Thanks to Dallas J. Elgin for encouragement and 
Ed Miles for helpful comments. 

\end{acks}

\bibliographystyle{unsrt} %%ACM-Reference-Format%%
\bibliography{report} 

\appendix

\section{Appendix}

\subsection{Supplemental Figure}

\subsubsection{Neural Network} The multilayer perceptron (MLP) is a simple
back-propogation neural network that takes the features as input; a single 
hidden layer comprises the weighted combination of the input variables, and 
the output layer represents a response probability. During model training, 
the weights of the predictor variables are first randomly initialized and 
then iteratively adjusted to minimize an error function (e.g. gradient 
descent) \cite{brown12}.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
  \caption{Neural Net Classifier: Multilayer Perceptron with a Single Hidden Layer.}
  \label{f:Figure5}
\end{figure}

\end{document} 
