\documentclass[sigconf]{acmart}

\input{format/final}

\begin{document}
  \title{Supervised Learning Models of Student Performance for Learning Analytics}
  \author{Sean M. Shiverick}
  \affiliation{\institution{smshiverick@gmail.com}
  }
\renewcommand{\shortauthors}{S.M. Shiverick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Model interpretation is an important aspect of predictive analytics.

Modeling student performance can help educators to identify factors that 
contribute to student achievement and students who are struggling. This 
paper modeled student performance as a continuous variable using linear 
regression, regression trees, regression random forests, and gradient 
boosted models. 

Student performance was assessed by final course grade (range: 1-20).

The student performance dataset obtained consisted of N=1044 observations from
two secondary schools in Portugal \cite{cortez08}. 

The target variable, student performance, was 
evaluated as a binary outcome (pass, fail). 

Model performance was evaluated on the testing set. 

All three models identified as the most informative feature for predicting 
student performance. 

The models differed in the importance they assigned to

Advantages and limitations of the different models are  discussed. 

\footnote{Address correspondence to \textit{smshiverick@gmail.com}.}

\end{abstract}
\keywords{Predictive Modeling, Supervised Learning, Classification Models}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

an immense volume of data in higher education The exponential increase in 
computing speed and processing power in recent 
decades has led to the generation 
\cite{Daniel17}. 

The development and use of analytic approaches for 
statistical 'analysis and predictive modeling allows researchers to discover 
patterns in data and provide insights about learning for effective decision 
making. Learning Analytics (LA) is an emerging, multidisciplinary field at 
the intersection of learning science, social science, statistics, and computer 
science that leverages big data to understand learning and the environments 
in which it occurs \cite{siemens13}. The development of LA as a field of 
inquiry have been primarily driven by big data in education and the shift 
toward online learning. The primary stakeholders for LA are learners, educators, 
researchers and administrators. The ability to mine large scale data from online 
learning platforms has applications for student success, course design, and 
institutional programs \cite{Lester19}. This paper reviews LA methodologies, 
applications, and considers challenges and opportunities for implementing 
LA in education.

Predictive modeling provides useful methods for analyzing the student 
performance and identifying features that contribute to successful learning. 
Data mining can predict individuals who may be susceptible to failing or 
dropping out and provide insights for improving student retention. This study 
compares the performance of several classification models to determine the 
best approaches for modeling student performance and success. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 1. Learning Analytics (LA) and Educational Data Mining (EDM)}

LA is a form of education data mining (EDM) that uses research methods and 
predictive analysis to implement platforms for improving student performance and 
instructional design \cite{Baker09, Lester19}. LA and EDM share similar goals 
at the intersection of learning science and data-driven analytics, but these 
approaches differ in origins, emphasis, techniques, and knowledge discovery 
\cite{siemens12}. For example, EDM takes a reductionist approach to analyzing 
individual components in the learning process that focuses on automated discovery
and adaptive learning (e.g., intelligent tutoring). By contrast, LA takes a 
holistic viewpoint to understand the complexity of learning, by leveraging human 
judgment to inform and empower instructors and learners \cite{Papamitsiou14}. 
The development of LA as a separate field of inquiry from EDM has shown a gradual 
shift from a technological focus toward an educational focus or learner-focused 
analytics (Ferguson, 2012). Despite the differences in origin and goals of LA 
and EDM, they can be seen as complementary approaches using similar methodologies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Education institutions have examined factors such as persistence and social 
integration as influential for learning and used analytic methods to track 
student drop-out rates \cite{ferguson12}. The development of the web opened 
up new avenues for collecting information from online sources along with methods 
for processing and sharing content. As extensive educational datasets became 
available for analysis, EDM researchers use a variety of data mining techniques 
(e.g., classification, clustering, association rule mining) to discover 
potentially useful patterns in data for understanding students and the settings 
in which they learn \cite{bakerYucef09}. To a large extent, EDM emerged from 
a computational approach to analyzing logs of student-computer interactions with
data mined from LMS, ITS (intelligent tutoring system), and technology enhanced 
learning (TEL) systems to evaluate learning processes, enhance web-based learning 
and help learners \cite{penaAyala14, romero10}. Thus, EDM research has focused 
more on the technical challenge of extracting value about learning from big data 
in education. Practitioners distinguish LA as a separate field of research by a 
more holistic, education-focused approach to learning, which seeks to ground 
analytics in learning theory, to benefit learners and teachers, and optimize 
learning in online environments \cite{lang17, papamitsiou14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Together, LA and EDM represent an ecosystem of methodologies and techniques 
for gathering, processing, and acting on data to promote learning. These 
procedures facilitate the preparation, measurement, and collection of data 
about learning activities for subsequent analysis, interpretation, and r
eporting. Much LA/EDM research data is collected within a virtual learning 
environment (VLE), learning management system (LMS), or massive open online 
course (MOOC), which can automatically track student participation by login 
frequency, number of resources accessed, time to solve tasks, response times 
to answer questions, number of discussion posts or chat messages between 
participants, questions submitted to instructors. In addition to quantitative 
measures, meaningful analytics systems will include qualitative measures about 
the content or type of contribution (on-topic, evaluative, question). 
Researchers have also studied how affective states (e.g., boredom, 
frustration, confusion, happiness) influence engagement and learning outcomes 
\cite{pardos14}. Merging LMS data with institutional data about student 
performance (i.e., past grades) can provide valuable insights about the 
conditions for successful learning and decision making (Hora, 2019). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LA/EDM consists of a diverse set of methodologies and approaches for 
addressing challenges in higher education such as underprepared students, 
decreased retention, and foster student success. The most common approaches in 
LA research are modeling students and student behavior, predicting performance, 
dropout, and retention, increasing (self-)reflection and (self-)awareness, 
analyzing connections among learners, improving feedback and assessment, and 
recommending educational resources \cite{lang17, lester19, papamitsiou14}. 
Descriptive, correlational, and predictive methodologies are used to study 
learning processes and context, including predictive modeling, relationship 
mining, similarity grouping, content analysis, and social network analysis. 
Classification is the most commonly used data analytic method, followed by 
clustering, regression (logistic, multiple), and predictive modeling (discovery
with models). Several performance metrics are used to evaluate different 
methods: precision, accuracy, sensitivity, coherence, fitness measures, 
similarity weights \cite{bakerYucef09}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 2. Student / Student Behavior Modeling}.
Modeling student characteristics takes a descriptive approach to reveal factors 
for learning such as domain knowledge, 
motivation, metacognition, attitudes, affect, metacognitive states, learning 
strategies, and behavior \cite{Papamitsiou14}. Student characteristics can be 
combined with data from the LMS system about support, teacher feedback, 
curriculum, and course sequencing. Modeling student characteristics is most 
often based on frequency, but can involve correlation analysis, similarity 
grouping or clustering (k-means), content analysis (i.e., text mining, NLP). 
Quantitative data about activities and behavior (e.g., response time, or 
time reflecting on hints) are helpful for modeling engagement or disengagement 
in learning activities, and provide information that allows software to respond 
to individual differences. Discovery and modeling of behaviors during learner 
interactions in a MOOC.  Clustering can be used to grouping similar individuals 
together based on learner profiles and to aid in the development of more 
personalized learning environment (PLE)\cite{Vellido10}. Analysis of qualitative 
data, may rely on human judgment, can reveal deeper concepts about learning, 
such as inferring reasoning strategies from interaction with a cognitive tutor 
\cite{Fournier11}. Predictive approaches are used also for modeling student 
behavior and assessing functionality in intelligent tutoring systems (ITS)
\cote{penaayala14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predicting Performance} 

The Course Signals program developed at Purdue University 
is one of the most well known platforms for predicting students at-risk of 
falling behind using statistical analysis of LMS data \cite{arnoldPistilli12}. 
In addition to grades, demographic information, academic history, and student 
interaction on the Blackboard LMS were used to track student performance. A 
predictive algorithm was used to calculate likelihood of student success based 
on performance, effort, history, and student characteristics. The course signals 
provided students with real-time feedback about their status via the LMS as 
traffic signal indicators (i.e., red=high risk, yellow=moderate risk, green=low 
risk). Instructors enacted interventions for high risk students providing them 
with actionable information about their performance in emails, texts, referrals
to academic advising, or academic resources center, and face to face meetings.
Courses that implemented the Signals program and provided feedback for students
showed an increase in satisfactory grades, decrease in withdrawals, and 
improved retention. Course signals helped to integrate students into the 
university academically in several ways: first, by facilitating contact 
between faculty and students, second, by providing high risk students with 
resources for student success, and third, by using analytics with real time 
data about student performance. in terms of academic success and retention, 
underprepared students in difficult courses using the signals program fared 
better than more well-prepared students in courses that were not using the 
signals program. Furthermore, LA provided faculty with a useful tool for 
identifying struggling students and encouraging them to take corrective actions.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most common approach used to predict student performance, dropout, and 
retention is classification, which operationalizes performance as a binary 
outcome. Classification methods include logistic regression, naive Bayes 
models, decision trees, machine learning algorithms, and neural networks 
\cite{Lykourentzou09}. Past empirical findings indicate that the number of 
quizzes successfully passed is the most influential predictor of final grades, 
in addition to engagement, participation in course activities, number of posts, 
frequency of events, and time spent in the course LMS 
\cite{Papamitsiou14, romerozaldivar12}. Affect and motivation are also 
influential for predicting performance; notably, concentration and 
frustration are significantly correlated with final grades, but boredom and 
confusion were negatively related to performance (Pardos et al., 2013). 
Using data-driven machine learning algorithms on student profiles and data 
about activity within the LMS can facilitate early detection of at-risk 
students \cite{Dekkar09}. Furthermore, studentsâ€™ sense of belonging in a 
course is essential for engagement and improved satisfaction which can 
reduces student dropout.measures of student satisfaction can vary according 
to their perceived usefulness or efficiency of training courses. The most 
common reasons for  student disengagement in MOOCs were personal commitments, 
work conflict and course overload (Kizilcec, 2013). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 3. Predictive Modeling}


Predictive methodologies can lead to a better 
understanding of student engagement in online education, behavior patterns 
that predict successful performance, and factors that influence student failure 
and non-retention. 

Predictive modeling, statistical learning, or machine learning describe a 
set of procedures and automated processes for extracting knowledge from 
data \cite{james13, kuhn13, muller17, raschka17}. The two main branches 
of predictive modeling are supervised learning and unsupervised learning. 
Supervised learning problems involve prediction about a specific target 
or outcome variable. If a dataset has no target outcome, unsupervised 
learning methods can help to reveal underlying structure in the data 
(e.g. clustering). Supervised learning is used to predict an outcome based 
on input provided to a model, when examples of input/output pairs are 
available in the data \cite{muller17}. A statistical learning model is 
constructed using a set of observations to train the model and then make
predictions with new observations. Two main approaches for supervised learning 
problems are regression and classification. When the target variable
is continuous or there is continuity in the outcome (e.g. home prices), a 
regression model tests how a set of features predicts the target variable. 
If the target is a class label, binary variable, or set of categories 
(e.g., spam or ham emails, benign or malignant cells), a classification model 
will predict which class or category label new instances are assigned to. 
This study used a supervised learning approach to classify prescription pain
reliever misuse and abuse as a binary outcome. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Educational institutions collect large volumes of data about students yet 
much student information is protected to ensure privacy. 

Data on student performance used in the present study was obtained from the
UC-Irvine machine learning repository (UCI-MLR) from two secondary schools 
in Portugal \cite{cortez09}. The dataset includes information collected using
students surveys and school grade records. The predictor variables of interest
were demographic information about family characteristics (e.g., ).

The target variable was student achievement (pass/fail) assessed by the 
final grade report in two courses, mathematics and Portugese language. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Variables Included in the Sample Data for Model Construction.}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    \textit{Predictor Variables} &    \\
    \midrule
    1. Sex (0=Male,1=Female) & SEX  \\  
    2. Age (15, 16, 17, 18, 19+ years) & AGE  \\
    3. Home address type (0=Rural, 1=Urban) & AREA  \\
    4. Family size (0=Three or less, 1=More than three) & FAMSIZE  \\
    5. Parents' cohabitation status (0=Separate, 1=Together) & PARENTS  \\ 
    6. Mother's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & MEDU  \\
    7. Father's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & FEDU  \\
    8. Mother's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & MJOB  \\
    9. Father's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & Fjob  \\
    10. Student's guardian (0=Other, 1=Father, 2=Mother) & Guardian  \\
    11. Time from home to school (1=<15 min, 2=15-30 min, 3=30-60 min, 4=>60 min) & TRAVEL  \\
    12. Weekly study time (1=<2 hours, 2=2-5 hours, 3=5-10 hours, 4=>10 hours) & STUDY  \\
    13. Failures in past classes (numeric: n if 1<=n<3, else 4) & FAILURES  \\
    14. Extra educational support (0=No, 1=Tes) & SCHOOLSUP  \\
    15. Family educational support (0=No, 1=Tes) & FAMSUP  \\
    16, Paid extra subject classes (0=No, 1=Tes) & PAID  \\
    17. Extra-curricular activities (0=No, 1=Tes) & ACTIVITIES  \\
    18. Wants to take higher education (0=No, 1=Yes) & HIGHER  \\
    19. Internet access at home (0=No, 1=Tes) & INTERNET  \\
    20. In a romantic relationship (0=No, 1=Tes) & ROMANTIC  \\
    21. Quality of family relationships (1=Very Bad, 5=Excellent) & FAMREL  \\
    22. Free time after school (1=Very Low, 5=Very High) & FREETIME  \\
    23. Going out with friends (1=Very Low, 5=Very High) & GOOUT  \\
    24. Workday alcohol consumption (1=Very Low, 5=Very High) & DALC  \\
    25. Weekend alcohol consumption (1=Very Low, 5=Very High) & WALC  \\
    26. Current health status (1=Very Bad, 5=Very Good) & HEALTH  \\ 
    27. Number of school absences (Count range: 0 to 93) & ABSENCES  \\
    28. Course in mathematics or Portugese (0=Portugese, 1=Math) & COURSES  \\
    \midrule
    \textit{Target Variable} &  \\
    \midrule
    29. Final course grade (0=Lowest, 20=Highest) & GRADE \\
    \bottomrule
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Supervised Machine Learning} 

Machine learning is a set of procedures and automated processes for extracting 
knowledge from data, that fall into two main branches: supervised learning and 
unsupervised learning. Supervised learning problems involve prediction about a 
specific target variable or outcome of interest. If a given dataset has no 
target outcome, unsupervised learning methods can be used to discover underlying 
structure in unlabeled data. Supervised learning is used to predict a certain 
outcome from a given input, when examples of input/output pairs are available 
\cite{muller17}. The project uses supervised learning to predict prescription 
opioid misuse and abuse. A machine learning model is constructed from the 
training set of input-output pairs, to predict new test data not previously 
seen by the model. The two major approaches to supervised learning problems are 
regression and classification. When the target variable to be predicted is 
continuous, or there is continuity between the outcome (e.g., home values, 
income), a regression model is used to test the set of features that predict 
the target variable. If the target is a class label, set of categorical or 
binary outcomes (e.g., spam or ham, benign or malignant cells), classification 
is used to predict which class or category label new instances are assigned to.
Comparing the performance of different learning algorithms can be helpful for 
selecting the best model for a given problem \cite{raschka17}. Several different 
learning algorithms are considered below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Linear Model and Regularization} 

Linear models make a prediction using a linear function of the input features
\cite{muller17}. The standard linear model, given in equation 1, describes the
relationship between predicted target variable (Y) from a set of features 
(X[1] ... X[p]), with some measure of error . 

\begin{equation}
  \ Y = \beta[0] + \beta[1]*X[1] + \beta[2]*X[2] +... + \beta[p]*X[p] + \epsilon
\end{equation}

The predicted value of the target outcome can be thought of as the weighted 
sum of the input features with the weights or coefficients (i.e., beta values) 
indicating the influence of a given feature on the outcome. In the number of
observations ('n') is much larger than the number of features ('p'), ordinary 
least squares estimates will have low variance, and perform well on test
observations. If n is not much larger than p high variability in the least
squares fit can result in overfitting and poor prediction on test observations.
For high-dimensional datasets, where p is much greater than n, the least
squares coefficient estimate breaks down. The simple linear model can be
improved by using alternative fitting approaches that produce better 
prediction accuracy and model interpretability \cite{statlearn13}. Three
important methods for improving the linear model fit are (a) subset selection,
(b) dimension reduction, and (c) regularization or shrinkage. This paper 
focuses on regularization which includes all p predictor features in the
linear model, but constrains or regularizes the coefficient estimates, 
effectively by shrinking them towards zero. Regularization reduces variablity 
which improves test set accuracy with a slight increase in bias. In many cases, 
multiple features in a regression analysis are not associated with the target
response; shrinking the coefficient estimates of irrelevant features to zero 
reduces overfitting and provides a more interpretable model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ridge Regression and the L2 Penalty} 

Similar to least squares, ridge regression seeks coefficient estimates that
fit the data well by reducing error, but ridge regression introduces a 
shrinkage penalty (L2) that has the effect of shrinking the coefficient 
estimates towards zero. When the tuning parameter (lambda) is set to zero, 
the shrinkage penalty has no effect and ridge regression produces the least
squares estimates. As the tuning parameter lambda increases, values of the 
ridge regression coefficients approach zero \cite{statlearn13}. Selecting a 
good value of the tuning parameter lambda is important, as ridge
regression will produce a different set of coefficients for each new value of 
lambda. It is important to note that the shrinkage penalty is applied to every
features, but not to the intercept (beta[0]). The advantage of ridge 
regressions over least squares is based on the bias-variance tradeoff. As 
the tuning parameter lambda increases, flexibility of the ridge regression 
decreases, leading to decreased variance but increased bias. Ridge regression 
is often applied after standardizing the predictor variables so that they are 
all on the same scale (e.g., M=0, SD=1). Ridge regression performs 
well with high-dimensional datasets (p>>n) by trading off a small increase
in bias for a large decrease in variance. A disadvantage of ridge regression 
is that, because it includes all predictors in the model, the penalty shrinks
the coefficients toward zero, but does not set any of them exactly to zero, 
which can create a problem for model interpretation with a dataset that
has a very large number of predictor variables. The Lasso is a relatively
recent approach to overcome this limitation. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Lasso and the L1 Penalty} 

The lasso and ridge regression have similar formulations, but the lasso has a 
major advantage over ridge regression as it produces simpler, more interpretable
models based on a subset of features. The lasso uses an L1 penalty which has the 
effect of forcing some of the coefficient estimates to be equal exactly to zero 
when the tuning parameter lambda is sufficiently large \cite{statlearn13}. Thus, 
the lasso performs variable selection, and produces sparse models based on a 
subset of the features, which are generally easier to interpret than ridge 
regression. In this sense, the lasso is similar to best subset selection, as it
tries to find the set of coefficient estimates that lead to the smallest RSS. 
Selecting a good value of the tuning parameter lambda is important, and cross-
validation is used to choose an optimal value. In terms of the bias-variance
tradeoff, the lasso is qualitatively similar to ridge regression. As lambda
increases, the variances decreases and bias increases somewhat; however, the
variance of ridge regression is slightly lower than the variance of the loasso.
The lasso implicitly assumes that a number of the feature coefficients or
weight truly equal to zero. In general, the lasso can be expected to perform
better than ridge regression in situations where a small number of features
account for most of the variability in the target outcome, and the remaining
features have coefficients that are very small or equal to zero. Ridge 
regression performs better when the target is a function of a large number of 
predictors that contribute approximately equal coefficients. Cross-validation
can be used to determine which approach is better for a given problem. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decision Trees and Random Forests}

Decision tree models are widely used for classification and regression. Tree 
models ``learn'' a hierarchy of if-else questions that are represented in the
form of a decision tree. Building decision trees proceeds from a root node as 
the starting point and continues through a series of decisions or choices.
Each node in the tree either represents either a question or a terminal node 
(i.e.,leaf) that contains the outcome. In constructing the tree, the algorithm 
searches through all possible decisions or tests, and find a solution that is 
most informative about the target outcome. Decision tree regression is used 
for continuous target outcomes. The recursive branching process of tree based 
models yields a binary tree of decisions, with each node representing a test 
that considers a single feature. This process of recursive partitioning is 
repeated until each leaf in the decision tree contains only a single target. 
Prediction for a new data point proceeds by checking which region of the 
partition the point falls in, and predicting the majority in that feature space. 
The main advantage of tree based models is that they require little adjustment 
and are easy to interpret. A drawback is that they can lead to very complex
models that are highly overfit to the training data. A common strategy to 
prevent overfitting is \emph{pre-pruning}, which stops tree construction early 
by limiting the maximum depth of the tree, or the maximum number of leaves. 
One can also set the minimum number of points in a node required for splitting.
Another approach is to build the tree and then remove or collapse nodes with 
little information (i.e., \emph{post-pruning}). Decision trees work well with 
features measured on very different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ensemble of Random Trees: Random Forest}

A random forest is a collection of decision trees that are slightly different 
from the others, which each overfits the data in different ways. The idea 
behind random forests is that overfitting can be reduced by building many 
trees and averaging their results. This approach retains the predictive power 
of trees while reducing overfitting. Randomness is introduced into the tree 
building process in two ways: (a) selecting a bootstrap sample of the data, 
and (b) selecting features in each node branch \cite{muller17,raschka17}. In 
building the random forest, we first decide how many trees to build (e.g., 10 
or 100), and the algorithm makes different ransom choices so that each tree is 
distinct. The bootstrapping method repeatedly draws random samples of size n 
from the dataset (with replacement). The decision trees are build on these 
random samples that are the same size as the original data, with some points 
missing and some data points repeated. The algorithm also selects a random 
subset of p features, repeated separately each node in the tree, so that 
each decision at the node branch is made using a different subset of features.
These two processes help ensure that all of the decision trees in the random
forest are different. The important parameters for the random forests 
algorithm are the number of sampled data points and the maximum number of 
features; the algorithm could look at all of the features in the dataset
or a limited number. A high value for \emph{maximum-features} will produce 
trees in the random forest that are very similar and will fit the data 
easily based on the most distinctive features, whereas a low value will 
produce trees that are very different from each other, and reduces over-
fitting. Random forests is of the most widely used ML algorithms that works 
well without very much parameter tuning or scaling of data. A limitation of 
this approach is that Random forests do not perform well with very high-
dimensional, data that is sparse data, such as text data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Project Goals} 

The general idea of the project is that the prescription opioid dependency 
and addicted are associated with the use of illicit drugs such as heroin or 
synthetic opioids such as fentanyl. It was hypothesized that individuals who 
report using illicit drugs may also be prone to misusing prescription opioids. 
Machine learning has been for clinical diagnosis of smoking and drug use from 
functional MRI neuroimaging \cite{zhang05, pariyadath14}; to current
knowledge, no research has used machine learning to learn features of 
individuals susceptible to misuse or abuse prescription opioids. This study 
applies supervised machine learning models to identify the set of demographic 
characteristics and mental health attributes that predict prescription opioid 
misuse and abuse. The data for the study was obtained from a large national 
survey on drug use and health (NSHUH-2015) \cite{samhsa16}. Survey research 
provides data on a wide range of issues that people may be reluctant to 
disclose, including mental health disorders, personal medical issues, 
prescription medications, and illicit drug use. Responses to surveys may be 
biased to some degree, and it can be difficult to obtain reliable information 
about illicit or illegal behaviors, but the anonymity of survey response are 
designed to preserve confidentiality and can help to assure more accurate 
disclosures. The data were fitted using several models, including general 
linear models, decision trees, and random forests. This method can help to 
address prescription opioid dependency and addiction in the following ways: 
(i) Identify demographic factors related to prescription opioid abuse, (ii) 
Identify associations between prescription opioid misuse and abuse and 
illicit drug use, (iii) Identify the model that gives the best accuracy
for predicting new observations, and provides an interpretable solution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}

\subsection{Data}

The sample consisted of 1044 students (56.6\% female, \textit{M} age=16.71 
years, \textit{SD}=1.19, \textit{Median}=17 years). Age was measured as 
a categorical variable (\textit{minimum}=15, \textit{maximum}=22); 
\textit{n}=10 individuals between 20 and 22 were added to the category of 
19+ years. The student performance dataset was downloaded from the UCI-MLR 
and saved as a data frame object in a python interactive notebook. The data 
was collected from two secondary schools in the Alentejo region of Portugal 
during the 2005-2006 school year and contained information from a 
questionnaire and school reports of student grades \cite{cortez09}. 
The dataset included thirty independent variables consisting of demographic 
information (e.g. family size, parental education and employment status), 
social/ emotional attributes (e.g. extracurricular activities, romantic
relationship, alcohol consumption), school related variables (e.g., weekly 
time studying, past class failures, absences) related to student
(see Table 1). Student performance was evaluated on a 20 point scale--as in 
other European countries (e.g., France)--at three points during the school 
year (i.e., G1, G2, G3) for two courses, mathematics (n=395) and Portugese 
(n=649). The target outcome was the final course grade. A binary dummy
variable of student performance was calculated based on the measure of 
final exam grades (Pass$>$10, Fail$<=$10) for comparison.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Model Construction}

In constructing and evaluating models it is important to select a model that 
performs well not only with the data used to train the model, but also with 
new observations. The sample was divided using a 70 to 30 percent split to 
create the training set ($n_1$=731) and testing set ($n_2$=313). Regression
analysis proceeded by first fitting the model to training data and then 
evaluating the model on the testing set. All regression models were constructed
in R, including multivariate linear regression, ridge regression (L2 penalty), 
lasso regression (L1 Penalty), regression trees, random forests regression, and gradient boosted regression \cite{james09}. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Summary Table of Student Performance for Final Course Grade 
  (Pass$>=$10, Fail$<$10) by Predictor Variables }
  \label{tab:freq}
  \begin{tabular}{llllll}
    \toprule
              & Pass & & & Fail & \\
    Attribute & N & \% &  & N & \% \\
    \midrule
    \textbf{Total} & 661 & 63.3\% & & 383 & 36.7\% \\
    \midrule
    Male      & 277 & 61.1\% & & 176 & 38.9\%  \\
    Female    & 384 & 65.0\% & & 207 & 35.0\%  \\
    \midrule
    \textbf{Past Course Failures} &  &  &  &  & \\
    None            & 622 & 72.2\% & &  239 & 27.8\% \\
    One             & 33  & 27.5\% & &   87 & 72.5\% \\
    Two             & 5   & 15.2\% & &   28 & 84.8\% \\
    Three           & 1   &  3.0\% & &   29 & 96.7.0\% \\
    \midrule
    \textbf{Higher Education Plans} &  &  &  &  & \\
    Planned         & 640 & 67.0\% & & 315 & 33.0\%  \\
    No Plans        &  21 & 23.6\% & &  68 & 76.4\%  \\
    \midrule
    \textbf{Study Time} &  &  &  &  &     \\
    More than 10 hrs. &  45 & 72.6\% & &  17 & 27.4\% \\
    5 to 10 hrs.      & 123 & 75.9\% & &  39 & 24.1\% \\
    2 to 5 hrs.       & 321 & 63.8\% & & 182 & 36.2\% \\    
    Less than 2 hrs.  & 172 & 54.3\% & & 145 & 45.7\% \\
    \midrule
    \textbf{Course} &  &  &  &  & \\
    Portugese       & 452 & 69.6\% & & 197 & 30.4\%  \\
    Math            & 209 & 52.9\% & & 186 & 47.1\%  \\  
    \midrule
    \textbf{School Support} &  &  &  &  & \\
    Received        &  63 & 52.9\% & &  56 & 47.1\%  \\
    None            & 598 & 64.6\% & & 327 & 33.4\%  \\ 
    \midrule
    \textbf{Mother's Education} &  &  &  &  & \\
    Higher Ed       & 235 & 76.8\% & &  71 & 23.2\% \\
    Secondary       & 143 & 49.5\% & &  95 & 32.9\% \\
    Grades 5 to 9   & 180 & 75.6\% & & 109 & 45.8\% \\
    Primary         &  98 & 47.5\% & & 106 & 52.5\% \\
    None            &   7 & 77.8\% & &   2 & 22.2\% \\
    \midrule    
    \textbf{Romantic Relationship} &  &  &  &  & \\
    No Relationship & 440 & 65.4\% & & 233 & 34.6\%  \\
    Relationship    & 221 & 59.6\% & & 150 & 40.4\%  \\    
    \midrule
    \textbf{Internet} &  &  &  &  & \\
    No        & 118 & 54.4\% & &  99 & 45.6\%  \\
    Yes        & 543 & 65.7\% & & 284 & 34.3\%  \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\section{Results}

\subsection{Exploratory Data Analysis}

Table 2 provides a summary of descriptive statistics for selected attributes. 
Chi-squared tests of independence were conducted to examine the proportion of 
passing and failing students for each attribute. Student performance did not 
vary significantly by sex (\textit{p}$=$0.20). There was a significant association
between student performance and previous course failures (\textit{p}$<$0.001). 
The majority of students with no past failures received a passing course grade,
whereas the majority of students with one or more failures in previous courses 
received a failing grade. In addition, student performance was significantly 
related to students' plans for higher education (\textit{p}$<$0.001). Two-thirds 
of students who received a passing grade had plans to pursue higher education, 
but more than three-quarters of students who received failing grades had no 
plans for higher education. Study time was significantly associated with 
students performance (\textit{p}$<$0.001). The proportion of passing and failing
grades for students who studied 5 to 10 hours or more per week was 
significantly different than students who studied less than 5 hours per week. 

Student performance also varied by course; more than two-thirds of students in 
the Portugese course received a passing grade, but only slightly more than half 
of the students in the Math course received a passing final grade. There was 
a significant relationship between extra educational school support and 
student performance (\textit{p}$<$0.001). Just over half of students who 
received extra educational support school received a passing grade compared
to nearly two-thirds of students who received no extra educational support,
Mother's education was also related to student performance (\textit{p}$<$0.05). 
As shown in Table 2, the proportion of passing and failing grades varies by 
level of mother's education, but the pattern is non-linear. The relationship
between internet access and student performance was marginally significant 
(\textit{p}$<$0.06). The majority of students in a relationship or not in a
relationship received a passing grade, but a slightly larger proportion of 
students who were not in a relationship received a passing grade compared 
to students who in a relationship. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure1.pdf}
  \caption{Proportion of Students with Plans for Higher Education as a 
  Function of Passing or Failing Final Course Grades}
  \label{f:Figure1}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Coefficient Estimates for Regression Models of Student Performance on 
  Training Set and Testing Set}
  \label{tab:freq}
  \begin{tabular}{lllllll}
    \toprule
                        & Training Set  &           &         & Testing Set &         &          \\
    \midrule
    Variables           & Coefficient   & Est.S.E.  & t-value & Coefficient & Est.S.E.& t-Value  \\
    \midrule
    Intercept           &       10.315***    & 0.873 & 11.811 &          7.31***    & 1.473 &  4.96 \\
    Failures            & \textbf{-1.546}*** & 0.206 & -7.49  & \textbf{-1.574}***  & 0.307 & -5.14 \\
    Higher Ed           & \textbf{ 1.070}**  & 0.476 &   2.25 & \textbf{ 3.210}***  & 0.752 &  4.27 \\
    Study Time          & \textbf{ 0.300}**  & 0.151 &   1.98 & \textbf{ 0.780}***  & 0.250 &  3.12 \\
    Course              & \textbf{-1.918}*** & 0.256 & -7.49  & \textbf{-1.041}**   & 0.427 & -2.44 \\
    School Support      & \textbf{-1.204}*** & 0.392 & -3.07  & \textbf{-1.347}**   & 0.617 & -2.19 \\
    Mother's Education  & \textbf{ 0.382}*** & 0.119 &   3.22 & \textbf{ 0.381}*    & 0.203 &  1.88 \\
    Romantic Relationship& \textbf{-0.633}** & 0.255 &  -2.49 & \textbf{-0.807}*    & 0.441 & -1.83 \\
    Internet Access     & \textbf{ 0.800}**  & 0.311 &   2.57 &          0.224      & 0.521 &  0.43 \\
    Family Relations    & \textbf{ 0.249*}   & 0.136 &  1 .83 &         -0.047      & 0.206 & -0.23 \\
    Going Out           & \textbf{-0.412}*** & 0.109 &  -3.77 &         -0.031      & 0.170 & -0.18 \\
    Father's Job        & \textbf{ 0.269}*   & 0.139 &   1.92 &         -0.008      & 0.234 & -0.03 \\
    Health              & \textbf{-0.219}**  & 0.086 &  -2.56 &          0.002      & 0.145 &  0.02 \\
    \midrule
    \textit{n}          & 730                &       &        &         314         &       &       \\
    \textit{F-Value}    & \textbf{3.29}***   &       &        & \textbf{3.50}***    &       &       \\
    \textit{df}         & (12, 717)          &       &        &         (12, 301)   &       &       \\
    \textit{$R^2$}      &      0.267         &       &        &         0.265       &       &       \\ 
    \textit{Adj. $R^2$} &      0.255         &       &        &         0.236       &       &       \\
    \bottomrule
    Note. Significance levels & *$<$0.10      & **$<$0.05  & ***$<$0.01 & & &
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Linear Regression Model}

Before conducting regression analyses, the correlations matrix among the 
variables was examined. The three course grade variables (G1, G2, G3) were 
highly inter-correlated (all \textit{p}s$<$0.001). The first and second course 
grade variables were excluded from subsequent analyses to control for
multicolinearity. Student performance was regressed on the 28 independent 
variables with the training set; this regression was statistically significant, 
\textit{F}(28, 701) = 9.57, \textit{p} $<$ 0.001, ($R^2$ = 0.277, adjusted 
$R^2$ = 0.248). The regression model was rerun excluding non-significant 
predictors and this model was also statistically significant,
\textit{F}(12, 717) = 21.79, \textit{p} $<$ 0.001, ($R^2$ = 0.267, adjusted 
$R^2$ = 0.255). An anova revealed no significant difference between the two 
models (\textit{F}$<$ 1.0, \textit{p} $=$ 0.91) and the simpler model was 
retained as more parsimonious. The final model was evaluated on the testing 
set and yielded a statistically significant relationship between student 
performance and the independent variables, \textit{F}(12, 301) = 9.05, 
\textit{p} $<$ 0.001, ($R^2$ = 0.265, adjusted $R^2$ = 0.236). 

Table 3 provides the coefficient estimates, standard errors, and t-values of 
predictor variables on the final model for the training and testing sets.
Overall, between 25.5\% to 23.6\% of the variation in the predicted value of 
student performance was due to changes in the independent variables, taking 
into account the number of independent variables in the model. On the training 
set, 12 out of the 28 predictor variables were statistically significant; 
however, only 7 of the 12 predictors in the testing set were significant. 
This difference in variability accounted for may indicate overfitting to the 
training set data, but the lower adjusted R-squared of the test set may be 
due to decreased sample size. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The left side of Table 3 shows that, in the regression with the training set, 
previous course failures, course topic, school support, romantic relationship, 
going out with friends, and overall health were negatively correlated with 
student performance, whereas plans for higher education, study time, mother's
education level, internet access, quality of family relations, and father's 
job were positively related to student achievement. The right side of Table 3
shows the final regression model with the testing set. A one-unit change in 
past course failures yielded a -1.57 decrease in the predicted value of 
student performance, holding the effect of other predictor variables constant. 
For students in the math course, there was a -1.04 decrease in predicted 
performance compared to students in the Portugese course, controlling for all 
other factors. Students receiving school support had a -1.35 lower predicted 
final course grade than students who did not receive school support, holding
other factors constant. There was a -0.81 decrease in predicted performance 
for students in a romantic relationship compared to students who were not in 
a relationship . In terms of positive relationships, having plans to pursue 
higher education yielded a 3.21 increase in predicted student performance 
than having no plans for higher education, controlling for the effect of other
independent variables. A one-unit change in weekly study time resulted in a 
0.78 increase in predicted student performance, holding other variables 
constant. In addition, a unit change in mother's level of education was 
associated with a 0.38 increase in predicted student performance, controlling 
for the effect of other independent variables. 






\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure2.pdf}
  \caption{Ridge Regression Coefficients for Student Performance on Training Set
  with L2 Shrinkage as a function of Lambda}
  \label{f:Figure2}
\end{figure}





\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure3.pdf}
  \caption{Lasso Regression Coefficients for Student Performance on Test Set 
  with L1 Shrinkage as a function of Lambda}
  \label{f:Figure3}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Decision Tree Regression}

olve stratifying or segmenting the
predictor space into a number of simple regions, and making a prediction for 
a new observation using the mean of the training observations in the region 
to which it belongs. A good strategy for building a regression tree is to
grow a very large tree and then prune it back to obtain a subtree that
provides the lowest test error rate. Steps in building a decision tree are:
(1) Use a recursive binary splitting to grow a large tree on training data, 
stopping only when each terminal node has fewer than some minimum number of 
observations. (2) Apply a cost complexity (i.e., weakest link) pruning  to 
the large tee to obtain a sequence of best subtrees, as a function of alpha. 
(3) Use k-fold cross-validation to choose alpha, dividing the training 
observations into k-folds, and for each k=1...K, (a) repeate steps 1 and 2,
(b) evaluate the mean square predictoin error on data in the left-out k-fold,
as a function of alpha, averaging the results for each level of alpha, and 
picking a value of alpha that minimizes the average error. (4) Return the 
subtree that corresponds to the chosen value of alpha. 

The tree library was loaded in R, and a random seed was set for consistency. 
For this analysis, SUICATT, PRLANY were removed from the opioid dataset. The 
training set of 40000 rows was constructed based on a 70/30 split. The tree
was fit to the training data, and the regression returned a tree with 6
terminal nodes, with a residual mean variance of 3.22. The summary of the 
regression tree showed that four features were used in the construction of 
the tree: TRTMENT, COCAINE, HEROINUSE, and TRQLZERS. 

Cross-validation 
was used to determine optimal tree complexity with produces a small reduction
in the residual mean variance of 3.18. Figure X shows the pruned decision tree 
for PRLMISAB with the root node of TRTMENT. 

The left branch shows a low 
score for treatment, and the feature at the next branch was Cocaine use. 
The branche to the right indicates a high score on cocaine then lead to 
the feature heroin use. This shows that respondents with higher scores for 
concaine and heroin use were also more likely to have a high score on 
opioid pain reliever misuse and abuse. Following the right branch of the 
root node, for cases with a high scores for treatment, the next decision 
feature was Tranquilizers, which indicates that for individuals in treatment, 
tranquilizer use was associated with opioid pain reliever misuse and abuse. 
The regression tree model was evaluated on the test set; Figure 9 shows 
the prediction (yhat) plotted against data in the test set.


\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure4.pdf}
  \caption{Decision Tree Model of Student Performance on Training Set}
  \label{f:Figure4}
\end{figure}

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
  \caption{Feature importance for Random Forests Model on Test Set}
  \label{f:Figure5}
\end{figure}







\subsubsection{Decision Trees} 

The decision tree model was prepruned to a maximum depth of 4, which means 
the algorithm split on four consecutive features (see Figure 4). The  
algorithm selected cocaine as the root node which includes the n=127738 
observations in the training set. With a classification tree, we predict 
that each observation will belong to the class of training observations in 
the region to which it belongs, and want to determine not only the class
prediction belonging to a terminal node region, but also the class 
proportions among the training observations in that region \cite{james13}. 
One way to interpret a decision tree is by following the number of samples 
represented at the split for each node. Another way to interpret the decision 
tree in Figure 4 is by the examining the proportion of observations of 
class A captured by that leaf over the entire number of observations captured 
by the leaf during model training. Starting from the root node, the branch 
to the left represents n=112730 observations with no or low cocaine use; 
of those, 0.07 or 7\% reported misusing or abusing opioid pain relievers. 
Following the right branch are n=15008 observations positive for cocaine use, 
of which 0.39 or 39\% reported pain reliever misuse or abuse. This means that 
the proportion of pain reliever misuse and abuse was more than five times as 
large for individuals who reported using cocaine than those who had not. 

%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure1.pdf}
%  \caption{Decision Tree Model of Student Performance on Training Set}
%  \label{f:Figure1}
%\end{figure}

%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure2.pdf}
%  \caption{Feature importance for Random Forests Model on Test Set}
%  \label{f:Figure2}
%\end{figure}

The second split was based on heroin use; the left branch included n=12876 
respondents who had not used heroin, of which 0.34 or 34\% reported pain
reliever misuse or abuse. Following the right branch to the terminal node 
(i.e., `leaf'), n=2132 individuals reported heroin use, of which 0.70 or 
70\% had misused or abused pain relievers. Thus, the proportion of PRLMISAB 
was twice as large for respondents who had used heroin than those who had 
not used heroin (as seen in Figure 3). The third split in the decision tree
was based on tranquilizers. The left branch represented n=9757 individuals 
who reported no or low tranquilizer use; of these, the proportion of 
PRLMISAB was 0.28 or 28\%. The branch to the right represented n=3119 
observations with moderate to high tranquilizer use, of which 0.52 or 52\% 
reported PRLMISAB. The rate of pain reliever misuse and abuse for individuals 
with moderate to high tranquilizer use was almost twice as large as for 
those reporting no to low tranquilizer use. The fourth split was based on age 
category; the branch to the left represented n=1394 individuals age 36 or older, 
of which 0.38 or 38\% reported PRLMISAB. The branch to the right represented 
n=1725 individuals age 35 and younger, of whom 0.63 or 63\% reported PRLMISAB. 
This findings shows that pain reliever misuse and abuse was much more likely
among respondents age 35 or younger than among individuals older than 35 years. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Feature Importance for the Random Forests Model}


Random forests build many different decision trees, that are decorrelated, and 
then averages the trees to reduce variance. The advantage of randome forests is 
that it provides a more accurate model, but the tradeoff is that the large 
ensemble of trees can be more difficult to interpret. The random forest package 
was loaded into R, and the random forests regression model was fit on PRLMISAB, 
with 500 trees, and three variables considered at each split (e.g., mtry=3).
The mean of the squared residuals was 2.99, which shows better performance
in terms of error than a single decision tree. The model accounted for 26
percent of the variance in opioid pain reliever misuse and abuse. Figure 10
shows the feature importance for the random forest regression model by the
percent increase in MSE on the left panel, and as a function of the increase
in node purity on the right panel. 


The random forest model was fit using 1000 trees, with all of the features 
considered at each node to determine the randomness of each tree. After a 
large number of trees is generated, each tree represents a vote for the
most popular class. Although random forests perform well with data that has
imbalanced classes, it can be difficult to interpret the result of the 
averaged trees. Feature importance is a model summary for random forests 
that rates how important each feature is for classification decisions made 
in the algorithm. The Gini index provides a measure of node purity which is 
used to evaluate the quality of a particular split; a small value indicates 
that a node predominantly contains observations from a single class. 
Feature importance was measured by the mean decrease in Gini coefficient 
(Table 8), which indicates how each variable contributes to the homogeneity 
of the nodes and leaves in the resulting random forest. For classification 
trees, the splits are chosen so as to minimize entropy or Gini impurity in 
the resulting subsets. For random forests, variables that result in nodes 
with higher purity have a higher decrease in the Gini coefficient. Feature 
importance is computed by aggregating the feature importance over trees in 
the random forest, and gives non-zero importance to more features than a 
single tree. A feature may have a low importance value because another feature 
encodes the same information. Table 8 provides the feature importance for 
the random forests model sorted by the mean decrease in the Gini coefficient. 
The algorithm selected cocaine as the most informative feature for predicting 
pain reliever misuse and abuse. In contrast to the single decision tree, 
amphetamines, mental health, and health were selected among the top four most 
important features in the random forests model, followed by tranquilizers, 
age category, and heroin use, which were ranked as more influential in the 
single tree (Figure 4). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Gradient Boosting Regression}

Similar to random forests, gradient boosting builds many smaller trees, but 
each new tree tries to correct for deficiencies of the current ensemble of 
trees. Gradient boosting grows smaller, stubbier trees, and goes after bias. 
The gbm package ("Gradient Boosted Machines", Friedman) was loaded, and the 
gbm() function was called on PRLMISAB using the Gaussian distribution, with
10000 shallow trees, a shrinkage parameters=0.01, and interaction depth of 
4 splits. The relative influence plot in Table 3 shows that the most 
influential features in the gradient boosting model were: TRQLZRS, TRTMENT,
HEROINUSE, COCAINE use, and AMPHETMN. Figure 11 shows the influential 
features of the gradient boosting regression as a horizontal barplot. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Feature Importance for Student Performance from the Random Forests 
  (Node Purity) and Gradient Boosted Model}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
            & Random & Forests & Gradient Boosted \\    
    \midrule   
    
    Predictor Variables & Inc. MSE & Node Purity & Rel. Imp. \\    
    \midrule
    Failures            & 25.89 & 896.51 &  7.69 \\    
    Absences            &  9.08 & 568.34 & 13.66 \\
    Mother's Job        &  7.83 & 449.28 &  4.94 \\   
    Going Out           &  4.02 & 426.00 &  5.68 \\
    Age                 &  9.00 & 407.77 &  4.65 \\
    Mother's Education  & 11.35 & 383.32 &  3.79 \\
    Free Time           &  7.49 & 382.20 &  5.29 \\
    Father's Job        &  7.32 & 367.77 &  5.21 \\       
    Study Time          &  5.99 & 346.16 &  4.23 \\       
    Courses             &  7.25 & 344.43 &  5.55 \\ 
    Health              &  1.56 & 341.38 &  4.85 \\ 
    Weekly Alcohol      &  6.61 & 335.49 &  3.64 \\
    Father's Education  &  6.08 & 334.26 &  3.32 \\    
    Family Relations    &  4.39 & 314.83 &  3.43 \\    
    Daily Alcohol       &  6.32 & 258.02 &  2.30 \\
    Higher Education    & 13.65 & 251.74 &  1.63 \\   
    Travel Time         &  1.60 & 196.75 &  1.80 \\       
    Student Guardian    &  3.79 & 193.83 &  1.85 \\
    Sex                 &  4.87 & 169.87 &  2.05 \\
    Paid extra courses  &  5.34 & 165.76 &  1.69 \\
    Rom. Relationship   &  3.29 & 160.21 &  2.10 \\
    Area                &  2.83 & 153.56 &  1.44 \\
    Family Support      &  3.53 & 151.48 &  1.89 \\
    Extra Activities    &  2.73 & 149.95 &  1.81 \\
    Family Size         &  7.00 & 148.53 &  1.59 \\
    School Support      & 10.88 & 146.88 &  2.22 \\    
    Internet Access     &  4.17 & 128.53 &  1.20 \\
    Parents             &  1.40 &  83.33 &  0.52 \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISCUSSION}

The results showed that prescription opioid use, misuse, and abuse in this 
sample was higher than use of illicit opioids such as heroin and fentanyl. 
The use of Hydrocodone (Vicodan) was double that of Oxycodone (Oxycodone) 
across almost all age groups. Illicit drug use was highest between the ages 
of 18 to 25. Almost twice as many young adults reported a need for substance 
use treatment, but had not received treatment, compared to the youngest age 
group. Of individuals who reported misusing prescription opioid medications, 
twice as many said they had used heroin than had not (see Figure 1), which is 
consistent with the hypothesis that prescription opioid misuse is associated 
with heroin use. The different learning models provided different estimates 
of the features important for predicting pain reliever misuse and abuse. 
The multiple regression showed multiple features together significantly 
predicted pain reliever abuse, but there may be non-linear trends in the data.
Compared to ridge regression, the lasso performed variable selection, 
indicating that a model with five features (Treatment, Heroin, Cocaine, 
Amphetamine, Any Pain Relievers) explained a significant portion of
variabilty in pain reliever abuse, and adding more factors did not improve
the performance greatly. The regression tree model created a solution
with four features (Treatment, Cocaine, Heroin, Tranquilizers), and the
random forest regression selected Tranquilizers as most informative of
pain reliever misuse, which is consistent with the results of the gradient
boosting model tree ensemble (shown in Table 3). Overall, the models 
agreed in selecting the top five most influential predictors of pain
reliever abuse, but there was some variation in the ordering of importance. 
The linear models showed that substance treatment had the highest
coefficients (followed by heroin use), whereas the tree ensemble methods
indicated that tranquilizer use was the most important feature. Given the 
relatively low rates of prescription opioid and heroin use in the sample, 
additional evidence is needed to clarify questions regarding the importance
of various features for predicting opioid misuse and abuse. 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Limitations}

A surprising finding is that 

A limitation of the study is that only a small proportion of the sample 
reported having used or misused prescription opioids. It is possible that the 
level of opioid use in the sample was not representative of opioid use in the 
general population. According to the Centers for Disease Control, the rate
of heroin use in the general population of adults is 2.6 percent, whereas
the rate of heroin use in the NSDUH-2015 sample was 1.6 percent \cite{cdc16}. 
Obtaining reliable information about medication consumption can be difficult 
based on self-reports. Survey data can be biased by under-reporting or by 
minimizing reports of illicit substance use. People may also be reluctant to 
disclose mental health issues or health problems (e.g., STDs, HIV, suicide 
attempts). Another consideration is that, owing to the nature of the methods 
used in the study, it was not possible to determine the direction of the 
relationship between prescription opioid use and heroin use. Individuals who 
misuse or abuse prescription opioid medications may turn to synthetic opioids 
or heroin as cheaper, more readily available than prescription medications. 
Alternatively, individuals who used heroin may be inclined to abuse 
prescription opioids based on availability. Past research has shown that drug 
dosage and medication type play a significant role in opioid misuse: Treatment 
with high daily dose opioids (e.g., more than 120 mg/ day) and short-acting 
schedule II opioids increases the risk of misuse, abuse, and drug overdose 
\cite{sullivan10}. The results of the present analysis showed that demographic 
characteristics played a relatively minor role compared to the use of illicit 
drugs or tranquilizers. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Collaboration}

Learning is a complex phenomenon that is not always directly observable and 
often inferred from student behavior in online learning platforms. Theories of 
learning reveal the importance of collaboration; from the social constructivist 
perspective, knowledge is constructed through interaction with more knowledgeable
partners, parents, teachers, or peers (Vygotsky, 1978). Sociological research
has investigated the characteristic structure of social networks, showing the 
strength of weak social connections (Granovetter, 1973). Social network analysis 
(SNA) provides a valuable tool for exploring interactions among learners in 
various contexts. Visualization tools help to reveal connections in large 
datasets and analyze collaboration in online learning platforms (Dawson, 2009). 
Content analysis and automated recommender systems have also been used to guide 
learners toward more personalized learning environments (PLE) (Siemens, 2012, 2013). 
Individual differences in metacognitive (e.g., self-reflection, self-awareness), 
disposition, experience and motivation are influential for developing learning 
relationships (Dawson, et al., 2014; GaÅ¡eviÄ‡, et al., 2015). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

A general conclusion is that... 
The findings...

A general conclusion is that people who reported misusing prescription opioids 
were also likely to have received substance treatment. More than any other 
demographic features, a history of prescription medication use, or illicit drug 
use, both seemed to contribute highly to the abuse of pain reliever medications, 
particularly for those who reported using tranquilizers, heroin, cocaine, or 
amphetamines. The findings suggest that the opioid crisis may be driven by the 
widespread availability of prescription medications. Even for people with no 
previous history of mental health issues, exposure to highly addictive opioid 
medications puts people at risk for drug dependency and addiction. As mentioned 
in the introduction, even for those who have been in drug treatment programs, 
a lack of continuity in treatment can leave many people in recovery at risk 
for relapse or possible overdose as they are released back into environments 
associated with their drug use. The sharp increase in overdose deaths in the 
U.S. due to synthetic opioids (other than methadone) has coincided with the 
increased availability of illicitly manufactured fentanyl \cite{nida17}. 
Because the dosage levels and potency of illicit opioids are largely unknown, 
there is greater risk of drug overdose death. Recent findings suggest the 
opioid overdose epidemic is getting worse, and requires urgent action to prevent 
opioid abuse, addiction, and death. The findings reported here seek to raise 
awareness about the risk factors for prescription opioid addiction for patients 
and health care providers in order to help reduce opioid overdose deaths. 

Additional research 
is needed to understand the relationships among the variables identified by 
predictive models. The findings may inform decision making and policy efforts 
to address the opioid crisis and reduce the risk of overdose death. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt} %%ACM-Reference-Format%%
\bibliography{report} 


\end{document} 
