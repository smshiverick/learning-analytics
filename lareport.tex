\documentclass[sigconf]{acmart}

\input{format/final}

\begin{document}
  \title{Supervised Learning Models of Student Performance for Learning Analytics}
  \author{Sean M. Shiverick}
  \affiliation{\institution{smshiverick@gmail.com}
  }
\renewcommand{\shortauthors}{S.M. Shiverick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Model interpretation is an important aspect of predictive analytics.

Modeling student performance can help educators to identify factors that 
contribute to student achievement and students who are struggling. This 
paper modeled student performance as a continuous variable using linear 
regression, regression trees, regression random forests, and gradient 
boosted models. 

Student performance was assessed by final course grade (range: 1-20).

The student performance dataset obtained consisted of N=1044 observations from
two secondary schools in Portugal \cite{cortez08}. 

The target variable, student performance, was 
evaluated as a binary outcome (pass, fail). 

Model performance was evaluated on the testing set. 

All three models identified as the most informative feature for predicting 
student performance. 

The models differed in the importance they assigned to

Advantages and limitations of the different models are  discussed. 

\footnote{Address correspondence to \textit{smshiverick@gmail.com}.}

\end{abstract}
\keywords{Predictive Modeling, Supervised Learning, Classification Models}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The large volume of data generated by education institutions has increased 
dramatically due to exponential increases in computing speed and processing 
power in recent decades \cite{Daniel17}. 

Researchers, educators, and 
administrators have used various analytic approaches to identify patterns
in education data to predict students performance, provide insights for
decision making, improve learning and promote student success. 



The development and use of analytic approaches for 
statistical 'analysis and predictive modeling allows researchers to discover 
patterns in data and provide insights about learning for effective decision 
making. 

Learning Analytics (LA) is an emerging, multidisciplinary field at 
the intersection of learning science, social science, statistics, and computer 
science that leverages big data to understand learning and the environments 
in which it occurs \cite{siemens13}. The development of LA as a field of 
inquiry have been primarily driven by big data in education and the shift 
toward online learning. The primary stakeholders for LA are learners, educators, 
researchers and administrators. The ability to mine large scale data from online 
learning platforms has applications for student success, course design, and 
institutional programs \cite{Lester19}. This paper reviews LA methodologies, 
applications, and considers challenges and opportunities for implementing 
LA in education.

Predictive modeling provides useful methods for analyzing the student 
performance and identifying features that contribute to successful learning. 
Data mining can predict individuals who may be susceptible to failing or 
dropping out and provide insights for improving student retention. This study 
compares the performance of several classification models to determine the 
best approaches for modeling student performance and success. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 1. Learning Analytics (LA) and Educational Data Mining (EDM)}

LA is a form of education data mining (EDM) that uses research methods and 
predictive analysis to implement platforms for improving student performance and 
instructional design \cite{Baker09, Lester19}. LA and EDM share similar goals 
at the intersection of learning science and data-driven analytics, but these 
approaches differ in origins, emphasis, techniques, and knowledge discovery 
\cite{siemens12}. For example, EDM takes a reductionist approach to analyzing 
individual components in the learning process that focuses on automated discovery
and adaptive learning (e.g., intelligent tutoring). By contrast, LA takes a 
holistic viewpoint to understand the complexity of learning, by leveraging human 
judgment to inform and empower instructors and learners \cite{Papamitsiou14}. 
The development of LA as a separate field of inquiry from EDM has shown a gradual 
shift from a technological focus toward an educational focus or learner-focused 
analytics (Ferguson, 2012). Despite the differences in origin and goals of LA 
and EDM, they can be seen as complementary approaches using similar methodologies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Education institutions have examined factors such as persistence and social 
integration as influential for learning and used analytic methods to track 
student drop-out rates \cite{ferguson12}. The development of the web opened 
up new avenues for collecting information from online sources along with methods 
for processing and sharing content. As extensive educational datasets became 
available for analysis, EDM researchers use a variety of data mining techniques 
(e.g., classification, clustering, association rule mining) to discover 
potentially useful patterns in data for understanding students and the settings 
in which they learn \cite{bakerYucef09}. To a large extent, EDM emerged from 
a computational approach to analyzing logs of student-computer interactions with
data mined from LMS, ITS (intelligent tutoring system), and technology enhanced 
learning (TEL) systems to evaluate learning processes, enhance web-based learning 
and help learners \cite{penaAyala14, romero10}. Thus, EDM research has focused 
more on the technical challenge of extracting value about learning from big data 
in education. Practitioners distinguish LA as a separate field of research by a 
more holistic, education-focused approach to learning, which seeks to ground 
analytics in learning theory, to benefit learners and teachers, and optimize 
learning in online environments \cite{lang17, papamitsiou14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Together, LA and EDM represent an ecosystem of methodologies and techniques 
for gathering, processing, and acting on data to promote learning. These 
procedures facilitate the preparation, measurement, and collection of data 
about learning activities for subsequent analysis, interpretation, and r
eporting. Much LA/EDM research data is collected within a virtual learning 
environment (VLE), learning management system (LMS), or massive open online 
course (MOOC), which can automatically track student participation by login 
frequency, number of resources accessed, time to solve tasks, response times 
to answer questions, number of discussion posts or chat messages between 
participants, questions submitted to instructors. In addition to quantitative 
measures, meaningful analytics systems will include qualitative measures about 
the content or type of contribution (on-topic, evaluative, question). 
Researchers have also studied how affective states (e.g., boredom, 
frustration, confusion, happiness) influence engagement and learning outcomes 
\cite{pardos14}. Merging LMS data with institutional data about student 
performance (i.e., past grades) can provide valuable insights about the 
conditions for successful learning and decision making (Hora, 2019). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LA/EDM consists of a diverse set of methodologies and approaches for 
addressing challenges in higher education such as underprepared students, 
decreased retention, and foster student success. The most common approaches in 
LA research are modeling students and student behavior, predicting performance, 
dropout, and retention, increasing (self-)reflection and (self-)awareness, 
analyzing connections among learners, improving feedback and assessment, and 
recommending educational resources \cite{lang17, lester19, papamitsiou14}. 
Descriptive, correlational, and predictive methodologies are used to study 
learning processes and context, including predictive modeling, relationship 
mining, similarity grouping, content analysis, and social network analysis. 
Classification is the most commonly used data analytic method, followed by 
clustering, regression (logistic, multiple), and predictive modeling (discovery
with models). Several performance metrics are used to evaluate different 
methods: precision, accuracy, sensitivity, coherence, fitness measures, 
similarity weights \cite{bakerYucef09}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 2. Student / Student Behavior Modeling}.
Modeling student characteristics takes a descriptive approach to reveal factors 
for learning such as domain knowledge, 
motivation, metacognition, attitudes, affect, metacognitive states, learning 
strategies, and behavior \cite{Papamitsiou14}. Student characteristics can be 
combined with data from the LMS system about support, teacher feedback, 
curriculum, and course sequencing. Modeling student characteristics is most 
often based on frequency, but can involve correlation analysis, similarity 
grouping or clustering (k-means), content analysis (i.e., text mining, NLP). 
Quantitative data about activities and behavior (e.g., response time, or 
time reflecting on hints) are helpful for modeling engagement or disengagement 
in learning activities, and provide information that allows software to respond 
to individual differences. Discovery and modeling of behaviors during learner 
interactions in a MOOC.  Clustering can be used to grouping similar individuals 
together based on learner profiles and to aid in the development of more 
personalized learning environment (PLE)\cite{Vellido10}. Analysis of qualitative 
data, may rely on human judgment, can reveal deeper concepts about learning, 
such as inferring reasoning strategies from interaction with a cognitive tutor 
\cite{Fournier11}. Predictive approaches are used also for modeling student 
behavior and assessing functionality in intelligent tutoring systems (ITS)
\cote{penaayala14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predicting Performance} 

The Course Signals program developed at Purdue University 
is one of the most well known platforms for predicting students at-risk of 
falling behind using statistical analysis of LMS data \cite{arnoldPistilli12}. 
In addition to grades, demographic information, academic history, and student 
interaction on the Blackboard LMS were used to track student performance. A 
predictive algorithm was used to calculate likelihood of student success based 
on performance, effort, history, and student characteristics. The course signals 
provided students with real-time feedback about their status via the LMS as 
traffic signal indicators (i.e., red=high risk, yellow=moderate risk, green=low 
risk). Instructors enacted interventions for high risk students providing them 
with actionable information about their performance in emails, texts, referrals
to academic advising, or academic resources center, and face to face meetings.
Courses that implemented the Signals program and provided feedback for students
showed an increase in satisfactory grades, decrease in withdrawals, and 
improved retention. Course signals helped to integrate students into the 
university academically in several ways: first, by facilitating contact 
between faculty and students, second, by providing high risk students with 
resources for student success, and third, by using analytics with real time 
data about student performance. in terms of academic success and retention, 
underprepared students in difficult courses using the signals program fared 
better than more well-prepared students in courses that were not using the 
signals program. Furthermore, LA provided faculty with a useful tool for 
identifying struggling students and encouraging them to take corrective actions.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most common approach used to predict student performance, dropout, and 
retention is classification, which operationalizes performance as a binary 
outcome. Classification methods include logistic regression, naive Bayes 
models, decision trees, machine learning algorithms, and neural networks 
\cite{Lykourentzou09}. Past empirical findings indicate that the number of 
quizzes successfully passed is the most influential predictor of final grades, 
in addition to engagement, participation in course activities, number of posts, 
frequency of events, and time spent in the course LMS 
\cite{Papamitsiou14, romerozaldivar12}. Affect and motivation are also 
influential for predicting performance; notably, concentration and 
frustration are significantly correlated with final grades, but boredom and 
confusion were negatively related to performance (Pardos et al., 2013). 
Using data-driven machine learning algorithms on student profiles and data 
about activity within the LMS can facilitate early detection of at-risk 
students \cite{Dekkar09}. Furthermore, studentsâ€™ sense of belonging in a 
course is essential for engagement and improved satisfaction which can 
reduces student dropout.measures of student satisfaction can vary according 
to their perceived usefulness or efficiency of training courses. The most 
common reasons for  student disengagement in MOOCs were personal commitments, 
work conflict and course overload (Kizilcec, 2013). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 3. Predictive Modeling}

Predictive methodologies can lead to a better understanding of student 
engagement, characteristics that contribute to successful performance, and 
potential failure and non-retention. 

Predictive modeling, statistical learning, or machine learning describe a 
set of procedures and automated processes for extracting knowledge from 
data \cite{james13, kuhn13, muller17, raschka17}. The two main branches 
of predictive modeling are supervised learning and unsupervised learning. 
Supervised learning problems involve prediction about a specific target 
or outcome variable. If a dataset has no target outcome, unsupervised 
learning methods can help to reveal underlying structure in the data 
(e.g. clustering). Supervised learning is used to predict an outcome based 
on input provided to a model, when examples of input/output pairs are 
available in the data \cite{muller17}. A statistical learning model is 
constructed using a set of observations to train the model and then make
predictions with new observations.

Two main approaches for supervised learning 
problems are regression and classification. When the target variable
is continuous or there is continuity in the outcome (e.g. home prices), a 
regression model tests how a set of features predicts the target variable. 
If the target is a class label, binary variable, or set of categories 
(e.g., spam or ham emails, benign or malignant cells), a classification model 
will predict which class or category label new instances are assigned to. 
This study used a supervised learning approach to classify prescription pain
reliever misuse and abuse as a binary outcome. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Educational institutions collect large volumes of data about students yet 
much student information is protected to ensure privacy. 

Data on student performance used in the present study was obtained from the
UC-Irvine machine learning repository (UCI-MLR) from two secondary schools 
in Portugal \cite{cortez09}. The dataset includes information collected using
students surveys and school grade records. The predictor variables of interest
were demographic information about family characteristics (e.g., ).

The target variable was student achievement (pass/fail) assessed by the 
final grade report in two courses, mathematics and Portugese language. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Supervised Learning} 

The two main branches of machine learning supervised learning and unsupervised 
learning. 

Supervised learning problems involve prediction about a 
specific target variable or outcome of interest. 
If a given dataset has no 
target outcome, unsupervised learning methods can be used to discover underlying 
structure in unlabeled data. 

Supervised learning is used to predict a certain 
outcome from a given input, when examples of input/output pairs are available 
\cite{muller17}. The project uses supervised learning to predict student 
performance based on final course grade. 

A machine learning model is constructed from the 
training set of input-output pairs, to predict new test data not previously 
seen by the model. The two major approaches to supervised learning problems are 
regression and classification. When the target variable to be predicted is 
continuous, or there is continuity between the outcome (e.g., home values, 
income), a regression model is used to test the set of features that predict 
the target variable. If the target is a class label, set of categorical or 
binary outcomes, classification is used to predict which class or category label 
new instances are assigned to. Comparing the performance of different learning 
algorithms can help determine which model is best for a given problem and 
the data available \cite{raschka17}. Several different 
learning algorithms are considered below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Linear Model} 

Linear models make a prediction using a linear function of the input features
\cite{muller17}. The standard linear model, given in equation 1, describes the
relationship between predicted target variable (Y) from a set of features 
(X[1] ... X[p]), with some measure of error . 

\begin{equation}
  \ Y = \beta[0] + \beta[1]*X[1] + \beta[2]*X[2] +... + \beta[p]*X[p] + \epsilon
\end{equation}

The predicted value of the target outcome can be thought of as the weighted 
sum of the input features with the weights or coefficients (i.e., beta values) 
indicating the influence of a given feature on the outcome. In the number of
observations ('n') is much larger than the number of features ('p'), ordinary 
least squares estimates will have low variance, and perform well on test
observations. If n is not much larger than p high variability in the least
squares fit can result in overfitting and poor prediction on test observations.
For high-dimensional datasets, where p is much greater than n, the least
squares coefficient estimate breaks down. The simple linear model can be
improved by using alternative fitting approaches that produce better 
prediction accuracy and model interpretability \cite{statlearn13}. 

\subsubsection{Regularization} 

Three
important methods for improving the linear model fit are (a) subset selection,
(b) dimension reduction, and (c) regularization or shrinkage. 

This paper 
focuses on regularization which includes all p predictor features in the
linear model, but constrains or regularizes the coefficient estimates, 
effectively by shrinking them towards zero. Regularization reduces variablity 
which improves test set accuracy with a slight increase in bias. 

In many cases, 
multiple features in a regression analysis are not associated with the target
response; shrinking the coefficient estimates of irrelevant features to zero 
reduces overfitting and provides a more interpretable model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ridge Regression and the L2 Penalty} 

Similar to least squares, ridge regression seeks coefficient estimates that
fit the data well by reducing error, but ridge regression introduces a 
shrinkage penalty (L2) that has the effect of shrinking the coefficient 
estimates towards zero. When the tuning parameter (lambda) is set to zero, 
the shrinkage penalty has no effect and ridge regression produces the least
squares estimates. As the tuning parameter lambda increases, values of the 
ridge regression coefficients approach zero \cite{statlearn13}. 

The advantage of ridge 
regressions over least squares is based on the bias-variance tradeoff. As 
the tuning parameter lambda increases, flexibility of the ridge regression 
decreases, leading to decreased variance but increased bias. Ridge regression 
is often applied after standardizing the predictor variables so that they are 
all on the same scale (e.g., M=0, SD=1). Ridge regression performs 
well with high-dimensional datasets (p>>n) by trading off a small increase
in bias for a large decrease in variance. A disadvantage of ridge regression 
is that, because it includes all predictors in the model, the penalty shrinks
the coefficients toward zero, but does not set any of them exactly to zero, 
which can create a problem for model interpretation with a dataset that
has a very large number of predictor variables. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Lasso and the L1 Penalty} 

The lasso and ridge regression have similar formulations, but the lasso has a 
major advantage over ridge regression as it produces simpler, more interpretable
models based on a subset of features. The lasso uses an L1 penalty which has the 
effect of forcing some of the coefficient estimates to be equal exactly to zero 
when the tuning parameter lambda is sufficiently large \cite{statlearn13}. Thus, 
the lasso performs variable selection, and produces sparse models based on a 
subset of the features, which are generally easier to interpret than ridge 
regression. 



The lasso implicitly assumes that a number of the feature coefficients or
weight truly equal to zero. In general, the lasso performs better than ridge 
regression in situations where a small number of features account for most 
of the variability in the target outcome, and the remaining
features have coefficients that are very small or equal to zero. 

Ridge 
regression performs better when the target is a function of a large number of 
predictors that contribute approximately equal coefficients. Cross-validation
can be used to determine which approach is better for a given problem. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regression Trees}

Decision tree models are widely used for classification and regression. Tree 
models ``learn'' a hierarchy of if-else questions that are represented in the
form of a decision tree. Building decision trees proceeds from a root node as 
the starting point and continues through a series of decisions or choices.
Each node in the tree either represents either a question or a terminal node 
(i.e.,leaf) that contains the outcome. In constructing the tree, the algorithm 
searches through all possible decisions or tests, and find a solution that is 
most informative about the target outcome. Decision tree regression is used 
for continuous target outcomes. The recursive branching process of tree based 
models yields a binary tree of decisions, with each node representing a test 
that considers a single feature. This process of recursive partitioning is 
repeated until each leaf in the decision tree contains only a single target. 
Prediction for a new data point proceeds by checking which region of the 
partition the point falls in, and predicting the majority in that feature space. 
The main advantage of tree based models is that they require little adjustment 
and are easy to interpret. A drawback is that they can lead to very complex
models that are highly overfit to the training data. A common strategy to 
prevent overfitting is \emph{pre-pruning}, which stops tree construction early 
by limiting the maximum depth of the tree, or the maximum number of leaves. 
One can also set the minimum number of points in a node required for splitting.
Another approach is to build the tree and then remove or collapse nodes with 
little information (i.e., \emph{post-pruning}). Decision trees work well with 
features measured on very different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Random Forest Regression}

A random forest is a collection of decision trees that are slightly different 
from the others, where each tree overfits the data in a different way. This 
approach reduces overfitting by building many trees and averaging 
their results. Randomness is introduced into the tree building process by
drawing a bootstrap sample of the data and selecting features at each node 
branch \cite{muller17,raschka17}. 

In building the random forest, we first decide how many trees to build (e.g., 10 
or 100), and the algorithm makes different random choices so that each tree is 
distinct. The bootstrapping method repeatedly draws random samples of size n 
from the dataset (with replacement). The decision trees are build on these 
random samples that are the same size as the original data, with some points 
missing and some data points repeated. 

The algorithm also selects a random 
subset of p features, repeated separately each node in the tree, so that 
each decision at the node branch is made using a different subset of features.
These two processes help ensure that all of the decision trees in the random
forest are different. 

 A limitation of 
this approach is that Random forests do not perform well with very high-
dimensional, data that is sparse data, such as text data.



Feature importance is a model summary for random forests 
that rates how important each feature is for classification decisions made 
in the algorithm. The Gini index provides a measure of node purity which is 
used to evaluate the quality of a particular split; a small value indicates 
that a node predominantly contains observations from a single class. 
Feature importance was measured by the mean decrease in Gini coefficient 
(Table 8), which indicates how each variable contributes to the homogeneity 
of the nodes and leaves in the resulting random forest. For classification 
trees, the splits are chosen so as to minimize entropy or Gini impurity in 
the resulting subsets. For random forests, variables that result in nodes 
with higher purity have a higher decrease in the Gini coefficient. Feature 
importance is computed by aggregating the feature importance over trees in 
the random forest, and gives non-zero importance to more features than a 
single tree. A feature may have a low importance value because another feature 
encodes the same information. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Project Goals} 

The project examines the relationships between demographic characteristics
and student performance 

It was hypothesized that individuals who 
report using illicit drugs may also be prone to misusing prescription opioids. 
Machine learning has been for clinical diagnosis of smoking and drug use from 
functional MRI neuroimaging \cite{zhang05, pariyadath14}; 

to current
knowledge, no research has used machine learning to learn features of 
individuals susceptible to misuse or abuse prescription opioids. This study 
applies supervised machine learning models to identify the set of demographic 
characteristics and mental health attributes that predict prescription opioid 
misuse and abuse. The data for the study was obtained from a large national 
survey on drug use and health (NSHUH-2015) \cite{samhsa16}. Survey research 
provides data on a wide range of issues that people may be reluctant to 
disclose, including mental health disorders, personal medical issues, 
prescription medications, and illicit drug use. Responses to surveys may be 
biased to some degree, and it can be difficult to obtain reliable information 
about illicit or illegal behaviors, but the anonymity of survey response are 
designed to preserve confidentiality and can help to assure more accurate 
disclosures. The data were fitted using several models, including general 
linear models, decision trees, and random forests. This method can help to 
address prescription opioid dependency and addiction in the following ways: 
(i) Identify demographic factors related to prescription opioid abuse, (ii) 
Identify associations between prescription opioid misuse and abuse and 
illicit drug use, (iii) Identify the model that gives the best accuracy
for predicting new observations, and provides an interpretable solution. 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Variables in the Student Performance Dataset \cite{cortez08}}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    \textit{Target Variable} &  \\
    \midrule
    Final course grade (0=Lowest, 20=Highest) & G3 \\
    \midrule
    \textit{Predictor Variables} &    \\
    \midrule
    1. Sex (0=Male,1=Female) & SEX  \\  
    2. Age (15, 16, 17, 18, 19+ years) & AGE  \\
    3. Home address type (0=Rural, 1=Urban) & AREA  \\
    4. Family size (0=Three or less, 1=More than three) & FAMSIZE  \\
    5. Parents' cohabitation status (0=Separate, 1=Together) & PARENTS  \\ 
    6. Mother's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & MEDU  \\
    7. Father's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & FEDU  \\
    8. Mother's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & MJOB  \\
    9. Father's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & Fjob  \\
    10. Student's guardian (0=Other, 1=Father, 2=Mother) & Guardian  \\
    11. Time from home to school (1=<15 min, 2=15-30 min, 3=30-60 min, 4=>60 min) & TRAVEL  \\
    12. Weekly study time (1=<2 hours, 2=2-5 hours, 3=5-10 hours, 4=>10 hours) & STUDY  \\
    13. Extra educational support (0=No, 1=Tes) & SCHOOLSUP  \\
    14. Family educational support (0=No, 1=Tes) & FAMSUP  \\
    15, Paid extra subject classes (0=No, 1=Tes) & PAID  \\
    16. Extra-curricular activities (0=No, 1=Tes) & ACTIVITIES  \\
    17. Wants to take higher education (0=No, 1=Yes) & HIGHER  \\
    18. Internet access at home (0=No, 1=Tes) & INTERNET  \\
    19. In a romantic relationship (0=No, 1=Tes) & ROMANTIC  \\
    20. Quality of family relationships (1=Very Bad, 5=Excellent) & FAMREL  \\
    21. Free time after school (1=Very Low, 5=Very High) & FREETIME  \\
    22. Going out with friends (1=Very Low, 5=Very High) & GOOUT  \\
    23. Workday alcohol consumption (1=Very Low, 5=Very High) & DALC  \\
    24. Weekend alcohol consumption (1=Very Low, 5=Very High) & WALC  \\
    25. Current health status (1=Very Bad, 5=Very Good) & HEALTH  \\ 
    26. Number of school absences (Count range: 0 to 93) & ABSENCES  \\
    27. Course subject (0=Portugese, 1=Mathematics) & COURSE  \\
    
    \bottomrule
  \end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}

\subsection{Data}

The sample consisted of 1044 students (56.6\% female, \textit{M} age=16.71 
years, \textit{SD}=1.19, \textit{Median}=17 years). Age was measured as 
a categorical variable (\textit{minimum}=15, \textit{maximum}=22); 
\textit{n}=10 individuals between 20 and 22 were added to the category of 
19+ years. The student performance dataset was downloaded from the UCI-MLR 
and saved as a data frame object in a python interactive notebook. The data 
was collected from two secondary schools in the Alentejo region of Portugal 
during the 2005-2006 school year and contained information from a 
questionnaire and school reports of student grades \cite{cortez09}. 
The dataset included thirty independent variables consisting of demographic 
information (e.g. family size, parental education and employment status), 
social/ emotional attributes (e.g. extracurricular activities, romantic
relationship, alcohol consumption), school related variables (e.g., weekly 
time studying, past class failures, absences) related to student
(see Table 1). Student performance was evaluated on a 20 point scale--as in 
other European countries (e.g., France)--at three points during the school 
year (i.e., G1, G2, G3) for two courses, mathematics (n=395) and Portugese 
(n=649). The target outcome was the final course grade. A binary dummy
variable of student performance was calculated based on the measure of 
final exam grades (Pass$>$10, Fail$<=$10) for comparison.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Model Construction}

All models were constructed in R using Rstudio following examples provided 
in \emph{An Introduction to Statistical Learning with Applications in R} 
\cite{jamesetal13}. After preliminary exploration of the data, the sample was 
divided into the training set ($n_1$=731) and testing set ($n_2$=313) using 
a 70 to 30 percent split. The models were first fit to training data and then
evaluating on the testing set. 

The regression 
models were constructed using 27 independent variables listed in Table 1. 
After constructing an initial regression model on the training set with the
full set of independent variables, the model was rerun excluding non-
significant predictors to obtain the final model. This model was evaluated
on the testing set. Additional predictors that were statistically significant 
in the testing were entered into a revised regression with the testing set. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Ridge regression (L2 penalty)} 

The glmnet package was used to fit the ridge regression and lasso models. 
The glmnet() function does not use model formula language (e.g., `y ~ x`), 
so the 'x' matrix of predictors and target vector 'y' were passed to the 
model. The model.matrix() function produced a matrix corresponding to the 
27 predictors and automatically transformed any qualitative variables into
dummy variables. The `alpha` parameter in the glmnet() function determines 
what kind of model is fit: alpha=0 is used to fit ridge regression. It is
important to select an appropriate value of the parameter lambda, as the
algorithm generates a different set of coefficients for each value of 
lambda. By default, the glmnet() function performs ridge regression for an 
automatically selected range of lambda values (e.g., 100). The glmnet 
function also standardizes the variables so they are all on the same scale.
The shrinkage penalty is applied to every features, but not the intercept. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Lasso regression (L1 Penalty))} 
 
The lasso is similar to best subset selection as it tries to find the set of 
coefficient estimates that lead to the smallest RSS. Cross-validation is used to 
select an optimal value of the tuning parameter lambda. In terms of the bias-variance
tradeoff, the lasso is qualitatively similar to ridge regression. As lambda
increases, the variances decreases and bias increases somewhat; however, the
variance of ridge regression is slightly lower than the variance of the lasso.
 
The Lasso model was fit using the glmnet() function with alpha=1. The model
automatically calculates correlation estimates for a wide range of lambda
values.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Regression Trees} 

 The tree was fit to the training data, and the regression 
returned a tree with 6 terminal nodes, with a residual mean variance of 3.22. 
The summary of the regression tree showed that four features were used in the 
construction of the tree: TRTMENT, COCAINE, HEROINUSE, and TRQLZERS. 

The tree library was loaded in R, and a random seed was set for consistency. 
For this analysis, SUICATT, PRLANY were removed from the opioid dataset. The 
training set of 40000 rows was constructed based on a 70/30 split. 

The tree library was loaded in R, and a random seed was set for consistency. 
For this analysis,

Cross-validation was used to determine optimal tree complexity with produces 
a small reduction in the residual mean variance of 3.18. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Random Forests Regression} 

The important parameters for the random forests 
algorithm are the number of sampled data points and the maximum number of 
features; the algorithm could look at all of the features in the dataset
or a limited number. A high value for \emph{maximum-features} will produce 
trees in the random forest that are very similar and will fit the data 
easily based on the most distinctive features, whereas a low value will 
produce trees that are very different from each other, and reduces over-
fitting. Random forests is of the most widely used ML algorithms that works 
well without very much parameter tuning or scaling of data.



The random forest model was fit using 1000 trees, with all of the features 
considered at each node to determine the randomness of each tree. After a 
large number of trees is generated, it can be difficult to interpret the result 
of the averaged trees. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Gradient Boosted Regression} 

Similar to random forests, gradient boosting builds many smaller trees, but 
each new tree tries to correct for deficiencies of the current ensemble of 
trees. Gradient boosting grows smaller, stubbier trees, and goes after bias. 
The gbm package ("Gradient Boosted Machines", Friedman) was loaded, and the 
gbm() function was called on PRLMISAB using the Gaussian distribution, with
10000 shallow trees, a shrinkage parameters=0.01, and interaction depth of 
4 splits. 




\begin{table}
  \caption{Correlation Matrix of Previous Course Failures and Course Grade 
  Variables}
  \label{tab:freq}
  \begin{tabular}{lllll}
    \toprule
    Variable    & Failures  & Grade 1 & Grade 2 & Grade 3  \\
    \midrule
    Failures    &  1,00     &  0.37***  & 0.38***   &  0.38***  \\
    Grade 1     &  0.37***  &  1,00     & 0.86***   &  0.81***  \\
    Grade 2     &  0.38***  &  0.86***  & 1.00      &  0.91***  \\  
    Grade 3     &  0.38***  &  0.81***  & 0.91***   &  1.00     \\    
    \bottomrule
    Note. *** \textit{p}$<$0.001 & & &
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure1.pdf}
  \caption{Proportion of Students with Plans for Higher Education as a 
  Function of Passing or Failing Final Course Grades}
  \label{f:Figure1}
\end{figure} 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
 
\begin{table}
  \caption{Summary Table of Student Performance by Final Course Grade 
  (Pass$>=$10, Fail$<$10) for Selected Variables}
  \label{tab:freq}
  \begin{tabular}{llllll}
    \toprule
                    &  \textbf{Pass} & & & \textbf{Fail} & \\
    Attribute & \textit{N} & \% &  & N & \% \\
    \midrule
    \textbf{Total}  & 661 & 63.3\% & & 383 & 36.7\% \\
    \midrule
    Male            & 277 & 61.1\% & & 176 & 38.9\%  \\
    Female          & 384 & 65.0\% & & 207 & 35.0\%  \\
    \midrule
    \textbf{Course} &  &  &  &  & \\
    Portugese       & 452 & 69.6\% & & 197 & 30.4\%  \\
    Math            & 209 & 52.9\% & & 186 & 47.1\%  \\  
    \midrule
    \textbf{School Support} &  &  &  &  & \\
    Received        &  63 & 52.9\% & &  56 & 47.1\%  \\
    None            & 598 & 64.6\% & & 327 & 33.4\%  \\ 
    \midrule    
    \textbf{Higher Education Plans} &  &  &  &  & \\
    Planned         & 640 & 67.0\% & & 315 & 33.0\%  \\
    No Plans        &  21 & 23.6\% & &  68 & 76.4\%  \\
    \midrule
    \textbf{Mother's Education} &  &  &  &  & \\
    Higher Ed       & 235 & 76.8\% & &  71 & 23.2\% \\
    Secondary       & 143 & 49.5\% & &  95 & 32.9\% \\
    Grades 5 to 9   & 180 & 75.6\% & & 109 & 45.8\% \\
    Primary         &  98 & 47.5\% & & 106 & 52.5\% \\
    None            &   7 & 77.8\% & &   2 & 22.2\% \\
    \midrule    
    \textbf{Study Time} &   &  &  &  &     \\
    More than 10 hrs. &  45 & 72.6\% & &  17 & 27.4\% \\
    5 to 10 hrs.      & 123 & 75.9\% & &  39 & 24.1\% \\
    2 to 5 hrs.       & 321 & 63.8\% & & 182 & 36.2\% \\    
    Less than 2 hrs.  & 172 & 54.3\% & & 145 & 45.7\% \\
    \midrule
    \textbf{Romantic Relationship} & &  &  &  & \\
    Yes             & 221 & 59.6\% & & 150 & 40.4\%  \\    
    None            & 440 & 65.4\% & & 233 & 34.6\%  \\
    \midrule
    \textbf{Internet Access} & &  &  &  & \\
    Yes             & 543 & 65.7\% & & 284 & 34.3\%  \\    
    None            & 118 & 54.4\% & &  99 & 45.6\%  \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\section{Results}

\subsection{Exploratory Data Analysis}

Preliminary examination of the data revealed significant correlations 
between previous course failures and the course evaluation variables,
Grade 1, Grade 2, and Grade 3 (Table 2). A chi-squared test of independence 
showed the proportion of passing and failing final grades (G3) was 
significantly related to previous course failures (\textit{p}$=$0.001)
(Figure 1). The majority of students with no past failures successfully 
passed their courses, but only a minority of students with one or more 
previous failures received a passing grade. Past course failures and 
the first two course grades (i.e., Grade 1, Grade 2) were not included 
in the regression models to address the issue of multicolinearity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Table 3 provides summary statistics for selected attributes. Chi-squared tests 
of independence were used to compare the proportion of passing and failing 
students by attribute. Performance did not vary by sex (chi-squared 
\textit{p}$=$0.20); however, student performance did vary significantly by 
course topic (chi-squared \textit{p}$<$0.001); more than two-thirds of students 
in the Portugese course received a passing grade, but only slightly more than 
half of students in the Math course successfully passed. Extra educational 
school support was also significantly related to student performance 
(chi-squared \textit{p}$<$0.001). Just over half of students who received 
extra educational support school received a passing grade compared to nearly 
two-thirds of students who did not receive extra support. In addition, there 
was a significant relationship between performance and students' plans for 
higher education (chi-squared \textit{p}$<$0.001). Two-thirds of students 
who received a passing grade had plans to pursue higher education, whereas 
less than one-quarter of students with no plans for higher education passed 
their courses. In other words, of students with plans for higher education, 
one-third received a failing grade; by contrast, the majority of students who 
failed had no plans to pursue higher education. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The proportion of passing and failing grades varied by level of mother's 
education (chi-squared \textit{p}$<$0.05); however, the relationship was 
non-linear. Study time was significantly associated with student performance 
(chi-squared \textit{p}$<$0.001). The proportion of passing and failing 
grades significantly different for students who studied 5 hours or more 
per week compared to students who studied less than 5 hours per week. 
The association between student performance and romantic relationships
was marginally significant (chi-squared \textit{p}$<$0.06). The proportion 
of passing and failing grades was significantly different for students in 
a relationship than students who were not in a relationship. The relationship 
between internet access and student performance was also marginally significant 
(chi-squared \textit{p}$<$0.06). The proportion of passing and failing grades 
was significantly different for students with access to the internet at home
compared to students without home internet access.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Coefficient Estimates for Regression Models of Student Performance on 
  Training Set and Testing Set}
  \label{tab:freq}
  \begin{tabular}{lllllll}
    \toprule
                        & Training Set      &        &         & Testing Set &         &          \\
    \midrule
    Variables           & Coefficient        & S.E.  & t-value & Coefficient & S.E. & t-Value  \\
    \midrule
    Intercept           &       13.289***    & 2.166 &  6.14  &          6.955*     & 3.629 &  1.92 \\
    Course              & \textbf{-2.225}*** & 0.261 & -8.53  & \textbf{-1.162}***  & 0.445 & -2.61 \\
    School Support      & \textbf{-1.435}*** & 0.416 & -3.45  & \textbf{-1.481}**   & 0.647 & -2.29 \\
    Higher Ed           & \textbf{ 1.695}*** & 0.487 &  3.48  & \textbf{ 4.101}***  & 0.772 &  5.31 \\
    Going Out           & \textbf{-0.442}*** & 0.114 & -3.89  &         -0.097      & 0.178 & -0.55 \\
    Mother's Education  & \textbf{ 0.485}*** & 0.122 &  3.97  & \textbf{ 0.473}**   & 0.213 &  2.25 \\
    Health              & \textbf{-0.262}*** & 0.088 & -2.97  &         -0.008      & 0.152 & -0.05 \\
    Study Time          & \textbf{ 0.454}*** & 0.156 &  2.91  & \textbf{ 0.893}***  & 0.260 &  3.43 \\
    Family Relations    & \textbf{ 0.340}**  & 0.140 &  2.43  &         -0.096      & 0.215 & -0.45 \\
    Age                 & \textbf{-0.250}**  & 0.114 & -2.20  &         -0.034      & 0.191 & -0.18 \\
    Romantic Relation   & \textbf{-0.600}**  & 0.266 & -2.26  & \textbf{-1.127}**   & 0.461 & -2.45 \\
    Internet Access     & \textbf{ 0.819}**  & 0.321 &  2.55  &          0.420      & 0.542 &  0.77 \\
    Family Size         & \textbf{-0.500}*   & 0.278 & -1.80  &         -0.594      & 0.458 & -1.30 \\
    Father's Job        & \textbf{ 0.278}*   & 0.145 &  1.92  &         -0.050      & 0.245 & -0.20 \\
    \midrule
    \textit{n}          &       730          &       &        &         314         &       &       \\
    \textit{F-Value}    & \textbf{15.41}***  &       &        & \textbf{5.96}***    &       &       \\
    \textit{df}         &   (13, 716)        &       &        &      (13, 300)      &       &       \\
    \textit{$R^2$}      &      0.219         &       &        &         0.205       &       &       \\ 
    \textit{Adj. $R^2$} &      0.204         &       &        &         0.171       &       &       \\
    \textit{Resid. S.E} &      3.396         &       &        &         3.643       &       &       \\
    \bottomrule
    Note. Significance levels & *$<$0.10      & **$<$0.05  & ***$<$0.01 & & &
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Linear Regression Model}

Student performance was first regressed on the 27 independent variables 
(Table 1) with the training set; this regression was statistically significant, 
\textit{F}(27, 702) = 7.62, \textit{p} $<$ 0.001 ($R^2$=0.227, adjusted 
$R^2$=0.197). The regression model was rerun excluding the non-significant 
predictors and this regression revealed a significant relationship between
students performance and the independent variables, \textit{F}(12, 717) = 21.79, 
\textit{p} $<$ 0.001 ($R^2$=0.219, adjusted $R^2$=0.204). An anova test revealed 
no significant difference between the two models (\textit{F}$<$ 1.0, 
\textit{p} $=$ 0.91) and the simpler model was retained. In the testing set,
20.5\% of the variation in the predicted value of student performance was due to 
changes in the independent variables, taking into account the number of 
independent variables in the model. For the final model on the training set, 
the predicted value of student performance is explained by the combined effect 
of the coefficients for the predictor variables presented in the left side of 
Table 3, as a weighted average.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{equation*}
% \begin{aligned}
%   Y = 13.29 - 2.23(Course) - 1.44(SchoolSupp) + 1.70(HighEd) - \\
%   (0.44GoOut) + 0.49(MotherEd) -0.26(Health) + \\
%   0.45(StudyTime) + 0.34(FamilyRelations) - 0.25(Age) - \\
%   0.60(RomanticRel) + 0.82(Internet) - \\
%   0.50(FamilySize) + 0.278(FatherJob) 
% \end{aligned}
%\end{equation*}

On the training set, there was a 2.23 decrease in predicted performance 
for students in the math course compared to students in the Portugese course, 
controlling for the effect of other independent variables. Students receiving 
school support had a 1.35 lower predicted final course grade than students 
who did not receive school support, holding all other variables constant. 
Students with plans for higher education had a 1.70 higher predicted final 
grade than students with no plans for higher education (*controlling for 
all other variables. A one-unit change in going out with friends yielded 
a 0.44 decrease in the predicted performance*. A unit change in mother's 
level of education was associated with a 0.49 increase in predicted performance.
A one-unit change in student health resulted in a 0.26 decrease in predicted 
student performance. Importantly, a one-unit change in weekly study time 
resulted in a 0.45 increase in predicted student performance. A one-unit 
change in quality of family relationships resulted in a 0.34 increase in 
student performance. A one-unit increase in students age was associated with
a 0.25 decrease in predicted performance. There was a -0.81 decrease in 
predicted performance for students in a romantic relationship compared to 
students who were not in a relationship, all else being equal. Students with 
access to the internet at home had a 0.82 higher predicted final grade than 
students with not internet access at home. Students whose family size was
greater than 3 had a 0.50 lower predicted performance than students with 
a family size of 3 or less. Finally, a unit change in father's job yielded 
a 0.29 increase in predicted performance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The final linear regression model was evaluated on the testing set which
yielded a significant relationship between student performance and the 
independent variables, \textit{F}(13, 300) = 5.96, \textit{p} $<$ 0.001
($R^2$=0.205, adjusted $R^2$=0.171). On the test set, the final model explained 
20.5\% of the variation in the predicted value of student performance owing 
to changes in the independent variables, or 17.1\% taking into account the 
number of independent variables in the model (adjusted $R^2$). As shown in 
Table 3, 13 of the 27 predictor variables on the training set were 
statistically significant; however, on the testing set only 6 of these 13 
predictors were significant, which suggests that the model was overfit to 
the data in the training set. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{equation*}
 \begin{aligned}
  Y = 6.96 - 1.16(Course) - 1.44(SchoolSupport) + 1.70(HighEd) + \\
  0.49(MotherEd) + 0.45(StudyTime) - 0.60(RomanticRelation)
 \end{aligned}
\end{equation*}

On the training set, there was a 1.16 decrease in predicted performance 
for students in the math course compared to students in the Portugese course, 
controlling for all other variables. Students receiving school support had 
a 1.48 lower predicted final course grade than students who did not receive 
school support, holding all other variables constant. Students with plans for
higher education had a 4.10 higher predicted final grade than students with 
no plans for higher education, controlling for other variables. A unit change 
in mother's level of education was associated with a 0.47 increase in predicted 
student performance, controlling for all other variables. A one-unit change in 
weekly study time resulted in a 0.89 increase in predicted student performance, 
holding constant the effect of other variables. There was a -1.13 decrease in 
predicted performance for students in a romantic relationship compared to 
students who were not in a relationship, all else being equal. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ridge Regression (L2 Penalty)}

Figure 5 shows the coefficients of predictors in the ridge regression model 
with the log values of lambda plotted on the x-axis. As the values of lambda 
become very large, the coefficient values are shrunk towards zero, but 
never equal exactly to zero. Cross-validation was used to select the best 
value of lambda, which was a value around 0. The features with the highest 
coefficient values were 

X, followed by Y, and Z. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure2.pdf}
  \caption{Ridge Regression Coefficients for Student Performance on Training Set
  with L2 Shrinkage as a function of Lambda}
  \label{f:Figure2}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Lasso (L1 Penalty)}

Figure 4 show the coefficients for the lasso regression plotted as a function 
of the log value of lambda. As the values of lambda becomes very large, many of 
the  coefficients are shrunk to be equal exactly to zero. The associated error 
of the lasso is very similar to ridge regression, but the lasso has an 
advantage over ridge regression in that the resulting coefficient estimates 
are sparse, and only a subset of the predictors are selected in the model. 
Cross-validation was used to select the best value of lambda, which 
revealed that a log value of lambda equal to -2.5 in the optimal range.


\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure3.pdf}
  \caption{Lasso Regression Coefficients for Student Performance on Test Set 
  with L1 Shrinkage as a function of Lambda}
  \label{f:Figure3}
\end{figure}

Figure 7 plots the fraction of deviance explained (i.e., variance accounted 
for) of the coefficients of predictors in the lasso model. As seen in 

Figure 7, a model that includes five predictors (TREATMENT, HEROINUSE, 
COCAINE, AMPHETAMINES, PRLANY) accounts for approximately 20 percent of the 
variability in opioid pain reliever misuse and abuse. Adding additional 
predictors to the model does not increase the fraction of deviance explained, 
and could likely result in a model that would overfit the data. 

In a model 
with five predictors, substance treatment and heroin use have the highest 
coefficient values. This reveals a positive relationship between pain reliever 
misuse, abuse, and drug treatment; respondents who reported misusing and 
abusing prescription opioid pain relievers also were likely to be or have
received drug treatment. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decision Tree Regression}

olve stratifying or segmenting the
predictor space into a number of simple regions, and making a prediction for 
a new observation using the mean of the training observations in the region 
to which it belongs. A good strategy for building a regression tree is to
grow a very large tree and then prune it back to obtain a subtree that
provides the lowest test error rate. Steps in building a decision tree are:
(1) Use a recursive binary splitting to grow a large tree on training data, 
stopping only when each terminal node has fewer than some minimum number of 
observations. (2) Apply a cost complexity (i.e., weakest link) pruning  to 
the large tee to obtain a sequence of best subtrees, as a function of alpha. 
(3) Use k-fold cross-validation to choose alpha, dividing the training 
observations into k-folds, and for each k=1...K, (a) repeate steps 1 and 2,
(b) evaluate the mean square predictioin error on data in the left-out k-fold,
as a function of alpha, averaging the results for each level of alpha, and 
picking a value of alpha that minimizes the average error. (4) Return the 
subtree that corresponds to the chosen value of alpha. 




The treewas fit to the training data, and the regression returned a tree 
with 6terminal nodes, with a residual mean variance of 3.22. 

The summary of the 
regression tree showed that four features were used in the construction of 
the tree: TRTMENT, COCAINE, HEROINUSE, and TRQLZERS. Cross-validation 
was used to determine optimal tree complexity with produces a small reduction
in the residual mean variance of 3.18. Figure 8 shows the pruned decision tree 
for PRLMISAB with the root node of TRTMENT. The left branch shows a low 
score for treatment, and the feature at the next branch was Cocaine use. 
The branche to the right indicates a high score on cocaine then lead to 
the feature heroin use. This shows that respondents with higher scores for 
concaine and heroin use were also more likely to have a high score on 
opioid pain reliever misuse and abuse. Following the right branch of the 
root node, for cases with a high scores for treatment, the next decision 
feature was Tranquilizers, which indicates that for individuals in treatment, 
tranquilizer use was associated with opioid pain reliever misuse and abuse. 
The regression tree model was evaluated on the test set; Figure 9 shows 
the prediction (yhat) plotted against data in the test set.




Figure X shows the 
pruned decision tree for PRLMISAB with the root node of TRTMENT. 

The left branch shows a low score for treatment, and the feature at the next 
branch was Cocaine use. 
The branch to the right indicates a high score on cocaine then lead to 
the feature heroin use. This shows that respondents with higher scores for 
concaine and heroin use were also more likely to have a high score on 
opioid pain reliever misuse and abuse. Following the right branch of the 
root node, for cases with a high scores for treatment, the next decision 
feature was Tranquilizers, which indicates that for individuals in treatment, 
tranquilizer use was associated with opioid pain reliever misuse and abuse. 
The regression tree model was evaluated on the test set; Figure 9 shows 
the prediction (yhat) plotted against data in the test set.

\subsubsection{Decision Trees} 

The decision tree model was prepruned to a maximum depth of 4, which means 
the algorithm split on four consecutive features (see Figure 4). The  
algorithm selected cocaine as the root node which includes the n=127738 
observations in the training set. With a classification tree, we predict 
that each observation will belong to the class of training observations in 
the region to which it belongs, and want to determine not only the class
prediction belonging to a terminal node region, but also the class 
proportions among the training observations in that region \cite{james13}. 
One way to interpret a decision tree is by following the number of samples 
represented at the split for each node. Another way to interpret the decision 
tree in Figure 4 is by the examining the proportion of observations of 
class A captured by that leaf over the entire number of observations captured 
by the leaf during model training. Starting from the root node, the branch 
to the left represents n=112730 observations with no or low cocaine use; 
of those, 0.07 or 7\% reported misusing or abusing opioid pain relievers. 
Following the right branch are n=15008 observations positive for cocaine use, 
of which 0.39 or 39\% reported pain reliever misuse or abuse. This means that 
the proportion of pain reliever misuse and abuse was more than five times as 
large for individuals who reported using cocaine than those who had not. 


The second split was based on heroin use; the left branch included n=12876 
respondents who had not used heroin, of which 0.34 or 34\% reported pain
reliever misuse or abuse. Following the right branch to the terminal node 
(i.e., `leaf'), n=2132 individuals reported heroin use, of which 0.70 or 
70\% had misused or abused pain relievers. Thus, the proportion of PRLMISAB 
was twice as large for respondents who had used heroin than those who had 
not used heroin (as seen in Figure 3). The third split in the decision tree
was based on tranquilizers. The left branch represented n=9757 individuals 
who reported no or low tranquilizer use; of these, the proportion of 
PRLMISAB was 0.28 or 28\%. The branch to the right represented n=3119 
observations with moderate to high tranquilizer use, of which 0.52 or 52\% 
reported PRLMISAB. The rate of pain reliever misuse and abuse for individuals 
with moderate to high tranquilizer use was almost twice as large as for 
those reporting no to low tranquilizer use. The fourth split was based on age 
category; the branch to the left represented n=1394 individuals age 36 or older, 
of which 0.38 or 38\% reported PRLMISAB. The branch to the right represented 
n=1725 individuals age 35 and younger, of whom 0.63 or 63\% reported PRLMISAB. 
This findings shows that pain reliever misuse and abuse was much more likely
among respondents age 35 or younger than among individuals older than 35 years. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure4.pdf}
  \caption{Decision Tree Model of Student Performance on Training Set}
  \label{f:Figure4}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Random Forests Regression}





Random forests build many different decision trees, that are decorrelated, and 
then averages the trees to reduce variance. The advantage of randome forests is 
that it provides a more accurate model, but the tradeoff is that the large 
ensemble of trees can be more difficult to interpret. The random forest package 
was loaded into R, and the random forests regression model was fit on PRLMISAB, 
with 500 trees, and three variables considered at each split (e.g., mtry=3).
The mean of the squared residuals was 2.99, which shows better performance
in terms of error than a single decision tree. The model accounted for 26
percent of the variance in opioid pain reliever misuse and abuse. Figure 10
shows the feature importance for the random forest regression model by the
percent increase in MSE on the left panel, and as a function of the increase
in node purity on the right panel. 




Table 8 provides the feature importance for 
the random forests model sorted by the mean decrease in the Gini coefficient. 
The algorithm selected cocaine as the most informative feature for predicting 
pain reliever misuse and abuse. In contrast to the single decision tree, 
amphetamines, mental health, and health were selected among the top four most 
important features in the random forests model, followed by tranquilizers, 
age category, and heroin use, which were ranked as more influential in the 
single tree (Figure 4). 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
  \caption{Feature importance for Random Forests Model on Test Set}
  \label{f:Figure5}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Gradient Boosting Regression}

The relative influence plot in Table 3 shows that the most 
influential features in the gradient boosting model were: TRQLZRS, TRTMENT,
HEROINUSE, COCAINE use, and AMPHETMN. Figure 11 shows the influential 
features of the gradient boosting regression as a horizontal barplot. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Feature Importance for Student Performance from the Random Forests 
  (Node Purity) and Gradient Boosted Model}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
            & Random  Forests & & Gradient Boosted \\    
    \midrule   
    
    Predictor Variables & \% Inc. MSE & & Rel. Imp. \\    
    \midrule
    Courses             &  1.429 &   &  7.156 \\ 
    Mother's Education  &  1.109 &   &  5.246 \\
    Absences            &  0.812 &   & 14.611 \\
    Age                 &  0.649 &   &  7.103 \\
    Study Time          &  0.514 &   &  4.573 \\     
    Go Out Friends      &  0.508 &   &  5.816 \\
    Weekly Alcohol      &  0.396 &   &  4.360 \\
    Higher Education    &  0.370 &   &  1.150 \\   
    Paid Extra Courses  &  0.361 &   &  1.609 \\
    Daily Alcohol       &  0.284 &   &  2.582 \\
    Sex                 &  0.264 &   &  2.209 \\
    Mother's Job        &  0.252 &   &  2.289 \\   
    Health              &  0.230 &   &  5.007 \\ 
    Father's Education  &  0.220 &   &  3.494 \\   
    Extra Activities    &  0.201 &   &  2.985 \\
    Romantic Relation   &  0.182 &   &  2.747 \\
    Father's Job        &  0.180 &   &  4.016 \\   
    Free Time           &  0.171 &   &  5.164 \\
    School Support      &  0.158 &   &  1.201 \\    
    Guardian            &  0.143 &   &  2.135 \\
    Area                &  0.131 &   &  1.375 \\
    Internet Access     &  0.112 &   &  1.716 \\    
    Family Support      &  0.082 &   &  1.837 \\
    Family Size         &  0.077 &   &  1.898 \\
    Family Relations    &  0.041 &   &  3.011 \\
    Parents             &  0.012 &   &  0.725 \\
    Travel Time         & -0.006 &   &  2.987 \\  
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISCUSSION}

previous course failures, course topic, school support, romantic relationship, 
going out with friends, and overall health were negatively correlated with 
student performance, whereas plans for higher education, study time, mother's
education level, internet access, quality of family relations, and father's 
job were positively related to student achievement.


The results showed that prescription opioid use, misuse, and abuse in this 
sample was higher than use of illicit opioids such as heroin and fentanyl. 
The use of Hydrocodone (Vicodan) was double that of Oxycodone (Oxycodone) 
across almost all age groups. Illicit drug use was highest between the ages 
of 18 to 25. Almost twice as many young adults reported a need for substance 
use treatment, but had not received treatment, compared to the youngest age 
group. Of individuals who reported misusing prescription opioid medications, 
twice as many said they had used heroin than had not (see Figure 1), which is 
consistent with the hypothesis that prescription opioid misuse is associated 
with heroin use. The different learning models provided different estimates 
of the features important for predicting pain reliever misuse and abuse. 


The multiple regression showed multiple features together significantly 
predicted pain reliever abuse, but there may be non-linear trends in the data.
Compared to ridge regression, the lasso performed variable selection, 
indicating that a model with five features (Treatment, Heroin, Cocaine, 
Amphetamine, Any Pain Relievers) explained a significant portion of
variabilty in pain reliever abuse, and adding more factors did not improve
the performance greatly. 

The regression tree model created a solution
with four features (Treatment, Cocaine, Heroin, Tranquilizers), and 

the
random forest regression selected Tranquilizers as most informative of
pain reliever misuse, which is consistent with the results of the gradient
boosting model tree ensemble (shown in Table 3). 

Overall, the models 
agreed in selecting the top five most influential predictors of pain
reliever abuse, but there was some variation in the ordering of importance. 
The linear models showed that substance treatment had the highest
coefficients (followed by heroin use), whereas the tree ensemble methods
indicated that tranquilizer use was the most important feature. Given the 
relatively low rates of prescription opioid and heroin use in the sample, 
additional evidence is needed to clarify questions regarding the importance
of various features for predicting opioid misuse and abuse. 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Limitations}

A surprising finding is that 

A limitation of the study is that only a small proportion of the sample 
reported having used or misused prescription opioids. It is possible that the 
level of opioid use in the sample was not representative of opioid use in the 
general population. 

According to the Centers for Disease Control, the rate
of heroin use in the general population of adults is 2.6 percent, whereas
the rate of heroin use in the NSDUH-2015 sample was 1.6 percent \cite{cdc16}. 
Obtaining reliable information about medication consumption can be difficult 
based on self-reports. Survey data can be biased by under-reporting or by 
minimizing reports of illicit substance use. People may also be reluctant to 
disclose mental health issues or health problems (e.g., STDs, HIV, suicide 
attempts). Another consideration is that, owing to the nature of the methods 
used in the study, it was not possible to determine the direction of the 
relationship between prescription opioid use and heroin use. Individuals who 
misuse or abuse prescription opioid medications may turn to synthetic opioids 
or heroin as cheaper, more readily available than prescription medications. 


Alternatively, individuals who used heroin may be inclined to abuse 
prescription opioids based on availability. Past research has shown that drug 
dosage and medication type play a significant role in opioid misuse: Treatment 
with high daily dose opioids (e.g., more than 120 mg/ day) and short-acting 
schedule II opioids increases the risk of misuse, abuse, and drug overdose 
\cite{sullivan10}. The results of the present analysis showed that demographic 
characteristics played a relatively minor role compared to the use of illicit 
drugs or tranquilizers. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Collaboration}

Learning is a complex phenomenon that is not always directly observable and 
often inferred from student behavior in online learning platforms. Theories of 
learning reveal the importance of collaboration; from the social constructivist 
perspective, knowledge is constructed through interaction with more knowledgeable
partners, parents, teachers, or peers (Vygotsky, 1978). Sociological research
has investigated the characteristic structure of social networks, showing the 
strength of weak social connections (Granovetter, 1973). Social network analysis 
(SNA) provides a valuable tool for exploring interactions among learners in 
various contexts. Visualization tools help to reveal connections in large 
datasets and analyze collaboration in online learning platforms (Dawson, 2009). 
Content analysis and automated recommender systems have also been used to guide 
learners toward more personalized learning environments (PLE) (Siemens, 2012, 2013). 
Individual differences in metacognitive (e.g., self-reflection, self-awareness), 
disposition, experience and motivation are influential for developing learning 
relationships (Dawson, et al., 2014; GaÅ¡eviÄ‡, et al., 2015). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

A general conclusion is that... 
The findings...

A general conclusion is that people who reported misusing prescription opioids 
were also likely to have received substance treatment. More than any other 
demographic features, a history of prescription medication use, or illicit drug 
use, both seemed to contribute highly to the abuse of pain reliever medications, 
particularly for those who reported using tranquilizers, heroin, cocaine, or 
amphetamines. The findings suggest that the opioid crisis may be driven by the 
widespread availability of prescription medications. Even for people with no 
previous history of mental health issues, exposure to highly addictive opioid 
medications puts people at risk for drug dependency and addiction. As mentioned 
in the introduction, even for those who have been in drug treatment programs, 
a lack of continuity in treatment can leave many people in recovery at risk 
for relapse or possible overdose as they are released back into environments 
associated with their drug use. The sharp increase in overdose deaths in the 
U.S. due to synthetic opioids (other than methadone) has coincided with the 
increased availability of illicitly manufactured fentanyl \cite{nida17}. 
Because the dosage levels and potency of illicit opioids are largely unknown, 
there is greater risk of drug overdose death. Recent findings suggest the 
opioid overdose epidemic is getting worse, and requires urgent action to prevent 
opioid abuse, addiction, and death. The findings reported here seek to raise 
awareness about the risk factors for prescription opioid addiction for patients 
and health care providers in order to help reduce opioid overdose deaths. 

Additional research 
is needed to understand the relationships among the variables identified by 
predictive models. The findings may inform decision making and policy efforts 
to address the opioid crisis and reduce the risk of overdose death. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt} %%ACM-Reference-Format%%
\bibliography{report} 


\end{document} 
