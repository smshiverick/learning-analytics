\documentclass[sigconf]{acmart}

\input{format/final}

\begin{document}
  \title{Supervised Learning Models of Student Performance for Learning Analytics}
  \author{Sean M. Shiverick}
  \affiliation{\institution{smshiverick@gmail.com}
  }
\renewcommand{\shortauthors}{S.M. Shiverick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Modeling student performance can help educators to identify factors that 
contribute to student achievement and students who are struggling. This 
paper modeled student performance as a continuous variable using linear 
regression, regression trees, regression random forests, and gradient 
boosted models. 

Student performance was assessed by final course grade (range: 1-20).

The student performance dataset obtained consisted of N=1044 observations from
two secondary schools in Portugal \cite{cortez08}. 

The target variable, student performance, was 
evaluated as a binary outcome (pass, fail). 

Model performance was evaluated on the testing set. 

All three models identified as the most informative feature for predicting 
student performance. 

The models differed in the importance they assigned to

Advantages and limitations of the different models are  discussed. 

\footnote{Address correspondence to \textit{smshiverick@gmail.com}.}

\end{abstract}
\keywords{Predictive Modeling, Supervised Learning, Classification Models}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

an immense volume of data in higher education The exponential increase in 
computing speed and processing power in recent 
decades has led to the generation 
\cite{Daniel17}. 

The development and use of analytic approaches for 
statistical 'analysis and predictive modeling allows researchers to discover 
patterns in data and provide insights about learning for effective decision 
making. Learning Analytics (LA) is an emerging, multidisciplinary field at 
the intersection of learning science, social science, statistics, and computer 
science that leverages big data to understand learning and the environments 
in which it occurs \cite{siemens13}. The development of LA as a field of 
inquiry have been primarily driven by big data in education and the shift 
toward online learning. The primary stakeholders for LA are learners, educators, 
researchers and administrators. The ability to mine large scale data from online 
learning platforms has applications for student success, course design, and 
institutional programs \cite{Lester19}. This paper reviews LA methodologies, 
applications, and considers challenges and opportunities for implementing 
LA in education.

Predictive modeling provides useful methods for analyzing the student 
performance and identifying features that contribute to successful learning. 
Data mining can predict individuals who may be susceptible to failing or 
dropping out and provide insights for improving student retention. This study 
compares the performance of several classification models to determine the 
best approaches for modeling student performance and success. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 1. Learning Analytics (LA) and Educational Data Mining (EDM)}

LA is a form of education data mining (EDM) that uses research methods and 
predictive analysis to implement platforms for improving student performance and 
instructional design \cite{Baker09, Lester19}. LA and EDM share similar goals 
at the intersection of learning science and data-driven analytics, but these 
approaches differ in origins, emphasis, techniques, and knowledge discovery 
\cite{siemens12}. For example, EDM takes a reductionist approach to analyzing 
individual components in the learning process that focuses on automated discovery
and adaptive learning (e.g., intelligent tutoring). By contrast, LA takes a 
holistic viewpoint to understand the complexity of learning, by leveraging human 
judgment to inform and empower instructors and learners \cite{Papamitsiou14}. 
The development of LA as a separate field of inquiry from EDM has shown a gradual 
shift from a technological focus toward an educational focus or learner-focused 
analytics (Ferguson, 2012). Despite the differences in origin and goals of LA 
and EDM, they can be seen as complementary approaches using similar methodologies. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Education institutions have examined factors such as persistence and social 
integration as influential for learning and used analytic methods to track 
student drop-out rates \cite{ferguson12}. The development of the web opened 
up new avenues for collecting information from online sources along with methods 
for processing and sharing content. As extensive educational datasets became 
available for analysis, EDM researchers use a variety of data mining techniques 
(e.g., classification, clustering, association rule mining) to discover 
potentially useful patterns in data for understanding students and the settings 
in which they learn \cite{bakerYucef09}. To a large extent, EDM emerged from 
a computational approach to analyzing logs of student-computer interactions with
data mined from LMS, ITS (intelligent tutoring system), and technology enhanced 
learning (TEL) systems to evaluate learning processes, enhance web-based learning 
and help learners \cite{penaAyala14, romero10}. Thus, EDM research has focused 
more on the technical challenge of extracting value about learning from big data 
in education. Practitioners distinguish LA as a separate field of research by a 
more holistic, education-focused approach to learning, which seeks to ground 
analytics in learning theory, to benefit learners and teachers, and optimize 
learning in online environments \cite{lang17, papamitsiou14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Together, LA and EDM represent an ecosystem of methodologies and techniques 
for gathering, processing, and acting on data to promote learning. These 
procedures facilitate the preparation, measurement, and collection of data 
about learning activities for subsequent analysis, interpretation, and r
eporting. Much LA/EDM research data is collected within a virtual learning 
environment (VLE), learning management system (LMS), or massive open online 
course (MOOC), which can automatically track student participation by login 
frequency, number of resources accessed, time to solve tasks, response times 
to answer questions, number of discussion posts or chat messages between 
participants, questions submitted to instructors. In addition to quantitative 
measures, meaningful analytics systems will include qualitative measures about 
the content or type of contribution (on-topic, evaluative, question). 
Researchers have also studied how affective states (e.g., boredom, 
frustration, confusion, happiness) influence engagement and learning outcomes 
\cite{pardos14}. Merging LMS data with institutional data about student 
performance (i.e., past grades) can provide valuable insights about the 
conditions for successful learning and decision making (Hora, 2019). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LA/EDM consists of a diverse set of methodologies and approaches for 
addressing challenges in higher education such as underprepared students, 
decreased retention, and foster student success. The most common approaches in 
LA research are modeling students and student behavior, predicting performance, 
dropout, and retention, increasing (self-)reflection and (self-)awareness, 
analyzing connections among learners, improving feedback and assessment, and 
recommending educational resources \cite{lang17, lester19, papamitsiou14}. 
Descriptive, correlational, and predictive methodologies are used to study 
learning processes and context, including predictive modeling, relationship 
mining, similarity grouping, content analysis, and social network analysis. 
Classification is the most commonly used data analytic method, followed by 
clustering, regression (logistic, multiple), and predictive modeling (discovery
with models). Several performance metrics are used to evaluate different 
methods: precision, accuracy, sensitivity, coherence, fitness measures, 
similarity weights \cite{bakerYucef09}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 2. Student / Student Behavior Modeling}.
Modeling student characteristics takes a descriptive approach to reveal factors 
for learning such as domain knowledge, 
motivation, metacognition, attitudes, affect, metacognitive states, learning 
strategies, and behavior \cite{Papamitsiou14}. Student characteristics can be 
combined with data from the LMS system about support, teacher feedback, 
curriculum, and course sequencing. Modeling student characteristics is most 
often based on frequency, but can involve correlation analysis, similarity 
grouping or clustering (k-means), content analysis (i.e., text mining, NLP). 
Quantitative data about activities and behavior (e.g., response time, or 
time reflecting on hints) are helpful for modeling engagement or disengagement 
in learning activities, and provide information that allows software to respond 
to individual differences. Discovery and modeling of behaviors during learner 
interactions in a MOOC.  Clustering can be used to grouping similar individuals 
together based on learner profiles and to aid in the development of more 
personalized learning environment (PLE)\cite{Vellido10}. Analysis of qualitative 
data, may rely on human judgment, can reveal deeper concepts about learning, 
such as inferring reasoning strategies from interaction with a cognitive tutor 
\cite{Fournier11}. Predictive approaches are used also for modeling student 
behavior and assessing functionality in intelligent tutoring systems (ITS)
\cote{penaayala14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predicting Performance} 

The Course Signals program developed at Purdue University 
is one of the most well known platforms for predicting students at-risk of 
falling behind using statistical analysis of LMS data \cite{arnoldPistilli12}. 
In addition to grades, demographic information, academic history, and student 
interaction on the Blackboard LMS were used to track student performance. A 
predictive algorithm was used to calculate likelihood of student success based 
on performance, effort, history, and student characteristics. The course signals 
provided students with real-time feedback about their status via the LMS as 
traffic signal indicators (i.e., red=high risk, yellow=moderate risk, green=low 
risk). Instructors enacted interventions for high risk students providing them 
with actionable information about their performance in emails, texts, referrals
to academic advising, or academic resources center, and face to face meetings.
Courses that implemented the Signals program and provided feedback for students
showed an increase in satisfactory grades, decrease in withdrawals, and 
improved retention. Course signals helped to integrate students into the 
university academically in several ways: first, by facilitating contact 
between faculty and students, second, by providing high risk students with 
resources for student success, and third, by using analytics with real time 
data about student performance. in terms of academic success and retention, 
underprepared students in difficult courses using the signals program fared 
better than more well-prepared students in courses that were not using the 
signals program. Furthermore, LA provided faculty with a useful tool for 
identifying struggling students and encouraging them to take corrective actions.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The most common approach used to predict student performance, dropout, and 
retention is classification, which operationalizes performance as a binary 
outcome. Classification methods include logistic regression, naive Bayes 
models, decision trees, machine learning algorithms, and neural networks 
\cite{Lykourentzou09}. Past empirical findings indicate that the number of 
quizzes successfully passed is the most influential predictor of final grades, 
in addition to engagement, participation in course activities, number of posts, 
frequency of events, and time spent in the course LMS 
\cite{Papamitsiou14, romerozaldivar12}. Affect and motivation are also 
influential for predicting performance; notably, concentration and 
frustration are significantly correlated with final grades, but boredom and 
confusion were negatively related to performance (Pardos et al., 2013). 
Using data-driven machine learning algorithms on student profiles and data 
about activity within the LMS can facilitate early detection of at-risk 
students \cite{Dekkar09}. Furthermore, studentsâ€™ sense of belonging in a 
course is essential for engagement and improved satisfaction which can 
reduces student dropout.measures of student satisfaction can vary according 
to their perceived usefulness or efficiency of training courses. The most 
common reasons for  student disengagement in MOOCs were personal commitments, 
work conflict and course overload (Kizilcec, 2013). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intro 3. Predictive Modeling}


Predictive methodologies can lead to a better 
understanding of student engagement in online education, behavior patterns 
that predict successful performance, and factors that influence student failure 
and non-retention. 

Predictive modeling, statistical learning, or machine learning describe a 
set of procedures and automated processes for extracting knowledge from 
data \cite{james13, kuhn13, muller17, raschka17}. The two main branches 
of predictive modeling are supervised learning and unsupervised learning. 
Supervised learning problems involve prediction about a specific target 
or outcome variable. If a dataset has no target outcome, unsupervised 
learning methods can help to reveal underlying structure in the data 
(e.g. clustering). Supervised learning is used to predict an outcome based 
on input provided to a model, when examples of input/output pairs are 
available in the data \cite{muller17}. A statistical learning model is 
constructed using a set of observations to train the model and then make
predictions with new observations. Two main approaches for supervised learning 
problems are regression and classification. When the target variable
is continuous or there is continuity in the outcome (e.g. home prices), a 
regression model tests how a set of features predicts the target variable. 
If the target is a class label, binary variable, or set of categories 
(e.g., spam or ham emails, benign or malignant cells), a classification model 
will predict which class or category label new instances are assigned to. 
This study used a supervised learning approach to classify prescription pain
reliever misuse and abuse as a binary outcome. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Educational institutions collect large volumes of data about students yet 
much student information is protected to ensure privacy. 

Data on student performance used in the present study was obtained from the
UC-Irvine machine learning repository (UCI-MLR) from two secondary schools 
in Portugal \cite{cortez09}. The dataset includes information collected using
students surveys and school grade records. The predictor variables of interest
were demographic information about family characteristics (e.g., ).

The target variable was student achievement (pass/fail) assessed by the 
final grade report in two courses, mathematics and Portugese language. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Supervised Machine Learning} 

Machine learning is a set of procedures and automated processes for extracting 
knowledge from data, that fall into two main branches: supervised learning and 
unsupervised learning. Supervised learning problems involve prediction about a 
specific target variable or outcome of interest. If a given dataset has no 
target outcome, unsupervised learning methods can be used to discover underlying 
structure in unlabeled data. Supervised learning is used to predict a certain 
outcome from a given input, when examples of input/output pairs are available 
\cite{muller17}. The project uses supervised learning to predict prescription 
opioid misuse and abuse. A machine learning model is constructed from the 
training set of input-output pairs, to predict new test data not previously 
seen by the model. The two major approaches to supervised learning problems are 
regression and classification. When the target variable to be predicted is 
continuous, or there is continuity between the outcome (e.g., home values, 
income), a regression model is used to test the set of features that predict 
the target variable. If the target is a class label, set of categorical or 
binary outcomes (e.g., spam or ham, benign or malignant cells), classification 
is used to predict which class or category label new instances are assigned to.
Comparing the performance of different learning algorithms can be helpful for 
selecting the best model for a given problem \cite{raschka17}. Several different 
learning algorithms are considered below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Linear Model and Regularization} 

Linear models make a prediction using a linear function of the input features
\cite{muller17}. The standard linear model, given in equation 1, describes the
relationship between predicted target variable (Y) from a set of features 
(X[1] ... X[p]), with some measure of error . 

\begin{equation}
  \ Y = \beta[0] + \beta[1]*X[1] + \beta[2]*X[2] +... + \beta[p]*X[p] + \epsilon
\end{equation}

The predicted value of the target outcome can be thought of as the weighted 
sum of the input features with the weights or coefficients (i.e., beta values) 
indicating the influence of a given feature on the outcome. In the number of
observations ('n') is much larger than the number of features ('p'), ordinary 
least squares estimates will have low variance, and perform well on test
observations. If n is not much larger than p high variability in the least
squares fit can result in overfitting and poor prediction on test observations.
For high-dimensional datasets, where p is much greater than n, the least
squares coefficient estimate breaks down. The simple linear model can be
improved by using alternative fitting approaches that produce better 
prediction accuracy and model interpretability \cite{statlearn13}. Three
important methods for improving the linear model fit are (a) subset selection,
(b) dimension reduction, and (c) regularization or shrinkage. This paper 
focuses on regularization which includes all p predictor features in the
linear model, but constrains or regularizes the coefficient estimates, 
effectively by shrinking them towards zero. Regularization reduces variablity 
which improves test set accuracy with a slight increase in bias. In many cases, 
multiple features in a regression analysis are not associated with the target
response; shrinking the coefficient estimates of irrelevant features to zero 
reduces overfitting and provides a more interpretable model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ridge Regression and the L2 Penalty} 

Similar to least squares, ridge regression seeks coefficient estimates that
fit the data well by reducing error, but ridge regression introduces a 
shrinkage penalty (L2) that has the effect of shrinking the coefficient 
estimates towards zero. When the tuning parameter (lambda) is set to zero, 
the shrinkage penalty has no effect and ridge regression produces the least
squares estimates. As the tuning parameter lambda increases, values of the 
ridge regression coefficients approach zero \cite{statlearn13}. Selecting a 
good value of the tuning parameter lambda is important, as ridge
regression will produce a different set of coefficients for each new value of 
lambda. It is important to note that the shrinkage penalty is applied to every
features, but not to the intercept (beta[0]). The advantage of ridge 
regressions over least squares is based on the bias-variance tradeoff. As 
the tuning parameter lambda increases, flexibility of the ridge regression 
decreases, leading to decreased variance but increased bias. Ridge regression 
is often applied after standardizing the predictor variables so that they are 
all on the same scale (e.g., M=0, SD=1). Ridge regression performs 
well with high-dimensional datasets (p>>n) by trading off a small increase
in bias for a large decrease in variance. A disadvantage of ridge regression 
is that, because it includes all predictors in the model, the penalty shrinks
the coefficients toward zero, but does not set any of them exactly to zero, 
which can create a problem for model interpretation with a dataset that
has a very large number of predictor variables. The Lasso is a relatively
recent approach to overcome this limitation. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Lasso and the L1 Penalty} 

The lasso and ridge regression have similar formulations, but the lasso has a 
major advantage over ridge regression as it produces simpler, more interpretable
models based on a subset of features. The lasso uses an L1 penalty which has the 
effect of forcing some of the coefficient estimates to be equal exactly to zero 
when the tuning parameter lambda is sufficiently large \cite{statlearn13}. Thus, 
the lasso performs variable selection, and produces sparse models based on a 
subset of the features, which are generally easier to interpret than ridge 
regression. In this sense, the lasso is similar to best subset selection, as it
tries to find the set of coefficient estimates that lead to the smallest RSS. 
Selecting a good value of the tuning parameter lambda is important, and cross-
validation is used to choose an optimal value. In terms of the bias-variance
tradeoff, the lasso is qualitatively similar to ridge regression. As lambda
increases, the variances decreases and bias increases somewhat; however, the
variance of ridge regression is slightly lower than the variance of the loasso.
The lasso implicitly assumes that a number of the feature coefficients or
weight truly equal to zero. In general, the lasso can be expected to perform
better than ridge regression in situations where a small number of features
account for most of the variability in the target outcome, and the remaining
features have coefficients that are very small or equal to zero. Ridge 
regression performs better when the target is a function of a large number of 
predictors that contribute approximately equal coefficients. Cross-validation
can be used to determine which approach is better for a given problem. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decision Trees and Random Forests}

Decision tree models are widely used for classification and regression. Tree 
models ``learn'' a hierarchy of if-else questions that are represented in the
form of a decision tree. Building decision trees proceeds from a root node as 
the starting point and continues through a series of decisions or choices.
Each node in the tree either represents either a question or a terminal node 
(i.e.,leaf) that contains the outcome. In constructing the tree, the algorithm 
searches through all possible decisions or tests, and find a solution that is 
most informative about the target outcome. Decision tree regression is used 
for continuous target outcomes. The recursive branching process of tree based 
models yields a binary tree of decisions, with each node representing a test 
that considers a single feature. This process of recursive partitioning is 
repeated until each leaf in the decision tree contains only a single target. 
Prediction for a new data point proceeds by checking which region of the 
partition the point falls in, and predicting the majority in that feature space. 
The main advantage of tree based models is that they require little adjustment 
and are easy to interpret. A drawback is that they can lead to very complex
models that are highly overfit to the training data. A common strategy to 
prevent overfitting is \emph{pre-pruning}, which stops tree construction early 
by limiting the maximum depth of the tree, or the maximum number of leaves. 
One can also set the minimum number of points in a node required for splitting.
Another approach is to build the tree and then remove or collapse nodes with 
little information (i.e., \emph{post-pruning}). Decision trees work well with 
features measured on very different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ensemble of Random Trees: Random Forest}

A random forest is a collection of decision trees that are slightly different 
from the others, which each overfits the data in different ways. The idea 
behind random forests is that overfitting can be reduced by building many 
trees and averaging their results. This approach retains the predictive power 
of trees while reducing overfitting. Randomness is introduced into the tree 
building process in two ways: (a) selecting a bootstrap sample of the data, 
and (b) selecting features in each node branch \cite{muller17,raschka17}. In 
building the random forest, we first decide how many trees to build (e.g., 10 
or 100), and the algorithm makes different ransom choices so that each tree is 
distinct. The bootstrapping method repeatedly draws random samples of size n 
from the dataset (with replacement). The decision trees are build on these 
random samples that are the same size as the original data, with some points 
missing and some data points repeated. The algorithm also selects a random 
subset of p features, repeated separately each node in the tree, so that 
each decision at the node branch is made using a different subset of features.
These two processes help ensure that all of the decision trees in the random
forest are different. The important parameters for the random forests 
algorithm are the number of sampled data points and the maximum number of 
features; the algorithm could look at all of the features in the dataset
or a limited number. A high value for \emph{maximum-features} will produce 
trees in the random forest that are very similar and will fit the data 
easily based on the most distinctive features, whereas a low value will 
produce trees that are very different from each other, and reduces over-
fitting. Random forests is of the most widely used ML algorithms that works 
well without very much parameter tuning or scaling of data. A limitation of 
this approach is that Random forests do not perform well with very high-
dimensional, data that is sparse data, such as text data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Project Goals} 

The general idea of the project is that the prescription opioid dependency 
and addicted are associated with the use of illicit drugs such as heroin or 
synthetic opioids such as fentanyl. It was hypothesized that individuals who 
report using illicit drugs may also be prone to misusing prescription opioids. 
Machine learning has been for clinical diagnosis of smoking and drug use from 
functional MRI neuroimaging \cite{zhang05, pariyadath14}; to current
knowledge, no research has used machine learning to learn features of 
individuals susceptible to misuse or abuse prescription opioids. This study 
applies supervised machine learning models to identify the set of demographic 
characteristics and mental health attributes that predict prescription opioid 
misuse and abuse. The data for the study was obtained from a large national 
survey on drug use and health (NSHUH-2015) \cite{samhsa16}. Survey research 
provides data on a wide range of issues that people may be reluctant to 
disclose, including mental health disorders, personal medical issues, 
prescription medications, and illicit drug use. Responses to surveys may be 
biased to some degree, and it can be difficult to obtain reliable information 
about illicit or illegal behaviors, but the anonymity of survey response are 
designed to preserve confidentiality and can help to assure more accurate 
disclosures. The data were fitted using several models, including general 
linear models, decision trees, and random forests. This method can help to 
address prescription opioid dependency and addiction in the following ways: 
(i) Identify demographic factors related to prescription opioid abuse, (ii) 
Identify associations between prescription opioid misuse and abuse and 
illicit drug use, (iii) Identify the model that gives the best accuracy
for predicting new observations, and provides an interpretable solution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method}

\subsection{The Data}

The student performance dataset was downloaded from the UCI-MLR and saved as 
a data frame object in a python interactive notebook. The data was collected 
from two secondary schools in the Alentejo region of Portugal during the 
2005-2006 school year and contained information from a questionnaire and 
school reports of student grades \cite{cortez09}. The dataset includes thirty 
predictor variables consisting of demographic information (e.g. family size, 
parental education and employment status), social/ emotional attributes 
(e.g. extracurricular activities, romantic relationship, alcohol consumption), 
school related variables (e.g., weekly time studying, past class failures, 
absences) related to student  (see Table 1). Student performance was evaluated 
on a 20 point scale-as in other European countries (e.g., France)-at three 
points during the school year (i.e., G1, G2, G3) for two courses, mathematics 
(n=395) and Portugese (n=649). The target outcome was the final grade. A binary 
dummy variable was created based on the final exam grades, where scores greater
than 10 indicated a passing grades and scores at or below 10 indicated a 
failing grade. 

The sample consisted of 1044 students (56.6\% female, Mage=16.71, SD=1.19). 
Descriptive statistics according to the binary measure of student achievement 
(i.e., pass/fail) are provided in Table 2 . The majority of students (73.7\%) 
were from urban areas, from families larger than three members (70.7\%), and 
with parents living together in the sam household (88%).

Ten individuals between 20 and 22 were added to the category of 19+ years. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training Test Sets}

The dataset was divided using a 70 to 30 percent split to create the training
set ($n_1$=731) and testing set ($n_2$=313). The same general procedure 
was used for constructing and evaluating each classification model: (i) The 
model was fit to the training set; (ii) New values were predicted on the 
holdout scores in the testing set; and (iii) Model performance on the test 
set was evaluated in a confusion matrix. The performance metrics of accuracy, 
sensitivity or recall, precision, and $f_1$-score were obtained or derived 
from the confusion matrix. The logistic regression classifier, LDA, QDA, 
decision trees classifier, random forests classifier, gradient boosted trees 
were constructed using the caret package. The KNN classifier, support 
vector classifier (SVC), naive Bayes classifier, and neural network 
(multilayer perceptron) were built using scikit-learn. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training Accuracy and Test Set Accuracy}

In constructing and evaluating models it is important to select a model 
that performs well not only with the data used to train the model, but 
also with new observations. A standard practice is to divide the sample 
data into a \emph{training set} and a \emph{testing set} of observations 
that is set aside and used to evaluate model performance. By convention, 
approximately 70 to 80 percent of observations are used in the training set 
and the remaining portion is held in the test set. Two main problems in 
evaluating model performance are \emph{overfitting} and \emph{underfitting}. 
In the case of overfitting, a model can have high accuracy on the training 
set but perform poorly with new data in the test set because the model is 
overly fit to the training data. By contrast, in the case of underfitting, 
a simple model may not generalize well to new observations as it does not 
include all of the features relevant for predicting the target outcome. 
Increased accuracy on the training set is associated with lower testing set 
accuracy; conversely, increased accuracy on the testing set is related to a 
decrease in training set accuracy. The ideal model is one that optimizes test 
set accuracy while striking a balance between the problems of overfitting 
and underfitting. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Variables Included in the Sample Data for Model Construction.}
  \label{tab:freq}
  \begin{tabular}{ll}
    \toprule
    \textit{Predictor Variables} &    \\
    \midrule
    1. Sex (0=Male,1=Female) & SEX  \\  
    2. Age (15, 16, 17, 18, 19+ years) & AGE  \\
    3. Home address type (0=Rural, 1=Urban) & AREA  \\
    4. Family size (0=Three or less, 1=More than three) & FAMSIZE  \\
    5. Parents' cohabitation status (0=Separate, 1=Together) & PARENTS  \\ 
    6. Mother's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & MEDU  \\
    7. Father's education (0=None, 1=Primary, 2=Grades 5-9,  3=Secondary, 4=Higher education) & FEDU  \\
    8. Mother's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & MJOB  \\
    9. Father's job (0=At home, 1=Other, 2=Civil Services, 3=Health Care, 4=Teacher) & Fjob  \\
    10. Student's guardian (0=Other, 1=Father, 2=Mother) & Guardian  \\
    11. Time from home to school (1=<15 min, 2=15-30 min, 3=30-60 min, 4=>60 min) & TRAVEL  \\
    12. Weekly study time (1=<2 hours, 2=2-5 hours, 3=5-10 hours, 4=>10 hours) & STUDY  \\
    13. Number of past class failures (numeric: n if 1<=n<3, else 4) & FAILURES  \\
    14. Extra educational support (0=No, 1=Tes) & SCHOOLSUP  \\
    15. Family educational support (0=No, 1=Tes) & FAMSUP  \\
    16, Paid extra subject classes (0=No, 1=Tes) & PAID  \\
    17. Extra-curricular activities (0=No, 1=Tes) & ACTIVITIES  \\
    18. Wants to take higher education (0=No, 1=Yes) & HIGHER  \\
    19. Internet access at home (0=No, 1=Tes) & INTERNET  \\
    20. In a romantic relationship (0=No, 1=Tes) & ROMANTIC  \\
    21. Quality of family relationships (1=Very Bad, 5=Excellent) & FAMREL  \\
    22. Free time after school (1=Very Low, 5=Very High) & FREETIME  \\
    23. Going out with friends (1=Very Low, 5=Very High) & GOOUT  \\
    24. Workday alcohol consumption (1=Very Low, 5=Very High) & DALC  \\
    25. Weekend alcohol consumption (1=Very Low, 5=Very High) & WALC  \\
    26. Current health status (1=Very Bad, 5=Very Good) & HEALTH  \\ 
    27. Number of school absences (Count range: 0 to 93) & ABSENCES  \\
    28. Course in mathematics or Portugese (0=Portugese, 1=Math) & COURSES  \\
    \midrule
    \textit{Target Variable} &  \\
    \midrule
    29. Final course grade (0=Lowest, 20=Highest) & GRADE \\
    \bottomrule
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\section{Results}

\subsection{Exploratory Data Analysis}

Table X provides a summary of descriptives statistics for the main
predictor variables according to the target outcome 

frequency and percent of respondents who passed or
failed the final evaluation by demographic characteristics. Sixty-three percent 
of the student sample passed their courses (61\% male, 65\% female) and 
approximately 37\% failed their courses (39\% male, 35\% female). The 
proportion of pass/ fail between males and females was not significant. 


Pain reliever misuse was most frequent among individuals who described 
themselves as single, divorced, or with some college education. The 
proportion of pain reliever misuse and abuse was highest among individuals 
between 18 to 25 years of age and decreased among older age groups. Two 
percent of respondents (n=3433) disclosed ever using heroin (2019 males; 
1414 females). Among respondents who reported using pain relievers in the 
past year, the rate of misuse and abuse of pain relievers was twice as 
large for individuals who reported using heroin than those who had not 
used heroin (shown in Figure 3), which is consistent with past findings 
that indicate a connection between MUPO and heroin use 
\cite{muhuri13, unick13}.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Summary Table of Student Performance for Final Course Grade 
  (Pass$>=$10, Fail$<$10) by Predictor Variables }
  \label{tab:freq}
  \begin{tabular}{llllll}
    \toprule
              & Pass & & & Fail & \\
    Attribute & N & \% &  & N & \% \\
    \midrule
    \textbf{Total} & 661 & 63.3\% & & 383 & 36.7\% \\
    \midrule
    Male      & 277 & 61.1\% & & 176 & 38.9\%  \\
    Female    & 384 & 65.0\% & & 207 & 35.0\%  \\
    \midrule
    \textbf{Past Course Failures} &  &  &  &  & \\
    None      & 622 & 72.2\% & &  239 & 27.8\% \\
    One       & 33  & 27.5\% & &   87 & 72.5\% \\
    Two       & 5   & 15.2\% & &   28 & 84.8\% \\
    Three     & 1   &  3.0\% & &   29 & 96.7.0\% \\
    \midrule
    \textbf{Higher Education Plans} &  &  &  &  & \\
    Planned   & 640 & 67.0\% & & 315 & 33.0\%  \\
    No Plans  &  21 & 23.6\% & &  68 & 76.4\%  \\
    \midrule
    \textbf{Study Time} &  &  &  &  &     \\
    More than 10 hrs. &  45 & 72.6\% & &  17 & 27.4\% \\
    5 to 10 hrs.      & 123 & 75.9\% & &  39 & 24.1\% \\
    2 to 5 hrs.       & 321 & 63.8\% & & 182 & 36.2\% \\    
    Less than 2 hrs.  & 172 & 54.3\% & & 145 & 45.7\% \\
    \midrule
    \textbf{Course} &  &  &  &  & \\
    Portugese  & 452 & 69.6\% & & 197 & 30.4\%  \\
    Math       & 209 & 52.9\% & & 186 & 47.1\%  \\  
    \midrule
    \textbf{School Support} &  &  &  &  & \\
    Rural    & 155 & 54.4\% & & 130 & 45.6\%  \\
    Urban    & 506 & 66.7\% & & 253 & 33.3\%  \\ 
    \midrule
    \textbf{Mother's Education} &  &  &  &  & \\
    Higher Ed     & 235 & 76.8\% & &  71 & 23.2\% \\
    Secondary     & 143 & 49.5\% & &  95 & 32.9\% \\
    Grades 5 to 9 & 180 & 75.6\% & & 109 & 45.8\% \\
    Primary       &  98 & 47.5\% & & 106 & 52.5\% \\
    None          &   7 & 77.8\% & &   2 & 22.2\% \\
    \midrule    
    \textbf{Romantic Relationship} &  &  &  &  & \\
    No Relationship & 440 & 65.4\% & & 233 & 34.6\%  \\
    Relationship    & 221 & 59.6\% & & 150 & 40.4\%  \\    
    \midrule
    \textbf{Internet} &  &  &  &  & \\
    No        & 118 & 54.4\% & &  99 & 45.6\%  \\
    Yes        & 543 & 65.7\% & & 284 & 34.3\%  \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht]
  \caption{Coefficient Estimates for Regression Models of Student Performance on 
  Training Set and Testing Set}
  \label{tab:freq}
  \begin{tabular}{lllllll}
    \toprule
     & Training Set & (N = 730)&& Testing Set& (N = 314)&  \\
    \midrule
    Variables & Coefficient & Est.S.E. & t-value & Coefficient & Est.S.E.& t-Value  \\
    \midrule
    Intercept           &       10.315***    & 0.873 & 11.811 &          7.31***    & 1.473 &  4.96 \\
    Failures            & \textbf{-1.546}*** & 0.206 & -7.49  & \textbf{-1.574}***  & 0.307 & -5.14 \\
    Higher Ed           & \textbf{ 1.070}**  & 0.476 &   2.25 & \textbf{ 3.210}***  & 0.752 &  4.27 \\
    Study Time          & \textbf{ 0.300}**  & 0.151 &   1.98 & \textbf{ 0.780}***  & 0.250 &  3.12 \\
    Course              & \textbf{-1.918}*** & 0.256 & -7.49  & \textbf{-1.041}**   & 0.427 & -2.44 \\
    School Support      & \textbf{-1.204}*** & 0.392 & -3.07  & \textbf{-1.347}**   & 0.617 & -2.19 \\
    Mother's Education  & \textbf{ 0.382}*** & 0.119 &   3.22 & \textbf{ 0.381}*    & 0.203 &  1.88 \\
    Romantic Relationship& \textbf{-0.633}** & 0.255 &  -2.49 & \textbf{-0.807}*    & 0.441 & -1.83 \\
    Internet Access     & \textbf{ 0.800}**  & 0.311 &   2.57 &          0.224      & 0.521 &  0.43 \\
    Family Relations    & \textbf{ 0.249*}   & 0.136 &  1 .83 &         -0.047      & 0.206 & -0.23 \\
    Going Out           & \textbf{-0.412}*** & 0.109 &  -3.77 &         -0.031      & 0.170 & -0.18 \\
    Father's Job        & \textbf{ 0.269}*   & 0.139 &   1.92 &         -0.008      & 0.234 & -0.03 \\
    Health              & \textbf{-0.219}**  & 0.086 &  -2.56 &          0.002      & 0.145 &  0.02 \\
    \bottomrule
    Note. significance levels & *\<0.10           & **\<0.05  & ***\<0.01 & & &
  \end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As past failures increased by one unit, the odds of successfully passing the 
course decreased by 4.79 times (379\% increase), holding the effects of other 
variables constant. In terms of course, the odds of successfully passing the 
course in mathematics decreased 3.07 times compared to course in Portugese. 
in other words, the odds of students passing the math course was 207\% lower 
than students in the Portugese course. For students with plans to pursue 
higher education, the odds of successfully passing increased by 3.26 times 
(226\%) compared to students with no plans for higher education. A one unit 
increase in father's education level was associated with a 1.29 or 29\% 
increase in a passing final grade. In addition, the odds of passing increased
by 1.56 for internet access; the odds of passing for students with internet 
access at home was 56\% higher than for students without internet access.  
Several factors had a negative impact on student success. For example, the 
odds of passing decreased 2.21 for students who received school support
compared to students who did not receive school support. A one-unit increase
in weekly alcohol consumption decreased the odds of successfully passing by 1.20 
(20\%). The odds of successfully passing increased by 1.39 (39\%) given a unit 
change in custodian guardian ('other', 'father', 'mother'). Finally, a one-unit 
increase in going out with friends decreased the odds of passing by 1.18 (18\%). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Decision Trees} 

The decision tree model was prepruned to a maximum depth of 4, which means 
the algorithm split on four consecutive features (see Figure 4). The  
algorithm selected cocaine as the root node which includes the n=127738 
observations in the training set. With a classification tree, we predict 
that each observation will belong to the class of training observations in 
the region to which it belongs, and want to determine not only the class
prediction belonging to a terminal node region, but also the class 
proportions among the training observations in that region \cite{james13}. 
One way to interpret a decision tree is by following the number of samples 
represented at the split for each node. Another way to interpret the decision 
tree in Figure 4 is by the examining the proportion of observations of 
class A captured by that leaf over the entire number of observations captured 
by the leaf during model training. Starting from the root node, the branch 
to the left represents n=112730 observations with no or low cocaine use; 
of those, 0.07 or 7\% reported misusing or abusing opioid pain relievers. 
Following the right branch are n=15008 observations positive for cocaine use, 
of which 0.39 or 39\% reported pain reliever misuse or abuse. This means that 
the proportion of pain reliever misuse and abuse was more than five times as 
large for individuals who reported using cocaine than those who had not. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\pagewidth]{images/Figure1.pdf}
  \caption{Decision Tree Model of Pain Reliever Misuse and Abuse
  fit to the Training Set.}
  \label{f:Figure1}
\end{figure}

The second split was based on heroin use; the left branch included n=12876 
respondents who had not used heroin, of which 0.34 or 34\% reported pain
reliever misuse or abuse. Following the right branch to the terminal node 
(i.e., `leaf'), n=2132 individuals reported heroin use, of which 0.70 or 
70\% had misused or abused pain relievers. Thus, the proportion of PRLMISAB 
was twice as large for respondents who had used heroin than those who had 
not used heroin (as seen in Figure 3). The third split in the decision tree
was based on tranquilizers. The left branch represented n=9757 individuals 
who reported no or low tranquilizer use; of these, the proportion of 
PRLMISAB was 0.28 or 28\%. The branch to the right represented n=3119 
observations with moderate to high tranquilizer use, of which 0.52 or 52\% 
reported PRLMISAB. The rate of pain reliever misuse and abuse for individuals 
with moderate to high tranquilizer use was almost twice as large as for 
those reporting no to low tranquilizer use. The fourth split was based on age 
category; the branch to the left represented n=1394 individuals age 36 or older, 
of which 0.38 or 38\% reported PRLMISAB. The branch to the right represented 
n=1725 individuals age 35 and younger, of whom 0.63 or 63\% reported PRLMISAB. 
This findings shows that pain reliever misuse and abuse was much more likely
among respondents age 35 or younger than among individuals older than 35 years. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Feature Importance for the Random Forests Model}

The random forest model was fit using 1000 trees, with all of the features 
considered at each node to determine the randomness of each tree. After a 
large number of trees is generated, each tree represents a vote for the
most popular class. Although random forests perform well with data that has
imbalanced classes, it can be difficult to interpret the result of the 
averaged trees. Feature importance is a model summary for random forests 
that rates how important each feature is for classification decisions made 
in the algorithm. The Gini index provides a measure of node purity which is 
used to evaluate the quality of a particular split; a small value indicates 
that a node predominantly contains observations from a single class. 
Feature importance was measured by the mean decrease in Gini coefficient 
(Table 8), which indicates how each variable contributes to the homogeneity 
of the nodes and leaves in the resulting random forest. For classification 
trees, the splits are chosen so as to minimize entropy or Gini impurity in 
the resulting subsets. For random forests, variables that result in nodes 
with higher purity have a higher decrease in the Gini coefficient. Feature 
importance is computed by aggregating the feature importance over trees in 
the random forest, and gives non-zero importance to more features than a 
single tree. A feature may have a low importance value because another feature 
encodes the same information. Table 8 provides the feature importance for 
the random forests model sorted by the mean decrease in the Gini coefficient. 
The algorithm selected cocaine as the most informative feature for predicting 
pain reliever misuse and abuse. In contrast to the single decision tree, 
amphetamines, mental health, and health were selected among the top four most 
important features in the random forests model, followed by tranquilizers, 
age category, and heroin use, which were ranked as more influential in the 
single tree (Figure 4). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure2.pdf}
%  \caption{Feature importance for Random Forests Model.}
%  \label{f:Figure2}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Feature Importance for Random Forests Model}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
            & Random & Forests & Gradient Boosted \\    
    \midrule   
    
    Predictor Variables & Inc. MSE & Node Purity & Rel. Imp. \\    
    \midrule
    Failures            & 25.89 & 896.51 &  7.69 \\    
    Absences            &  9.08 & 568.34 & 13.66 \\
    Mother's Job        &  7.83 & 449.28 &  4.94 \\   
    Going Out           &  4.02 & 426.00 &  5.68 \\
    Age                 &  9.00 & 407.77 &  4.65 \\
    Mother's Education  & 11.35 & 383.32 &  3.79 \\
    Free Time           &  7.49 & 382.20 &  5.29 \\
    Father's Job        &  7.32 & 367.77 &  5.21 \\       
    Study Time          &  5.99 & 346.16 &  4.23 \\       
    Courses             &  7.25 & 344.43 &  5.55 \\ 
    Health              &  1.56 & 341.38 &  4.85 \\ 
    Weekly Alcohol      &  6.61 & 335.49 &  3.64 \\
    Father's Education  &  6.08 & 334.26 &  3.32 \\    
    Family Relations    &  4.39 & 314.83 &  3.43 \\    
    Daily Alcohol       &  6.32 & 258.02 &  2.30 \\
    Higher Education    & 13.65 & 251.74 &  1.63 \\   
    Travel Time         &  1.60 & 196.75 &  1.80 \\       
    Student Guardian    &  3.79 & 193.83 &  1.85 \\
    Sex                 &  4.87 & 169.87 &  2.05 \\
    Paid extra courses  &  5.34 & 165.76 &  1.69 \\
    Rom. Relationship   &  3.29 & 160.21 &  2.10 \\
    Area                &  2.83 & 153.56 &  1.44 \\
    Family Support      &  3.53 & 151.48 &  1.89 \\
    Extra Activities    &  2.73 & 149.95 &  1.81 \\
    Family Size         &  7.00 & 148.53 &  1.59 \\
    School Support      & 10.88 & 146.88 &  2.22 \\    
    Internet Access     &  4.17 & 128.53 &  1.20 \\
    Parents             &  1.40 &  83.33 &  0.52 \\
    \bottomrule
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISCUSSION}

The results show that there were imbalanced classes in the sample data given 
that the proportion of respondents who reported no MUPO (i.e., negative class)
was much larger than the 11 percent of respondents who reported pain reliever 
misuse and abuse. Logistic regression, random forests, and decision trees
performed better than other classifier models using the $f_1$-score as the 
appropriate performance metric. Traditional algorithms tend to classify new 
observations based on the majority class, as seen with the naive Bayes model, 
which had a relatively high $f_1$-score despite identifying only 35 true 
positive instances and having a high number of false negatives. The random 
forests model performed slightly better than the other models as it correctly 
predicted more true positive instances than the logistic regression and had 
fewer false negative errors than the single decision tree. The QDA and LDA 
models both identified the largest number of true positives, but each correctly 
labeled fewer true negatives compared to the logistic regression model, which 
had higher $f_1$-score and accuracy overall. This finding indicates that the 
decision boundary for classifying pain reliever misuse and abuse can be modeled 
as a linear function of the predictor variables \cite{james13, raschka17}. 
The decision tree and random forests models would be preferred if pain reliever 
misuse and abuse were modeled as a non-linear function of the predictors,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Cocaine use was selected by all three models as the most informative predictor 
of pain reliever misuse and abuse (PRLMISAB). The odds of PRLMISAB increased 
99 percent given a one unit increase in cocaine use, holding other independent
variables constant. In terms of frequency, the proportion of PRLMISAB among 
individuals who reported moderate to high cocaine use was more than five times 
as large as for individuals who reported no or low cocaine use. Logistic 
regression and random forests both identified amphetamine use as the second 
most important feature for predicting PRLMISAB. The odds of PRLMISAB
increased by 84 percent for a unit increase in amphetamine use. In contrast, 
the single decision tree identified heroin use as the second most important 
predictor. Of those respondents who disclosed heroin use, more than two-thirds 
reported misusing or abusing pain relievers. In other words, the proportion of 
PRLMISAB was twice as large for respondents who had used heroin as for those 
who had not, Similarly, logistic regression model revealed that the odds of 
PRLMISAB increased by 112 percent given previous heroin use, holding other 
variables constant. These findings are consistent with past studies that
indicate a connection between pain reliever misuse and abuse and heroin use
\cite{jones13, jones15, muhuri13, unick13}. Tranquilizers were identified as 
the third most important variable by both the decision tree and logistic 
regression models. The odds of PRLMISAB increased 1.46 times for a one unit 
increase in tranquilizer use. Similar to the pattern observed for heroin, the 
proportion of PRLMISAB was nearly twice as large for respondents with moderate 
to high tranquilizer use as for individuals with no to low tranquilizer use. 

Age category, mental health, health issues, education level, and previous 
drug or alcohol treatment were also identified as important features for 
predicting PRLMISAB; however, the models differed in the importance they 
assigned to these variables. A general finding related to age is that pain 
reliever misuse and abuse was more prevalent among respondents between
18 to 25 years of age and decreased among older individuals. Although more 
females reported using pain relievers than males, more males reported 
misusing or abusing pain relievers than females, and the odds of pain reliever 
misuse or abuse was 17 percent lower for females as for males. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Limitations}

A surprising finding is that 

A limitation of the study is that the default 
settings were used in constructing the classifier models. Many complex models 
are sensitive to parameter settings and scaling of the data. Additional 
parameter tuning could have improved performance of complex models. 

Another limitation is that the aggregated features represent a 
subset of the entire range of features in the NSDUH datasets. 

In a future study, it may be useful to include a more comprehensive set of features 
to 
A widely accepted truism in predictive analytics that more data is always better. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Collaboration}

Learning is a complex phenomenon that is not always directly observable and 
often inferred from student behavior in online learning platforms. Theories of 
learning reveal the importance of collaboration; from the social constructivist 
perspective, knowledge is constructed through interaction with more knowledgeable
partners, parents, teachers, or peers (Vygotsky, 1978). Sociological research
has investigated the characteristic structure of social networks, showing the 
strength of weak social connections (Granovetter, 1973). Social network analysis 
(SNA) provides a valuable tool for exploring interactions among learners in 
various contexts. Visualization tools help to reveal connections in large 
datasets and analyze collaboration in online learning platforms (Dawson, 2009). 
Content analysis and automated recommender systems have also been used to guide 
learners toward more personalized learning environments (PLE) (Siemens, 2012, 2013). 
Individual differences in metacognitive (e.g., self-reflection, self-awareness), 
disposition, experience and motivation are influential for developing learning 
relationships (Dawson, et al., 2014; GaÅ¡eviÄ‡, et al., 2015). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

A general conclusion is that... 
The findings...


Predictive modeling offers several useful approaches for analyzing the misuse
and abuse of prescription opioids and identifying factors that contribute to 
MUPO. This study compared ten classification models of pain reliever misuse 
and abuse using the $f_1$-score to evaluate model performance with unbalanced 
classes in the data. Linear regression, random forests, and decision trees 
had the best performance compared to other models. Cocaine use was selected 
as the most informative variable by all three models, followed by amphetamine 
use which was selected as the second most important feature by the logistic
regression and random forests models. The models differed in  the importance 
they assigned to heroin use, tranquilizers, and demographic features such as 
age group, mental health, and health problems. A general conclusion is that 
there are tradeoffs between model complexity, performance, and 
interpretability. A simple decision tree provides an interpretable model 
that is prone to overfitting. 

Random forests reduce overfitting and improve 
generalizability, but are difficult to interpret. Linear regression strikes 
a balance between complexity and interpretability, but does not perform well
with non-linear relationships or high-dimensional data. Additional research 
is needed to understand the relationships among the variables identified by 
predictive models. The findings may inform decision making and policy efforts 
to address the opioid crisis and reduce the risk of overdose death. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt} %%ACM-Reference-Format%%
\bibliography{report} 


\end{document} 
