BIN STORAGE - Binary Classification TEXT

%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure4.pdf}
%  \caption{Lasso Regression Model: Coefficient Value and Variance in Predicted 
%  Student Performance Accounted For as a Function of Predictor Variables }
%  \label{f:Figure4}
%\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Decision Trees} 

The algorithm selected cocaine as the root node which includes the n=127738 
observations in the training set. With a classification tree, we predict 
that each observation will belong to the class of training observations in 
the region to which it belongs, and want to determine not only the class
prediction belonging to a terminal node region, but also the class 
proportions among the training observations in that region \cite{james13}. 
One way to interpret a decision tree is by following the number of samples 
represented at the split for each node. Another way to interpret the decision 
tree in Figure 4 is by the examining the proportion of observations of 
class A captured by that leaf over the entire number of observations captured 
by the leaf during model training. 

Starting from the root node, the branch 
to the left represents n=112730 observations with no or low cocaine use; 
of those, 0.07 or 7\% reported misusing or abusing opioid pain relievers. 
Following the right branch are n=15008 observations positive for cocaine use, 
of which 0.39 or 39\% reported pain reliever misuse or abuse. This means that 
the proportion of pain reliever misuse and abuse was more than five times as 
large for individuals who reported using cocaine than those who had not. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The second split was based on heroin use; the left branch included n=12876 
respondents who had not used heroin, of which 0.34 or 34\% reported pain
reliever misuse or abuse. Following the right branch to the terminal node 
(i.e., `leaf'), n=2132 individuals reported heroin use, of which 0.70 or 
70\% had misused or abused pain relievers. Thus, the proportion of PRLMISAB 
was twice as large for respondents who had used heroin than those who had 
not used heroin (as seen in Figure 3). The third split in the decision tree
was based on tranquilizers. The left branch represented n=9757 individuals 
who reported no or low tranquilizer use; of these, the proportion of 
PRLMISAB was 0.28 or 28\%. The branch to the right represented n=3119 
observations with moderate to high tranquilizer use, of which 0.52 or 52\% 
reported PRLMISAB. The rate of pain reliever misuse and abuse for individuals 
with moderate to high tranquilizer use was almost twice as large as for 
those reporting no to low tranquilizer use. The fourth split was based on age 
category; the branch to the left represented n=1394 individuals age 36 or older, 
of which 0.38 or 38\% reported PRLMISAB. The branch to the right represented 
n=1725 individuals age 35 and younger, of whom 0.63 or 63\% reported PRLMISAB. 
This findings shows that pain reliever misuse and abuse was much more likely
among respondents age 35 or younger than among individuals older than 35 years. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

On the training set, there was a 2.23 decrease in predicted performance 
for students in the math course compared to students in the Portugese course, 
controlling for the effect of other independent variables. Students receiving 
school support had a 1.35 lower predicted final course grade than students 
who did not receive school support, holding all other variables constant. 
Students with plans for higher education had a 1.70 higher predicted final 
grade than students with no plans for higher education (*controlling for 
all other variables. A one-unit change in going out with friends yielded 
a 0.44 decrease in the predicted performance*. A unit change in mother's 
level of education was associated with a 0.49 increase in predicted performance.
A one-unit change in student health resulted in a 0.26 decrease in predicted 
student performance. Importantly, a one-unit change in weekly study time 
resulted in a 0.45 increase in predicted student performance. A one-unit 
change in quality of family relationships resulted in a 0.34 increase in 
student performance. A one-unit increase in students age was associated with
a 0.25 decrease in predicted performance. There was a -0.81 decrease in 
predicted performance for students in a romantic relationship compared to 
students who were not in a relationship, all else being equal. Students with 
access to the internet at home had a 0.82 higher predicted final grade than 
students with not internet access at home. Students whose family size was
greater than 3 had a 0.50 lower predicted performance than students with 
a family size of 3 or less. Finally, a unit change in father's job yielded 
a 0.29 increase in predicted performance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


The left side of Table 3 shows that, in the regression with the training set, 
previous course failures, course topic, school support, romantic relationship, 
going out with friends, and overall health were negatively correlated with 
student performance, whereas plans for higher education, study time, mother's
education level, internet access, quality of family relations, and father's 
job were positively related to student achievement. The right side of Table 3
shows the final regression model with the testing set. A one-unit change in 
past course failures yielded a -1.57 decrease in the predicted value of 
student performance, holding the effect of other predictor variables constant. 
For students in the math course, there was a -1.04 decrease in predicted 
performance compared to students in the Portugese course, controlling for all 
other factors. Students receiving school support had a -1.35 lower predicted 
final course grade than students who did not receive school support, holding
other factors constant. There was a -0.81 decrease in predicted performance 
for students in a romantic relationship compared to students who were not in 
a relationship . In terms of positive relationships, having plans to pursue 
higher education yielded a 3.21 increase in predicted student performance 
than having no plans for higher education, controlling for the effect of other
independent variables. A one-unit change in weekly study time resulted in a 
0.78 increase in predicted student performance, holding other variables 
constant. In addition, a unit change in mother's level of education was 
associated with a 0.38 increase in predicted student performance, controlling 
for the effect of other independent variables. 

On the training 13 of the 27 predictor variables were statistically 
significant; however, only 6 of the 13 predictors in the testing set were 
significant, which suggests the model was overfit in the training data. 


A one unit increase in past failures was associated with a   the odds of 
successfully passing the 
course decreased by 4.79 times (379\% increase), holding the effects of other 
variables constant. In terms of course, the odds of successfully passing the 
course in mathematics decreased 3.07 times compared to course in Portugese. 
in other words, the odds of students passing the math course was 207\% lower 
than students in the Portugese course. For students with plans to pursue 
higher education, the odds of successfully passing increased by 3.26 times 
(226\%) compared to students with no plans for higher education. A one unit 
increase in father's education level was associated with a 1.29 or 29\% 
increase in a passing final grade. In addition, the odds of passing increased
by 1.56 for internet access; the odds of passing for students with internet 
access at home was 56\% higher than for students without internet access.  
Several factors had a negative impact on student success. For example, the 
odds of passing decreased 2.21 for students who received school support
compared to students who did not receive school support. A one-unit increase
in weekly alcohol consumption decreased the odds of successfully passing by 1.20 
(20\%). The odds of successfully passing increased by 1.39 (39\%) given a unit 
change in custodian guardian ('other', 'father', 'mother'). Finally, a one-unit 
increase in going out with friends decreased the odds of passing by 1.18 (18\%). 









\subsection{Intro 4. Classification Models}

There are several classifier algorithms for building predictive models of
education data. The following models were compared in order to determine
which model is best for predicting student performance and identifying
factors that influence student achievement. 

\subsection{Logistic regression} models the conditional distribution of 
probabilities for a binary response as a combination of a set of predictor 
variables. The decision boundary for the logistic regression classifier is a 
linear function of the input that separates two or more classes using a line, 
plane, or hyperplane. The model uses a maximum likelihood method to predict the 
coefficient estimates that correspond as closely as possible to the default 
state (i.e., `pass`) . The probability values for the outcome range between 
0 and 1, and the model predicts a number close to one for individuals with a 
final passing score and a number close to zero for individuals who do not. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Linear discriminant analysis} (LDA) models the distribution of predictors 
separately in each of the response classes and uses Bayes' theorem to `flip' 
these into estimates for, $Pr(Y=k | X=x)$ \cite{james13}. The discriminant 
functions are linear functions of the predictors. For distributions assumed to 
be normal, LDA is similar to logistic regression, but more stable. 

LDA is also preferred for outcomes with more than two response classes. 

The important assumptions for LDA are, first, a common covariance 
matrix for all classes, and second, the class boundaries are linear functions 
of the predictors. 

\subsection{Quadratic Discriminant Analysis} (QDA) is an approach 
that assumes each class has its own covariance matrix and the decision 
boundaries are quadratically curvilinear in the predictor space \cite{kuhn13}. 
LDA is less flexible as a classifier than QDA, but can perform better with 
relatively few training observations or when the majority of predictors in the 
data represent discrete categories. QDA is recommended over LDA with a very 
large training set or when the decision boundary between two classes is 
non-linear. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The performance of linear classifiers suffers when there is a non-linear 
relationship between the predictors and target outcome. With training
observations that can be separated by hyperplane, the maximal marginal 
classifier provides the maximum distance (i.e., margin) from each observation
to the hyperplane \cite{james13}. The test observations are classified based 
on which side of the hyperplane they fall; however, in many cases no separating 
hyperplane exists. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Support vector machines (SVM)} 

The \emph{support vector classifier} 
(SVC) extends the 
maximal margin classifier by using a soft margin that allows a small number 
of observations to be misclassified on the wrong side of the hyperplane 
\cite{kuhn13, cortes95}. The observations that fall directly on the margin or 
on the wrong side of the hyperplane are called `support vectors'. The parameter 
`C' indicates the number of observations that can violate the margin; if $C>0$, 
no more than C observations can be on the wrong side of the hyperplane. 
SVC addresses the problem of non-linear boundaries between classes by 
enlarging the feature space with higher order (e.g., quadratic, cubic, 
polynomial) functions of the predictors. 


(SVM) are an extension of SVC that use a 
kernel trick to reduce computational load. The radial basis function (RBF) 
kernel (i.e., Gaussian kernel) is one of the most commonly used approaches. 
In training the model, only a subset of data points is used to construct the 
decision boundary, namely the support vectors that lie on the border that 
separates the two classes. In predicting classes for new observations, the 
algorithm calculates the distance to each of the support vectors measured 
by the Guassian kernel \cite{muller17}. 

Figure 1 shows an example of the 
non-linear decision boundary obtained with SVM using the RBF kernel; the 
decision boundary is a smooth curve and the support vectors are the large 
points in bold outline. Even with the default settings, the RBF kernel 
provides a decision boundary that is decidedly non-linear. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The \subsubsection{Naive Bayes Classifier} is considered as a non-linear model.
The Bayes theorem (equation 2) is represented by a set of probabilities 
that answer the question, ``Based on a given set of predictors, 
what is the probability than an outcome belongs to a particular class?''

\begin{equation}
  \ P(Y=cl|X) = \frac{P(Y)*P(X|Y=cl)}{P(X)}\
\end{equation}

The prior probability, P(Y), is the expected probability of a given class 
based on what is known (e.g. rate of disease in the population). P(X) 
is the probability of the predictor variables. The conditional probability,
$P(X=cl|Y)$, is the probability of observing the predictor variables for data 
associated with a given class. The naive Bayes model assumes that all of the 
predictor variables are independent, which is not always realistic. The 
conditional probabilities are calculated based on the probability densities 
for each individual predictor \cite{kuhn13}. For categorical predictors, the 
observed frequencies in the training set data can be used to determine the 
probability distributions. The prior probability allows us to tilt the final 
probability toward a particular class. Class probabilities are created and 
the predicted class is the one that is associated with the largest class
probability. Despite the somewhat unrealistic assumption of independence among
predictors, the naive Bayes model is computationally quick, even with large 
training sets, and performs competitively compared to other models. The naive 
Bayes model encounters issues when dealing with frequencies or probabilities 
equal to zero, especially for small sample sizes. In addition, as the number 
of predictors increases relative to the size of the sample, the posterior 
probabilities will become more extreme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Neural Network} models are based on theories of connectivity in the brain \cite{kuhn13}. The multilayer perceptron (MLP) which is a feed-forward network 
in which the outcome is modeled by an intermediary set of unobserved variables called hidden units, which are linear combinations of the original predictors. 
Each hidden unit is a combination of some or all of the predictors which are 
then transformed by a nonlinear function (e.g. sigmoidal). A neural network 
usually has multiple hidden units used 
to model the outcome. 

The MLP classifier computes weights between the inputs 
and hidden units, and weights between the layers of hidden units and the 
output. After computing each hidden unit, the output is modeled by a nonlinear 
combination of the hidden units. 

The nonlinear function allows the neural 
network to fit more complicated functions than a linear model; however, 
neural networks are sensitive to the scaling of the features and can require
extensive data preprocessing. There are several ways to modify the complexity 
of a neural network: by selecting the number of hidden layers, the number of 
units within each layer, and the regularization parameter (L2) which shrinks 
the weights towards zero. The feature weights provide an estimate of feature 
importance. Although neural networks can capture information in large amounts 
of data with very complex models, they tend to overfit data used to train the 
model and can be difficult to interpret. Neural networks may work best with 
homogenous datasets where the predictor variables all have similar meanings 
\cite{muhuri13}. For datasets with many different kinds of features, 
tree-based methods offer a better approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Decision trees} are based on a hierarchy of \emph{`if-else'}
questions starting from a root node and proceeding through a series of binary 
decisions or choices. Each node in the tree represents either 
a question or a terminal node (i.e., leaf) that contains the outcome. Applied to 
a binary classification task, the decision tree algorithm learns the sequence
of if-else questions that arrives at the outcome most quickly. For continuous 
features, questions are expressed in the form: ``Is feature x larger than 
value y?'' In constructing the tree, the algorithm searches through all 
possible tests and finds a solution that is most informative about the target 
outcome \cite{muller17}. The recursive branching process yields a binary tree 
of decisions, with each node representing a test for a single feature. This 
process of partitioning is repeated until each leaf in the decision tree 
contains only a single target. 


Prediction for a new data point proceeds by 
checking which region of the partition the point falls in, and predicting the 
majority in that feature space. The main advantage of tree models is that they 
require little adjustment and are easy to interpret. 

A drawback is that they 
can lead to complex models which are highly overfit to the training data. 
`Prepruning' can help reduce overfitting by limiting the maximum depth of the 
tree, or the maximum number of leaves. Another approach is to set the minimum 
number of points in a node required for splitting. Decision trees work well 
with features measured on different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Random Forests} is an ensemble approach that combines many simple trees 
that each overfit the data in different ways. By building many trees and 
averaging their results, random forests help to reduce overfitting. In 
constructing the forests, the user selects the number of trees to build 
(e.g., 1000). Randomness is introduced using a bootstrapping method that 
repeatedly draws random samples of size \textit{n} from the data set, with 
replacement. The decision trees are built on these random samples 
of the same size, with some points missing and some data points repeated 
\cite{muller17,raschka17}. The algorithm makes a random selection of 
\textit{p}-features, and uses a different set of features at each node branch. 
These processes ensure that all of the decision trees in the random forest are 
different. Random forests is one of the most widely used supervised learning 
algorithms and works well without very much parameter tuning or scaling of data. 
A limitation is that random forests do not perform well with high-dimensional 
data, or data that is sparse (e.g., text).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Gradient Boosting} combines multiple decision trees in a serial fashion, 
with each tree trying to correct for mistakes of the previous one. Gradient 
boosted regression trees use strong prepruning, with shallow trees of a depth 
of one to five. Each tree only provides a good estimate of part of the data; 
combining many shallow trees (i.e., ``weak learners'') iteratively improves 
performance. Gradient boosting and random forests perform well on similar tasks 
and data. A common approach is to first perform random forests and then include 
gradient boosting to improve model accuracy. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluating Model Performance}

To identify which model is best for a given problem, with the data available, 
it is necessary to evaluate the performance of different learning algorithms. 
Binary classification is assessed in terms of the successful assignment of 
observations to one of two classes: positive or negative. No classification 
model can make perfect predictions, as errors are always to be found. Medical 
testing is often used as an example to illustrate classification decisions 
and errors. For example, in the actual state of the world, a patient either 
has an illness or not, and the person is either diagnosed as having the 
illness or not. In the present case, the positive class represents self-
reported pain reliever misuse and abuse (PRL Misuse), and predictions based on 
the classifier models will be either correct or incorrect in relation to the 
observed outcomes. For example, a person who has never misused or abused pain 
relievers may be misclassified as having done so (i.e., 'false positive'), or 
conversely, a person who actually has misused and abused pain relievers may 
be mislabeled as never having done so (i.e., 'false negative').

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Correct decisions and classification errors are represented in a 
\emph{confusion matrix} that indicates the correspondence between predicted 
and actual outcomes (Table 1). The confusion matrix is a two-by-two array in 
which the columns correspond to the actual observed classes and the rows 
correspond to the predicted classes. The main diagonal indicates the number 
of correctly classified samples (i.e., true negative, true positive), while 
the other entries represent the number of samples in one class that were 
mistakenly classified as another class. Several performance metrics can be 
obtained from the confusion matrix, including accuracy, sensitivity (i.e., 
recall), specificity, precision, and the $f_1$-score, presented in Table 2 
\cite{kuhn13, wiki18}. Model performance is most commonly evaluated using 
\emph{Accuracy} which is assessed by the number of correct predictions 
divided by the total observations. Sensitivity or \emph{Recall} provides the 
``True Positive Rate'' (TPR), measured as the number of positive samples that 
are correctly identified by the prediction (number of sick patients correctly
diagnosed). Recall is used when the goal is to avoid false negatives. The 
True Negative Rate (TNR) is described as Specificity, which indicates the 
proportion of negative cases correctly identified (healthy people not 
misdiagnosed). \emph{Precision} provides the ``Positive Predictive Value'' 
(PPV), which measures how many of the samples predicted as positive are 
actually positive. Precision is used as a metric when the goal is to limit
the number of false positives. The \emph{$f_1$-score} represents a harmonic 
mean between recall and precision (\(\frac{2}{ \frac{1}{R} + \frac{1}{P} }\)). 
The $f_1$-score can be a better measure of performance than accuracy in 
datasets with imbalanced classes, where one class is much more frequent 
than the other class, as it takes into account both recall and precision 
\cite{muller17, yun09}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Performance Metrics for Classifier Models.}
  \label{tab:freq}
  \begin{tabular}{lc}
    \toprule
    Accuracy & \(\frac{TP+TN}{TP+FP+TN+FN}\) \\
    & \\
    Sensitivity, Recall (TPR) &  \(\frac{TP}{TP+FN}\) \\
    & \\
    Specificity (TNR) &  \(\frac{TN}{TN+FP}\) \\
    & \\
    Precision (PPV) & \(\frac{TP}{TP+FP}\)  \\
    & \\
    f_1 score & 2*\(\frac{Precision*Recall}{Precision+Recall}\) \\
    & \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Confusion Matrix for Binary Classification.}
  \label{tab:freq}
  \begin{tabular}{cccc}
    \toprule
     & &  \textbf{Actual Outcome} & \\
    \midrule
    \textbf{Predicted} & \textbf{Outcome} & Pass & Fail \\
    \midrule
    & No & \textit{True Negative} & False Negative \\
    & & (TN) & (FN) \\
    \midrule
    & PRL Misuse & False Positive & \textit{True Positive} \\
    & & (FP) & (TP)  \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Classifier Model Performance}

Performance of the classification models was evaluated in the confusion 
matrices reported in Table 5, which includes the metrics of accuracy, 
sensitivity (recall), precision, and $f_1$-score. The model accuracy 
scores ranged form 86.5\% to 90.7\%. Because of the unbalanced classes in 
the sample data|the proportion of respondents who had not misused or abused 
pain relievers (89\%) was much greater than the proportion who had|the 
$f_1$-score was used as the preferred performance metric rather than 
accuracy. The $f_1$-scores ranged from 0.87 to 0.95. Logistic regression 
and the random forest model were tied for the highest $f_1$-score (0.95) 
followed by the decision tree model (0.949). The random forests model 
identified more true positives (1069) and fewer false negatives (3397) 
than the single decision tree or logistic regression. QDA identified 
the highest number of true positives, but detected far fewer true negatives
than logistic regression or LDA. In general, default parameter setting 
were used for most models; the main parameter settings for the 
classification models are presented in Table 6. The results for the 
three top performing models are described below.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Imbalanced Classes}

Previous studies have analyzed the misuse and abuse of prescription opioids
(MUPO) using logistic regression and identified factors that influence MUPO 
such as gender and mental illness \cite{rice12, unick13, jones15, mccabe12}. 
The present study extends previous work by comparing the performance of ten 
classifier models of pain reliever misuse and abuse and evaluating each model 
using four performance metrics (accuracy, sensitivity, precision, $f_1$-score). 
The sample data set has imbalanced classes in the target variable as the 
number of instances of the negative class greatly outnumber instances of 
the positive class. A difficulty of using traditional classification 
algorithms with imbalanced data is that they tend to classify observations
as belonging to the majority class when the class of interest (positive) is 
represented by the minority of observations \cite{brown12, yun09}. This can 
produce a result that underestimates the occurrence of positive cases which 
are misclassified as false negatives. Efforts to address the issue of 
imbalanced data have included sampling methods such as `boosting'. 
As described above, the $f_1$-score is considered a better measure of 
performance than accuracy with data that have imbalanced classes as it 
takes into account both recall and precision. The study also identified 
features important for predicting MUPO. Tradeoffs between model complexity,
performance, and interpretability are discussed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{table*}[ht]
  \caption{Confusion Matrices and Performance Metrics for Classification 
  Models of Pain Reliever Misuse and Abuse}
  \label{tab:freq}
  \begin{tabular}{llllllll}
    \toprule
    Model& & Confusion Matrix & & Accuracy & Sensitivity & Precision & F1-Score \\
    \midrule
    K-Nearest Neighbors & & Fail & Pass &  &  &  & \\
     & Fail & 37567 & 453 & 89.8\% & 0.900 & 0.870 & 0.870 \\
     & Pass & 3905 & 654 &  &  &  & \\
    
    \midrule
    Logistic Regression & & Fail & Pass &  &  &  & \\
     & Fail & 56 &  20 & 74.1\% & 0.48 & 0.727 & 0.598 \\
     & Pass & 58 & 177 &  &  &  & \\
    \midrule
    Linear Discriminant Analysis (LDA) & & Fail & Pass &  &  &  & \\
     & Fail & 55 &  24 & 72.8\% & 0.474 & 0.696 & 0.564 \\
     & Pass & 61 & 173 &  &  &  & \\
    \midrule
    Quadratic Discriminant Analysis (QDA) & & Fail & Pass &  &  &  & \\
     & Fail & 66 &  21 & 77.3\% & 0.569 & 0.759 & 0.650 \\
     & Pass & 50 & 176 &  &  &  & \\
    
    \midrule
    Support Vector Machines (SVM) & & Fail & Pass &  &  &  & \\
     & Fail & 37660 & 360 & 90.3\% & 0.900 & 0.880 & 0.880 \\
     & Pass & 3776 & 783 &  &  &  & \\
    \midrule
    Naive Bayes & & Fail & Pass &  &  &  & \\
     & Fail & 67 &  24 & 76.7\% & 0.578 & 0.736 & 0.648 \\
     & Pass & 49 & 173 &  &  &  & \\
    \midrule
    Neural Network (MLP) & & Fail & Pass &  &  &  & \\
     & Fail & 378 & 534 & 90.6\% & 0.90 & 0.89 & 0.880 \\
     & Pass & 342 & 116 &  &  &  & \\
    
    \midrule
    Decision Trees & & Fail & Pass &  &  &  & \\
     & Fail &  58 &  25 & 73.5\% & 0.50 & 0.699 & 0.583 \\
     & Pass &  58 & 172 &  &  &  & \\
    \midrule
    Random Forests & & Fail & Pass &  &  &  & \\
     & Fail &  57 &  22 & 74.1\% & 0.491 & 0.722 & 0.585 \\
     & Pass &  59 & 175 &  &  &  & \\
    \midrule
    Gradient Boosted Trees & & Fail & Pass &  &  &  & \\
     & Fail & 37955 &  65 & 89.9\% & 0.90 & 0.89 & 0.895 \\
     & Pass & 4261 & 298 &  &  &  & \\
    \bottomrule
  \end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Logistic Regression}

An initial logistic regression on student performance was run including all 
the predictor variables; the model was rerun leaving out non-significant 
features. The parameter estimates, error, z-values, and odds ratios for the 
second model are presented in Table 7. The coefficient estimates are 
interpreted as the multiplicative increase (or decrease) in the odds of the 
dependent variable event happening given a one unit change in the value of 
the predictor variable (for interval scale), or if the event represented 
by a dichotomous predictor variable occurs (e.g., parents together), holding
constant the effects of other independent variables. The parameters relate to 
the log of the odds ratio rather than to the dependent variable directly. 
The odds are calculated as the probability of the event occurring divided 
by the probability of the event not occurring ( \(\frac{P}{P-1}\) ). The 
odds ratio is obtained by taking the antilog of the estimated coefficient, 
which is the exponentiated parameter estimate ($e^x$). In general, taking the 
antilog of the coefficient estimate, subtracting 1 from it, and multiplying 
the result by 100, provides the percent change in the odds for a unit 
increase in the independent variable  \cite{gujarati09}.  







%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure3.pdf}
%  \caption{Proportion of Pain Reliever Misuse and Abuse
%  as a function of Heroin Use.}
%  \label{f:Figure3}
%\end{figure}


Main Parameters
K-Nearest Neighbors  Number of Neighbors = 4 
Support Vector Machines (RBF)  C = 10, gamma = 0.1
Naive Bayes  Cost = 0.01 
Neural Network  Hidden Layers = 1
Decision Trees  Maximum Depth = 4
Random Forests  Number of Trees = 1000 
Boosted Trees  Learning Rate = 0.01 


\subsection{Classification Models}

\subsubsection{Linear Models}

Logistic regression uses a maximum likelihood method to predict the coefficient
estimates that correspond as closely as possible to the default state. 
In other words, the model will predict a number close to one for individuals 
who have misused or abused pain relievers and a number close to  zero for 
individuals who have not. The distribution of conditional probabilities in 
the logit model has an S-shaped curve. By taking the natural log of the left 
side of the regression equation, we obtain the logit function which is linear 
in the parameters, where X = ($X_1$...$X_p$) predictor variables (equation 1). 
The coefficient estimates are selected to maximize the likelihood function, 
and are interpreted as an indication of the log-odds change in the outcome 
variable that is associated with a one-unit increase in the predictor variable, 
holding the effects of other predictor variables constant. The intercept, 
$\beta_0$, is the log of the odds ratio when X is 0 \cite{gujarati09}.

\begin{equation}
  \ ln\frac{P_i}{1-P_i} = \beta_0 + \beta_1X_1 + \beta_1X_2 +... \beta_pX_p \
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Linear discriminant analysis} (LDA) is an alternative approach to 
estimating probabilities that models the distribution of predictors 
separately in each of the response classes and then uses Bayes' theorem 
to `flip' these into estimates for, $Pr(Y=k | X=x)$ \cite{james13}. The term 
linear in LDA refers to the the discriminant functions being linear functions 
of the predictors. For distributions assumed to be normal (i.e., multivariate 
Gaussian), LDA provides a model that is similar in form to logistic regression, 
but more stable. LDA is also preferred for outcomes with more than two response 
classes. The important assumptions for LDA are, first, a common covariance 
matrix for all classes, and second, the class boundaries are linear functions 
of the predictors. \emph{Quadratic Discriminant Analysis} (QDA) is an approach 
that assumes each class has its own covariance matrix and the decision 
boundaries are quadratically curvilinear in the predictor space \cite{kuhn13}. 
LDA is less flexible as a classifier than QDA, but can perform better with 
relatively few training observations or when the majority of predictors in the 
data represent discrete categories. QDA is recommended over LDA with a very 
large training set or when the decision boundary between two classes is 
non-linear. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Support vector machines} (SVM) are an extension of SVC that use a 
kernel trick to reduce computational load. The radial basis function (RBF) 
kernel (i.e., Gaussian kernel) is one of the most commonly used approaches. 
In training the model, only a subset of data points is used to construct the 
decision boundary, namely the support vectors that lie on the border that 
separates the two classes. In predicting classes for new observations, the 
algorithm calculates the distance to each of the support vectors measured 
by the Guassian kernel \cite{muller17}. Figure 1 shows an example of the 
non-linear decision boundary obtained with SVM using the RBF kernel; the 
decision boundary is a smooth curve and the support vectors are the large 
points in bold outline. 

Even with the default settings, the RBF kernel provides a decision boundary 
that is decidedly non-linear. The parameters 
for SVM are `C', which regulates the importance of each data point, and 
`gamma' which controls the width of the Guassian kernel. A small value of 
C indicates a restricted model in which the influence of each data point is 
limited and the algorithm adjusts to the majority of data points. With larger 
values of C, more importance is given to each data point and the model tries 
to correctly classify as many training observations as possible, which results 
in more curvature in the decision boundary. Large values of gamma mean that 
only close values are relevant for classification, resulting in a smooth 
decision boundary. Small values of gamma mean that far points are similar. 
If the values of both C and gamma are large, each point can have a large 
influence in a small region, which produces a choppy decision boundary. 
If the values of C and gamma are both small, the decision boundary becomes 
close to linear.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Naive Bayes Classifier} is considered as a non-linear model.
The Bayes theorem (equation 2) is represented by a set of probabilities 
that answer the question, ``Based on a given set of predictors, 
what is the probability than an outcome belongs to a particular class?''

The prior probability, P(Y), is the expected probability of a given class 
based on what is known (e.g. rate of disease in the population). P(X) 
is the probability of the predictor variables. The conditional probability,
$P(X=cl|Y)$, is the probability of observing the predictor variables for data 
associated with a given class. The naive Bayes model assumes that all of the 
predictor variables are independent, which is not always realistic. The 
conditional probabilities are calculated based on the probability densities 
for each individual predictor \cite{kuhn13}. For categorical predictors, the 
observed frequencies in the training set data can be used to determine the 
probability distributions. The prior probability allows us to tilt the final 
probability toward a particular class. Class probabilities are created and 
the predicted class is the one that is associated with the largest class
probability. Despite the somewhat unrealistic assumption of independence among
predictors, the naive Bayes model is computationally quick, even with large 
training sets, and performs competitively compared to other models. The naive 
Bayes model encounters issues when dealing with frequencies or probabilities 
equal to zero, especially for small sample sizes. In addition, as the number 
of predictors increases relative to the size of the sample, the posterior 
probabilities will become more extreme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Neural Networks} are powerful models for classification and regression 
that are based on theories of connectivity in the brain \cite{kuhn13}. 
The present study considers a simple method called a multilayer perceptron 
(MLP) which is a feed-forward neural network \cite{muller17, raschka17}. 
The outcome is modeled by an intermediary set of unobserved variables called 
hidden units, which are linear combinations of the original predictors
(see Appendix). Each hidden unit is a combination of some or all of the 
predictors which are then transformed by a nonlinear function (e.g. 
sigmoidal). A neural network usually has multiple hidden units used 
to model the outcome. The MLP classifier computes weights between the inputs 
and hidden units, and weights between the layers of hidden units and the 
output. After computing each hidden unit, the output is modeled by a nonlinear 
combination of the hidden units. The nonlinear function allows the neural 
network to fit more complicated functions than a linear model; however, 
neural networks are sensitive to the scaling of the features and can require
extensive data preprocessing. There are several ways to modify the complexity 
of a neural network: by selecting the number of hidden layers, the number of 
units within each layer, and the regularization parameter (L2) which shrinks 
the weights towards zero. The feature weights provide an estimate of feature 
importance. Although neural networks can capture information in large amounts 
of data with very complex models, they tend to overfit data used to train the 
model and can be difficult to interpret. Neural networks may work best with 
homogenous datasets where the predictor variables all have similar meanings 
\cite{muhuri13}. For datasets with many different kinds of features, 
tree-based methods offer a better approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Tree-based Models} Decision trees are based on a hierarchy of 
\emph{`if-else'} questions starting from a root node and proceeding through a 
series of binary decisions or choices. Each node in the tree represents either 
a question or a terminal node (i.e., leaf) that contains the outcome. Applied to 
a binary classification task, the decision tree algorithm learns the sequence
of if-else questions that arrives at the outcome most quickly. For continuous 
features, questions are expressed in the form: ``Is feature x larger than 
value y?'' In constructing the tree, the algorithm searches through all 
possible tests and finds a solution that is most informative about the target 
outcome \cite{muller17}. The recursive branching process yields a binary tree 
of decisions, with each node representing a test for a single feature. This 
process of partitioning is repeated until each leaf in the decision tree 
contains only a single target. Prediction for a new data point proceeds by 
checking which region of the partition the point falls in, and predicting the 
majority in that feature space. The main advantage of tree models is that they 
require little adjustment and are easy to interpret. A drawback is that they 
can lead to complex models which are highly overfit to the training data. 
`Prepruning' can help reduce overfitting by limiting the maximum depth of the 
tree, or the maximum number of leaves. Another approach is to set the minimum 
number of points in a node required for splitting. Decision trees work well 
with features measured on different scales, or with data that has a mix of 
binary and continuous features. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Random Forests} is an ensemble approach that combines many simple trees 
that each overfit the data in different ways. By building many trees and 
averaging their results, random forests help to reduce overfitting. In 
constructing the forests, the user selects the number of trees to build 
(e.g., 1000). Randomness is introduced using a bootstrapping method that 
repeatedly draws random samples of size \textit{n} from the data set, with 
replacement. The decision trees are built on these random samples 
of the same size, with some points missing and some data points repeated 
\cite{muller17,raschka17}. The algorithm makes a random selection of 
\textit{p}-features, and uses a different set of features at each node branch. 
These processes ensure that all of the decision trees in the random forest are 
different. Random forests is one of the most widely used supervised learning 
algorithms and works well without very much parameter tuning or scaling of data. 
A limitation is that random forests do not perform well with high-dimensional 
data, or data that is sparse (e.g., text).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Gradient Boosting} is another ensemble method that combines multiple 
decision trees in a serial fashion, where each tree tries to correct for 
mistakes of the previous one \cite{muller17}. Gradient boosted regression 
trees use strong prepruning, with shallow trees of a depth of one to five. 
Each tree only provides a good estimate of part of the data; combining many 
shallow trees (i.e., ``weak learners'') iteratively improves performance. 
In addition to pre-pruning and the number of trees, an important parameter 
for gradient boosting is the \emph{learning rate} which determines how 
strongly each tree tries to correct for mistakes of previous trees. A high 
learning rate produces stronger corrections, allowing for more complex models. 
Adding more trees to the ensemble also increases model complexity. Gradient 
boosting and random forests perform well on similar tasks and data. A common 
approach is to first perform random forests and then include gradient boosting 
to improve model accuracy. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Result}

The distribution of conditional probabilities 
in the logit model has an S-shaped curve. By taking the natural log of the left 
side of the regression equation, we obtain the logit function which is linear 
in the parameters, where X = ($X_1$...$X_p$) predictor variables (equation 1). 

The coefficient estimates are selected to maximize the likelihood function, 
and are interpreted as an indication of the log-odds change in the outcome 
variable that is associated with a one-unit increase in the predictor variable, 
holding the effects of other predictor variables constant. The intercept, 
$\beta_0$, is the log of the odds ratio when X is 0 \cite{gujarati09}.

\begin{equation}
  \ ln\frac{P_i}{1-P_i} = \beta_0 + \beta_1X_1 + \beta_1X_2 +... \beta_pX_p \
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \caption{Parameter Estimates for Logistic Regression Model and Log-odds 
  Ratios (Training Set).}
  \label{tab:freq}
  \begin{tabular}{lllll}
    \toprule
    Predictor&  Estimate& Std. Err.& Z-Value & Odds Ratio \\    
    \midrule
    (Intercept)         &   0.91 &  0.52 &   1.77     &   \\
    Failures            &  -1.567 &  0.224 &  -6.98 ***  &  4.79  \\
    Course              &  -1.121 &  0.187 &  -6.00 ***  &  3.07  \\
    Higher Ed Plans     &   1.182 &  0.351 &   3.37 ***  &  3.26  \\
    Father's Education  &   0.256 &  0.085 &   2.99 **   &  1.29  \\ 
    School Support      &  -0.792 &  0.269 &  -2.94 **   &  2.21  \\    
    Weekly Alcohol      &  -0.184 &  0.076 &  -2.41 *    &  1.20  \\
    Guardian            &  -0.328 &  0.156 &  -2.11 *    &  1.39  \\
    Internet            &   0.445 &  0.218 &   2.04 *    &  1.56  \\
    Going Out           &  -0.169 &  0.086 &  -1.97 *    &  1.18  \\
    \bottomrule 
    Note: p-value& *$<$ 0.05  & **$<$ 0.01 & ***$<$ 0.001 &   
  \end{tabular}
\end{table}






%\begin{figure}[!ht]
% \centering\includegraphics[width=\columnwidth]{images/Figure4.pdf}
% \caption{Plot of Residual and Fitted Values of Pain Reliever Misuse
%  and Abuse for the Logistic Regression Model}
%  \label{f:Figure4}
%\end{figure}


%\begin{figure}[!ht]
%  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
%  \caption{Proportion of Individuals Reporting Pain Reliever Misuse and Abuse
%  and Heroin Use Ever as a function of Sex}
%  \label{f:Figure5}
%\end{figure}

Although the neural networks (MLP) model and support vector classifier (SLC) 
had the highest accuracy (90.6\% and 90.4\%, respectively), they both had very 
low $f_1$-scores compared to other models (0.88). In terms of the $f_1$-score, 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

According to the classical conditioning theory of addiction, situational cues 
or events can elicit a motivational state underlying the relapse to drug use. 
Following treatment, many addicted individuals return to the same environments 
associated with their drug use. Addictive behavior can be reinstated by exposure 
to drug-related cues or stressors in the environment \cite{shaham03}, putting
individuals in recovery at risk for relapse and possible overdose \cite{johnson11}. 
Furthermore, the structure of social contact networks can influence epidemic 
spreading or diffusion \cite{pastor01, watts98}, as person-to-person contact 
and indirect associations may facilitate the spread of drug use and addictive 
behavior. In the case of prescription opioids, anyone is potentially at 
risk of misusing opioids and becoming addicted. Rather than labeling people 
as addicted or not addicted, it may be useful to consider people as more or 
less susceptible to misusing or abusing opioid pain medication, leading to
a greater or lesser susceptability to becoming dependent or addicted to opioids.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The methods used in this project could be extended to better approximate big 
data for predicting opioid use in the following ways: (1) Include a larger 
selection of features from the attributes in the NSDUH-2015 dataset; (2) 
Include survey data from previous years (e.g., 2005-2015) for a larger sample;  
and (3) Obtain a broader sample from the population of patients who are 
taking prescribed opioid medications. The most immediate step would be to 
include additional features for use with the classifier models. Additional 
data from the NSDUH was downloaded from previous years (2012 to 2014); 
preliminary examination of the data revealed inconsistencies in questions 
and prescription opioid medications that would need to be resolved in order 
to combine data from multiple years. Data cleaning can be a time consuming 
process, but important for obtaining usable data. Unfortunately, owing to 
constraints of time for completing the project, it was not possible to
integrate data from previous years into the project dataset. In working with
big data, there are there are several steps involved in the consolidation of 
data from multiple sources into a single dataset (in addition to data 
cleaning), which include extraction, integration, and aggregation of features  
\cite{rahm00}. A future study could integrate data from different years, 
using a broader set of features, with more inclusive sample representative
of the larger population, and integrate data from multiple sources. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The z-score provides an indication of the relative influence of each predictor 
on the dependent variable. The model was rerun leaving out the non-significant 
variable, Mental Health Treatment, but excluding this variable did not 
significantly change the parameter estimates. 


The LDA model performed similarly to logistic regression in terms of 
$f_1$-score and accuracy, with more true positives and fewer false 
positives, but more false negatives. An important assumption for logistic
regression and LDA is that the class boundaries are linear functions 
of the predictors. When the decision boundary between two classes is 
non-linear, QDA is recommended over LDA, as QDA assumes the decision 
boundary is curvilinear in the predictor space \cite{kuhn13}. The plots 
of the residual and fitted values for the logistic regression in Figure 5
suggest that the relationship may not be linear and therefore a non-linear
may be a better approach for the sample data. It is interesting to note 
that the QDA model had the highest number of true positives, but a lower 
$f_1$-score than the naive Bayes classifier, even though the naive Bayes 
model returned only 30 true positives. Furthermore, the QDA model had a 
lower $f_1$-score and accuracy than decision tree and random forests, 
the tree based models may provide the best fit for the data. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Neural Network}

The multilayer perceptron (MLP) did not perform as well as other models, 
which indicates that it is not a good model for data with imbalanced classes 
\cite{yun09}. The neural network with a single hidden layer is presented in 
Figure 5 for comparison and discussion. The MLP model takes the features as 
the input to the model, a single hidden-layer comprises the weighted 
combination of the input variables, and the output layer represents a 
response probability. During model training, the weights of the predictor 
variables are first randomly initialized and then iteratively adjusted to 
minimize an error function (e.g., gradient descent) \cite{brown12}. 

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/Figure5.pdf}
  \caption{Neural Net Classifier: Multilayer Perceptron with Single Hidden Layer.}
  \label{f:Figure5}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Logistic 
regression provides the coefficient estimates and odds ratios, but is 
restricted to cases where the outcome is modeled as a linear function of 
the independent variables. Decision tree can model non-linear patterns in 
the data, are easy to interpret and explain to others. The results of a 
single decision tree can greatly overfit the data used to train the model, 
and very different trees can be obtained from different samples of the 
training data. Unpruned trees can also result in very complex models; 

Random forests decrease overfitting by building multiple trees trained on
bootstrap samples of the training set using random feature selection in the
process of tree generation \cite{brown12}. Although random forests models are
more accurate than a single tree and can compensate for overfitting, a 
limitation is that it can be difficult to interpret an ensemble of trees apart 
from the measure of feature importance provided in the output. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Survey data may be biased to some degree. People may under-report or minimize 
their use of illicit or illegal substances, and may also be reluctant to 
disclose mental health disorders or illnesses;b however, measures of 
confidentiality and anonymity help to assure more accurate disclosure 
of personal information. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 Furthermore, given the truism that ``more data is better'', 
data from previous years of the NSDUH survey could be included for analysis;
however, due to changes in medication names and wording of survey questions, 
data from NSDUH-2014 did not align with the format for surveys from 2015-2107. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Prescription opioid misuse and abuse was associated with heroin use: of 
the individuals who reported misusing prescription opioid medications, twice 
as many reported having used heroin than those who said that they had not used 
heroin. the importance of heroin use was a predictor of pain reliever misuse and
abuse was limited, given the very small number of individuals who reported 
using heroin.



\subsection{Model Complexity and Interpretability}

Every  model has advantages and limitations. There is a 
tradeoff between model complexity, performance, and interpretability.
Simple models provide interpretable solutions but have lower accuracy, 
whereas complex models yield improved performance but are more difficult 
to interpret. The advantage of logistic regression is that it provides 
coefficient estimates for individual features that can be interpreted in 
terms of odds ratios. The predicted outcome (Y-hat) is represented by the
weighted combination of all the independent variables. The logit function 
is linear in the parameters, but the probabilities of the expected outcome 
are non-linear. The drawbacks of logistic regression are that it does not 
perform well for modeling non-linear relationships between the target
outcome and predictor variables, or with high dimensional data (e.g., $p>n$). 

Decision trees are useful for modeling nonlinear data and can be 
interpreted in terms of the frequency or proportion of observations selected 
at each branch. A single tree visually represents the decision process in a 
manner that is effective for communicating results; however, a disadvantage 
is that decision can become very complex without pruning, especially as the 
number of predictors increases. Limiting the decision tree to a depth of four 
nodes improves the interpretability, but reduces the generalizability of the 
model to new observations. Random forests reduce overfitting by averaging
multiple different trees to identify class membership, which improves 
generalizability to new data. A limitation of random forests is that feature 
importance is determined by the mean decrease in Gini score, a measure of node
purity that is difficult to interpret in terms of the outcome. Gradient 
boosting typically improves accuracy using many simple models iteratively and 
correcting for individual trees; however, in the present study, the boosting 
model did not perform better than random forests. 

As the complexity of a model increases, interpretation becomes more difficult, 
as seen with neural networks. The multilayer perceptron (MLP) is one of the 
most widely used neural network models for classification. With a limited 
number of predictor variables and single hidden layer, it is possible to 
interpret the relations among weights and nodes in the hidden layer 
(see Appendix). As the number of predictors and hidden layers increases, 
complex neural network become opaque to interpretation and represent a 
``black box'' model that is not comprehensible at a human level. In 
applying sophisticated algorithms in predictive modeling, interpretability 
is often sacrificed for greater model accuracy \cite{elgin18}. 
